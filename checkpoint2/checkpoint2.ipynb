{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 2 Notebook\n",
    "\n",
    "__TODO__:\n",
    "- fill this cell in with info, results, insights from our EDA \n",
    "- preprocess: create preprocessing pipeline, store all code into script, then run script to preprocess all data and store into pickle\n",
    "- analysis: need \"3 hypothesis tests\" to validate hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- imports ------------- #\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "import polars as pl\n",
    "from typing import Union, List, Dict\n",
    "import json\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "# ------------ macros ------------ #\n",
    "SAMPLE_RATE = 16e3\n",
    "\n",
    "\n",
    "# ----------------------- preprocessing functions ----------------------- #\n",
    "def rename_audio_files(csv_path: str,\n",
    "                       audio_dir: str,\n",
    "                       output_csv: str = \"renamed_metadata.csv\") -> None:\n",
    "    \"\"\"\n",
    "    Renames audio files based on Participant and Label and saves new metadata.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to the input metadata CSV.\n",
    "        audio_dir (str): Directory containing audio files.\n",
    "        output_csv (str): Filename for the output metadata CSV.\n",
    "    \"\"\"\n",
    "    df = pl.read_csv(csv_path)\n",
    "    renamed_files = []\n",
    "    file_counts = {}\n",
    "\n",
    "    for file in df.iter_rows(named=True):\n",
    "        org_name = file['Filename']\n",
    "        id = file['Participant']\n",
    "        label = file['Label']\n",
    "\n",
    "        key = (id, label)\n",
    "        file_counts[key] = file_counts.get(key, 0) + 1\n",
    "        index = file_counts[key]\n",
    "\n",
    "        new_name = f\"{id}_{label}_{index}.wav\"\n",
    "        old_path = os.path.join(audio_dir, org_name)\n",
    "        new_path = os.path.join(audio_dir, new_name)\n",
    "\n",
    "        os.rename(old_path, new_path)\n",
    "        renamed_files.append((new_name, id, label, index))\n",
    "\n",
    "    renamed_df = pl.DataFrame(renamed_files, schema=[\"Filename\", \"ID\", \"Label\", \"Index\"], orient=\"row\")\n",
    "    output_path = os.path.join(audio_dir, output_csv)\n",
    "    renamed_df.write_csv(output_path)\n",
    "\n",
    "\n",
    "def load_audio_metadata(csv_path: str,\n",
    "                        audio_dir: str,\n",
    "                        limit: Union[int, None] = None) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads audio files from directory and returns a DataFrame with waveform and metadata.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to the CSV file containing data.\n",
    "        audio_dir (str): Directory where audio files are stored.\n",
    "        limit (int, optional): Number of rows to load. If None, loads all.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: DataFrame with columns: Filename, Audio, ID, Label, Duration, Index\n",
    "    \"\"\"\n",
    "    df = pl.read_csv(csv_path)\n",
    "    if limit:\n",
    "        df = df.head(limit)\n",
    "\n",
    "    audio_data = []\n",
    "\n",
    "    for row in df.iter_rows(named=True):\n",
    "        file_name = row['Filename']\n",
    "        file_path = os.path.join(audio_dir, file_name)\n",
    "\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "        duration = len(y) / sr\n",
    "\n",
    "        audio_data.append((\n",
    "            file_name,\n",
    "            y.tolist(),\n",
    "            row['ID'],\n",
    "            row['Label'],\n",
    "            duration,\n",
    "            row['Index']\n",
    "        ))\n",
    "\n",
    "    audio_df = pl.DataFrame(\n",
    "        audio_data,\n",
    "        schema=[\"Filename\", \"Audio\", \"ID\", \"Label\", \"Duration\", \"Index\"],\n",
    "        orient='row'\n",
    "    )\n",
    "\n",
    "    return audio_df\n",
    "\n",
    "\n",
    "def compute_or_load_global_stats(ys: List[np.ndarray],\n",
    "                                 sr: int,\n",
    "                                 n_mels: int = 128,\n",
    "                                 method: str = \"zscore\",\n",
    "                                 stats_file: str = \"global_stats.json\",\n",
    "                                 force_recompute: bool = False) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Computes or loads global normalization stats for Mel spectrograms.\n",
    "\n",
    "    Parameters:\n",
    "        ys (List[np.ndarray]): List of raw audio waveforms.\n",
    "        sr (int): Sample rate.\n",
    "        n_mels (int): Number of Mel bands.\n",
    "        method (str): 'zscore' or 'minmax'.\n",
    "        stats_file (str): Path to save/load stats JSON.\n",
    "        force_recompute (bool): If True, recomputes even if file exists.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: Stats dictionary (mean/std or min/max).\n",
    "    \"\"\"\n",
    "\n",
    "    if not force_recompute and os.path.exists(stats_file):\n",
    "        print(f\"ðŸ—‚ï¸ Loading global stats from {stats_file}\")\n",
    "        with open(stats_file, \"r\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    print(f\"ðŸ“Š Computing global stats with method '{method}'...\")\n",
    "    all_values = []\n",
    "\n",
    "    for y in ys:\n",
    "        S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
    "        S_db = librosa.power_to_db(S, ref=np.max)\n",
    "        all_values.append(S_db.flatten())\n",
    "\n",
    "    all_values = np.concatenate(all_values)\n",
    "    stats = {}\n",
    "\n",
    "    if method == \"zscore\":\n",
    "        stats = {\n",
    "            \"mean\": float(np.mean(all_values)),\n",
    "            \"std\": float(np.std(all_values))\n",
    "        }\n",
    "    elif method == \"minmax\":\n",
    "        stats = {\n",
    "            \"min\": float(np.min(all_values)),\n",
    "            \"max\": float(np.max(all_values))\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported method. Use 'zscore' or 'minmax'.\")\n",
    "\n",
    "    # Save stats to file\n",
    "    with open(stats_file, \"w\") as f:\n",
    "        json.dump(stats, f)\n",
    "        print(f\"ðŸ’¾ Saved global stats to {stats_file}\")\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "\n",
    "def audio_to_spectrogram(y: np.ndarray,\n",
    "                         sr: int,\n",
    "                         n_mels: int = 128,\n",
    "                         target_length: int = 128,\n",
    "                         normalization: str = \"minmax\",\n",
    "                         normalize_scope: str = \"sample\",  # \"sample\" or \"global\"\n",
    "                         global_stats: dict = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Converts a raw audio waveform into a normalized, fixed-size Mel spectrogram.\n",
    "\n",
    "    Parameters:\n",
    "        y (np.ndarray): Raw audio waveform.\n",
    "        sr (int): Sample rate of the audio.\n",
    "        n_mels (int): Number of Mel bands.\n",
    "        target_length (int): Number of time steps to pad/crop to.\n",
    "        normalization (str): 'minmax' or 'zscore'.\n",
    "        normalize_scope (str): 'sample' for per-sample normalization,\n",
    "                               'global' for dataset-wide using global_stats.\n",
    "        global_stats (dict): Required if normalize_scope='global'. Should contain\n",
    "                             'mean' and 'std' or 'min' and 'max'.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Mel spectrogram of shape (n_mels, target_length).\n",
    "    \"\"\"\n",
    "\n",
    "    def _normalize(S_db: np.ndarray, method: str, scope: str, stats: dict = None):\n",
    "        if scope == \"sample\":\n",
    "            if method == \"minmax\":\n",
    "                return (S_db - S_db.min()) / (S_db.max() - S_db.min())\n",
    "            elif method == \"zscore\":\n",
    "                mean = np.mean(S_db)\n",
    "                std = np.std(S_db)\n",
    "                return (S_db - mean) / std\n",
    "        elif scope == \"global\":\n",
    "            if stats is None:\n",
    "                raise ValueError(\"Global normalization requires global_stats.\")\n",
    "            if method == \"minmax\":\n",
    "                return (S_db - stats[\"min\"]) / (stats[\"max\"] - stats[\"min\"])\n",
    "            elif method == \"zscore\":\n",
    "                return (S_db - stats[\"mean\"]) / stats[\"std\"]\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported normalization scope. Use 'sample' or 'global'.\")\n",
    "\n",
    "    def _pad_or_crop(S: np.ndarray, target_len: int):\n",
    "        current_len = S.shape[1]\n",
    "        if current_len < target_len:\n",
    "            pad_width = target_len - current_len\n",
    "            return np.pad(S, ((0, 0), (0, pad_width)), mode='constant')\n",
    "        else:\n",
    "            return S[:, :target_len]\n",
    "\n",
    "    # Compute Mel spectrogram and convert to dB\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
    "    S_db = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "    # Normalize\n",
    "    S_norm = _normalize(S_db, method=normalization, scope=normalize_scope, stats=global_stats)\n",
    "\n",
    "    # Fix shape\n",
    "    S_fixed = _pad_or_crop(S_norm, target_len=target_length)\n",
    "\n",
    "    return S_fixed\n",
    "\n",
    "\n",
    "\n",
    "def pipeline(rename: bool=False, limit: Union[int, None] = None):\n",
    "    \"\"\"\n",
    "    Pipeline to run all preprocessing functions with timing.\n",
    "    \"\"\"\n",
    "    print(\"ðŸš€ Starting preprocessing pipeline...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    # Step 1: Rename files\n",
    "    if rename:\n",
    "        t0 = time.time()\n",
    "        rename_audio_files(\n",
    "            \"./ReCANVo/dataset_file_directory.csv\",\n",
    "            \"./ReCANVo/\"\n",
    "        )\n",
    "        print(f\"ðŸ“ rename_audio_files completed in {time.time() - t0:.2f} seconds\")\n",
    "\n",
    "    # Step 2: Load audio metadata\n",
    "    t0 = time.time()\n",
    "    df = load_audio_metadata(\n",
    "        csv_path=\"./ReCANVo/renamed_metadata.csv\",\n",
    "        audio_dir=\"./ReCANVo/\",\n",
    "        limit=limit\n",
    "    )\n",
    "    print(f\"â³ load_audio_metadata completed in {time.time() - t0:.2f} seconds\")\n",
    "\n",
    "    # Step 3: Compute or load global stats\n",
    "    t0 = time.time()\n",
    "    stats = compute_or_load_global_stats(df[\"Audio\"].to_numpy(), sr=SAMPLE_RATE)\n",
    "    print(f\"ðŸ§® compute_or_load_global_stats completed in {time.time() - t0:.2f} seconds\")\n",
    "    \n",
    "    print(\"\\nðŸ“ˆ Computed Statistics:\")\n",
    "    for k, v in stats.items(): \n",
    "        print(f\"  {k}: {v}\")\n",
    "    print()\n",
    "\n",
    "    # Step 4: Compute spectrograms\n",
    "    t0 = time.time()\n",
    "    df = df.with_columns([\n",
    "        pl.col(\"Audio\").map_elements(lambda y: audio_to_spectrogram(\n",
    "            y=np.array(y),\n",
    "            sr=SAMPLE_RATE,\n",
    "            normalization='zscore',\n",
    "            normalize_scope='global',\n",
    "            global_stats=stats\n",
    "        ), return_dtype=pl.Object).alias(\"Spectrogram\")\n",
    "    ])\n",
    "    print(f\"ðŸ”Š Spectrogram generation completed in {time.time() - t0:.2f} seconds\")\n",
    "\n",
    "    # Done\n",
    "    print(f\"\\nðŸ Full pipeline completed in {time.time() - start:.2f} seconds\\n\")\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set rename to False if files already renamed\n",
    "pipeline(\n",
    "    rename=True,\n",
    "    limit=None # processes all files \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
