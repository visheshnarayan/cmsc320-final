{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 2: Vocalization Feature Analysis/EDA for Non-Verbal Communication\n",
    "\n",
    "This notebook supports our broader project goal of developing techniques to detect and classify non-verbal vocalizations from autistic individuals.\n",
    "\n",
    "Specifically, this notebook focuses on:\n",
    "\n",
    "- Extracting acoustic features (e.g., pitch, MFCCs, spectrograms) from labeled vocal samples\n",
    "- Running statistical hypothesis tests to assess whether these features meaningfully differ across expressive intent labels (e.g., \"yes\" vs. \"no\", \"frustrated\" vs. \"delighted\")\n",
    "- Guiding downstream model development by identifying discriminative features that could improve classification performance\n",
    "\n",
    "The insights gained here help inform which audio characteristics are most relevant for modeling and how they correlate with the communicative intent behind each vocalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- imports ------------- #\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import librosa\n",
    "import scipy.signal as signal\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "import soundfile as sf \n",
    "from typing import Union, List, Dict, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import time\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "# import umap\n",
    "import numpy as np\n",
    "import pickle\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- macros ----------------------- #\n",
    "SAMPLE_RATE = 441e2\n",
    "ORG_CSV_PATH = './ReCANVo/dataset_file_directory.csv'\n",
    "RENAME_CSV_PATH = './ReCANVo/renamed_metadata.csv'\n",
    "AUDIO_DIR = './ReCANVo/'\n",
    "np.random.seed(42)  # Reproducible sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Audio Cleaning\n",
    "Raw audio signals are passed through a customizable cleaning pipeline that includes:\n",
    "\n",
    "- **Normalization** – Scales amplitude values to a consistent range.\n",
    "- **Noise Reduction** – Uses spectral gating to remove background noise.\n",
    "- **Silence Removal** – Removes low-energy segments using a dB threshold and frame-based segmentation.\n",
    "- **High-pass Filtering** – Reduces low-frequency hum and background noise.\n",
    "\n",
    "Optionally, the pipeline can **save both original and cleaned audio** for manual inspection.\n",
    "\n",
    "### 2. File Renaming & Metadata Generation\n",
    "Audio files are renamed using a consistent format:  \n",
    "`<Participant>_<Label>_<Index>.wav`  \n",
    "This enables easier lookup. The updated metadata is saved to a new CSV for downstream use.\n",
    "\n",
    "\n",
    "\n",
    "### 3. Feature Extraction – Mel Spectrograms\n",
    "Each cleaned audio waveform is converted into a **Mel spectrogram** (a time–frequency representation). We ensure consistency across samples by:\n",
    "\n",
    "- Using a fixed number of **Mel bands** (default: 128)\n",
    "- **Normalizing** each spectrogram using either:\n",
    "  - **Sample-level statistics** (mean/std or min/max per sample), or  \n",
    "  - **Global dataset statistics** (computed and optionally saved)\n",
    "  - Either can be swapped based for specific goal\n",
    "- **Padding or cropping** spectrograms to a fixed temporal length for modeling\n",
    "\n",
    "\n",
    "\n",
    "### 4. Final Dataset Output\n",
    "The resulting dataset includes:\n",
    "\n",
    "- Cleaned waveform (`Audio`)\n",
    "- Metadata (`Label`, `Participant ID`, `Filename`, etc.)\n",
    "- Normalized spectrogram (`Spectrogram`)\n",
    "\n",
    "We use a **Polars DataFrame** for speed (since we are working with heavy data). We serialize our data into a `.parquet` for easier loading when we restart our kernel. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- optional preprocessing ------------------- #\n",
    "def rename_audio_files(csv_path: str,\n",
    "                       audio_dir: str,\n",
    "                       output_csv: str = \"renamed_metadata.csv\") -> None:\n",
    "    \"\"\"\n",
    "    Renames audio files based on Participant and Label and saves new metadata.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to the input metadata CSV.\n",
    "        audio_dir (str): Directory containing audio files.\n",
    "        output_csv (str): Filename for the output metadata CSV.\n",
    "    \"\"\"\n",
    "    df = pl.read_csv(csv_path)\n",
    "    renamed_files = []\n",
    "    file_counts = {}\n",
    "\n",
    "    for file in df.iter_rows(named=True):\n",
    "        org_name = file['Filename']\n",
    "        id = file['Participant']\n",
    "        label = file['Label']\n",
    "\n",
    "        key = (id, label)\n",
    "        file_counts[key] = file_counts.get(key, 0) + 1\n",
    "        index = file_counts[key]\n",
    "\n",
    "        new_name = f\"{id}_{label}_{index}.wav\"\n",
    "        old_path = os.path.join(audio_dir, org_name)\n",
    "        new_path = os.path.join(audio_dir, new_name)\n",
    "\n",
    "        if not os.path.exists(old_path):\n",
    "            print(f\"❌ File not found: {old_path}. Skipping renaming process.\")\n",
    "            return  # Exit the function immediately if any file is missing\n",
    "\n",
    "        os.rename(old_path, new_path)\n",
    "        renamed_files.append((new_name, id, label, index))\n",
    "\n",
    "    # If renaming was successful, save the updated metadata\n",
    "    renamed_df = pl.DataFrame(renamed_files, schema=[\"Filename\", \"ID\", \"Label\", \"Index\"], orient=\"row\")\n",
    "    output_path = os.path.join(audio_dir, output_csv)\n",
    "    renamed_df.write_csv(output_path)\n",
    "    \n",
    "def save_audio_comparison(original_y: np.ndarray, \n",
    "                           cleaned_y: np.ndarray, \n",
    "                           sr: int, \n",
    "                           filename: str, \n",
    "                           output_dir: str = 'audio_comparisons') -> None:\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    base_name = os.path.splitext(filename)[0]\n",
    "    original_path = os.path.join(output_dir, f\"{base_name}_original.wav\")\n",
    "    cleaned_path = os.path.join(output_dir, f\"{base_name}_cleaned.wav\")\n",
    "\n",
    "    sf.write(original_path, original_y, sr)\n",
    "    sf.write(cleaned_path, cleaned_y, sr)\n",
    "\n",
    "\n",
    "def clean_audio(y: np.ndarray, \n",
    "                sr: int, \n",
    "                denoise: bool = True, \n",
    "                remove_silence: bool = True,\n",
    "                normalize: bool = True,\n",
    "                min_silence_duration: float = 0.3,\n",
    "                silence_threshold: float = -40) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Enhanced audio cleaning function tailored for voice recordings of autistic individuals.\n",
    "\n",
    "    Parameters:\n",
    "        y (np.ndarray): Input audio time series\n",
    "        sr (int): Sampling rate\n",
    "        denoise (bool): Apply noise reduction\n",
    "        remove_silence (bool): Remove long silent segments\n",
    "        normalize (bool): Normalize audio amplitude\n",
    "        min_silence_duration (float): Minimum duration of silence to remove (in seconds)\n",
    "        silence_threshold (float): Decibel threshold for silence detection\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Cleaned audio time series\n",
    "    \"\"\"\n",
    "    if len(y) == 0:\n",
    "        return y  # Return empty if the input is empty\n",
    "\n",
    "    cleaned_audio = y.copy()\n",
    "\n",
    "    if normalize:\n",
    "        cleaned_audio = librosa.util.normalize(cleaned_audio)\n",
    "\n",
    "    # Noise reduction using spectral gating\n",
    "    if denoise:\n",
    "        stft = librosa.stft(cleaned_audio)                # Compute STFT with valid n_fft\n",
    "        mag, phase = librosa.magphase(stft)               # Magnitude and phase\n",
    "        noise_threshold = np.median(mag) * 0.5\n",
    "        mask = mag > noise_threshold                      # Apply noise threshold mask\n",
    "        cleaned_stft = stft * mask                        \n",
    "        cleaned_audio = librosa.istft(cleaned_stft)       # Convert back to time domain\n",
    "\n",
    "    # Remove long silent segments\n",
    "    if remove_silence:\n",
    "        frame_length = int(sr * min_silence_duration)\n",
    "        hop_length = max(1, frame_length // 2)  # Ensure hop_length is at least 1\n",
    "\n",
    "        non_silent_frames = librosa.effects.split(\n",
    "            cleaned_audio, \n",
    "            top_db=abs(silence_threshold), \n",
    "            frame_length=frame_length, \n",
    "            hop_length=hop_length\n",
    "        )\n",
    "\n",
    "        if len(non_silent_frames) == 0:\n",
    "            return np.array([])  # Return empty if all frames are silent\n",
    "\n",
    "        cleaned_audio = np.concatenate([\n",
    "            cleaned_audio[start:end] for start, end in non_silent_frames\n",
    "        ])\n",
    "\n",
    "    # Apply high-pass filter to reduce low-frequency noise\n",
    "    b, a = signal.butter(6, 80 / (sr/2), btype='high')\n",
    "    cleaned_audio = signal.filtfilt(b, a, cleaned_audio)\n",
    "\n",
    "    return cleaned_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_metadata(csv_path: str,\n",
    "                        audio_dir: str,\n",
    "                        limit: Union[int, None] = None,\n",
    "                        clean_audio_params: dict = None,\n",
    "                        save_comparisons: bool = False,\n",
    "                        comparison_dir: str = 'audio_comparisons') -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads audio metadata and processes files in parallel.\n",
    "    \n",
    "    Args:\n",
    "        csv_path (str): Path to CSV file with metadata.\n",
    "        audio_dir (str): Directory where audio files are stored.\n",
    "        limit (int, optional): Number of rows to load.\n",
    "        clean_audio_params (dict, optional): Parameters for cleaning.\n",
    "        save_comparisons (bool): Save original vs cleaned audio files.\n",
    "        comparison_dir (str): Directory for saved audio comparisons.\n",
    "    \n",
    "    Returns:\n",
    "        pl.DataFrame: DataFrame with processed audio metadata.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pl.read_csv(csv_path).drop_nulls(subset=['Filename'])\n",
    "\n",
    "    if limit:\n",
    "        df = df.head(limit)\n",
    "\n",
    "    # Default audio cleaning parameters\n",
    "    default_clean_params = {\n",
    "        'denoise': True,\n",
    "        'remove_silence': True,\n",
    "        'normalize': True,\n",
    "        'min_silence_duration': 0.3,\n",
    "        'silence_threshold': -40\n",
    "    }\n",
    "    clean_params = {**default_clean_params, **(clean_audio_params or {})}\n",
    "\n",
    "    # Prepare file processing queue \n",
    "    file_info_list = [\n",
    "        (row['Filename'], \n",
    "         os.path.join(audio_dir, row['Filename']), \n",
    "         clean_params, \n",
    "         save_comparisons, \n",
    "         comparison_dir, \n",
    "         row['ID'],\n",
    "         row['Label'],  \n",
    "         row['Index']) \n",
    "        for row in df.iter_rows(named=True)\n",
    "    ]\n",
    "\n",
    "    # Modify process_audio_file to handle the additional parameters\n",
    "    def process_audio_file(file_info: Tuple[str, str, dict, bool, str, int, str, int]) -> Union[Tuple[str, List[float], int, str, float, int], None]:\n",
    "        \"\"\"\n",
    "        Loads and processes an audio file.\n",
    "\n",
    "        Args:\n",
    "            file_info (Tuple): Contains filename, full path, cleaning params, saving options, ID, Label, and Index.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[str, List[float], int, str, float, int] | None: Processed audio metadata or None if failed.\n",
    "        \"\"\"\n",
    "        file_name, file_path, clean_params, save_comparisons, comparison_dir, file_id, label, index = file_info\n",
    "\n",
    "        y, sr = librosa.load(file_path, sr=SAMPLE_RATE)  \n",
    "        cleaned_y = clean_audio(y, sr, **clean_params)\n",
    "\n",
    "        if save_comparisons:\n",
    "            save_audio_comparison(y, cleaned_y, sr, file_name, comparison_dir)\n",
    "\n",
    "        duration = len(cleaned_y) / sr\n",
    "        return file_name, cleaned_y.tolist(), file_id, label, duration, index  \n",
    "\n",
    "    # Use ThreadPoolExecutor for parallel processing\n",
    "    with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        results = list(executor.map(process_audio_file, file_info_list))\n",
    "\n",
    "    # Filter out None values from failed processing\n",
    "    audio_data = [res for res in results if res]\n",
    "\n",
    "    return pl.DataFrame(audio_data, schema=[\"Filename\", \"Audio\", \"ID\", \"Label\", \"Duration\", \"Index\"], orient='row')\n",
    "\n",
    "\n",
    "def compute_or_load_global_stats(ys: List[np.ndarray],\n",
    "                                 sr: int=SAMPLE_RATE,\n",
    "                                 n_mels: int = 128,\n",
    "                                 method: str = \"zscore\",\n",
    "                                 stats_file: str = \"global_stats.json\",\n",
    "                                 force_recompute: bool = False) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Computes or loads global normalization stats for Mel spectrograms.\n",
    "\n",
    "    Parameters:\n",
    "        ys (List[np.ndarray]): List of raw audio waveforms.\n",
    "        sr (int): Sample rate.\n",
    "        n_mels (int): Number of Mel bands.\n",
    "        method (str): 'zscore' or 'minmax'.\n",
    "        stats_file (str): Path to save/load stats JSON.\n",
    "        force_recompute (bool): If True, recomputes even if file exists.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: Stats dictionary (mean/std or min/max).\n",
    "    \"\"\"\n",
    "\n",
    "    if not force_recompute and os.path.exists(stats_file):\n",
    "        print(f\"🗂️ Loading global stats from {stats_file}\")\n",
    "        with open(stats_file, \"r\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    print(f\"📊 Computing global stats with method '{method}'...\")\n",
    "    all_values = []\n",
    "\n",
    "    for y in ys:\n",
    "        S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
    "        S_db = librosa.power_to_db(S, ref=np.max)\n",
    "        all_values.append(S_db.flatten())\n",
    "\n",
    "    all_values = np.concatenate(all_values)\n",
    "    stats = {}\n",
    "\n",
    "    if method == \"zscore\":\n",
    "        stats = {\n",
    "            \"mean\": float(np.mean(all_values)),\n",
    "            \"std\": float(np.std(all_values))\n",
    "        }\n",
    "    elif method == \"minmax\":\n",
    "        stats = {\n",
    "            \"min\": float(np.min(all_values)),\n",
    "            \"max\": float(np.max(all_values))\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported method. Use 'zscore' or 'minmax'.\")\n",
    "\n",
    "    # Save stats to file\n",
    "    with open(stats_file, \"w\") as f:\n",
    "        json.dump(stats, f)\n",
    "        print(f\"💾 Saved global stats to {stats_file}\")\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "def audio_to_spectrogram(y: np.ndarray,\n",
    "                         sr: int=SAMPLE_RATE,\n",
    "                         n_mels: int = 128,\n",
    "                         target_length: int = 128,\n",
    "                         normalization: str = \"minmax\",\n",
    "                         normalize_scope: str = \"sample\",  # \"sample\" or \"global\"\n",
    "                         global_stats: dict = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Converts a raw audio waveform into a normalized, fixed-size Mel spectrogram.\n",
    "\n",
    "    Parameters:\n",
    "        y (np.ndarray): Raw audio waveform.\n",
    "        sr (int): Sample rate of the audio.\n",
    "        n_mels (int): Number of Mel bands.\n",
    "        target_length (int): Number of time steps to pad/crop to.\n",
    "        normalization (str): 'minmax' or 'zscore'.\n",
    "        normalize_scope (str): 'sample' for per-sample normalization,\n",
    "                               'global' for dataset-wide using global_stats.\n",
    "        global_stats (dict): Required if normalize_scope='global'. Should contain\n",
    "                             'mean' and 'std' or 'min' and 'max'.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Mel spectrogram of shape (n_mels, target_length).\n",
    "    \"\"\"\n",
    "\n",
    "    def _normalize(S_db: np.ndarray, method: str, scope: str, stats: dict = None):\n",
    "        if scope == \"sample\":\n",
    "            if method == \"minmax\":\n",
    "                return (S_db - S_db.min()) / (S_db.max() - S_db.min())\n",
    "            elif method == \"zscore\":\n",
    "                mean = np.mean(S_db)\n",
    "                std = np.std(S_db)\n",
    "                return (S_db - mean) / std\n",
    "        else:\n",
    "            if method == \"minmax\":\n",
    "                return (S_db - stats[\"min\"]) / (stats[\"max\"] - stats[\"min\"])\n",
    "            elif method == \"zscore\":\n",
    "                return (S_db - stats[\"mean\"]) / stats[\"std\"]\n",
    "\n",
    "    def _pad_or_crop(S: np.ndarray, target_len: int):\n",
    "        current_len = S.shape[1]\n",
    "        if current_len < target_len:\n",
    "            pad_width = target_len - current_len\n",
    "            return np.pad(S, ((0, 0), (0, pad_width)), mode='constant')\n",
    "        else:\n",
    "            return S[:, :target_len]\n",
    "    \n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
    "    S_db = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "    S_norm = _normalize(S_db, method=normalization, scope=normalize_scope, stats=global_stats)\n",
    "    S_fixed = _pad_or_crop(S_norm, target_len=target_length)\n",
    "\n",
    "    return S_fixed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- pipeline ----------------------- #\n",
    "def pipeline(rename: bool = False, \n",
    "             limit: Union[int, None] = None,\n",
    "             clean_audio_params: dict = None,\n",
    "             save_comparisons: bool = False,\n",
    "             ) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Pipeline to run all preprocessing functions with timing and optional audio cleaning.\n",
    "    Only supports saving to .parquet (not CSV) to handle arrays properly.\n",
    "    \"\"\"\n",
    "    print(\"🚀 Starting preprocessing pipeline...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    if rename:\n",
    "        t0 = time.time()\n",
    "        rename_audio_files(\n",
    "            csv_path=ORG_CSV_PATH,\n",
    "            audio_dir=AUDIO_DIR,\n",
    "        )\n",
    "        print(f\"📝 rename_audio_files completed in {time.time() - t0:.2f} seconds\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    df = load_audio_metadata(\n",
    "        csv_path=RENAME_CSV_PATH,\n",
    "        audio_dir=AUDIO_DIR,\n",
    "        limit=limit,\n",
    "        clean_audio_params=clean_audio_params,\n",
    "        save_comparisons=save_comparisons\n",
    "    )\n",
    "    print(f\"⏳ load_audio_metadata completed in {time.time() - t0:.2f} seconds\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    stats = compute_or_load_global_stats(df[\"Audio\"].to_numpy(), sr=SAMPLE_RATE)\n",
    "    print(f\"🧮 compute_or_load_global_stats completed in {time.time() - t0:.2f} seconds\")\n",
    "    \n",
    "    print(\"\\n📈 Computed Statistics:\")\n",
    "    for k, v in stats.items(): \n",
    "        print(f\"  {k}: {v}\")\n",
    "    print()\n",
    "\n",
    "    t0 = time.time()\n",
    "    df = df.with_columns([\n",
    "        pl.col(\"Audio\").map_elements(lambda y: audio_to_spectrogram(\n",
    "            y=np.array(y),\n",
    "            sr=SAMPLE_RATE,\n",
    "            normalization='zscore',\n",
    "            normalize_scope='global',\n",
    "            global_stats=stats\n",
    "        ), return_dtype=pl.Object).alias(\"Spectrogram\")\n",
    "    ])\n",
    "    print(f\"🔊 Spectrogram generation completed in {time.time() - t0:.2f} seconds\")\n",
    "    \n",
    "    print(f\"🏁 Full pipeline completed in {time.time() - start:.2f} seconds\\n\")\n",
    "    print(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Do not run pipeline if data is cached already \n",
    "\n",
    "Or your computer might michael bay maxx\n",
    "\n",
    "Do you like our pipeline emojis ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_clean_params = {\n",
    "    'denoise': True,\n",
    "    'remove_silence': True,\n",
    "    'normalize': True,\n",
    "    'min_silence_duration': 0.3,\n",
    "    'silence_threshold': -40\n",
    "}\n",
    "\n",
    "df = pipeline(\n",
    "    rename=False, \n",
    "    limit=None,\n",
    "    clean_audio_params=custom_clean_params,\n",
    "    save_comparisons=False\n",
    ")\n",
    "\n",
    "# Convert data to numpy arrays for serialization\n",
    "df = df.with_columns([\n",
    "    pl.col(\"Audio\").map_elements(lambda y: np.array(y, dtype=np.float64).tolist(), return_dtype=pl.List(pl.Float64)),\n",
    "    pl.col(\"Spectrogram\").map_elements(lambda s: np.array(s, dtype=np.float64).tolist(), return_dtype=pl.List(pl.List(pl.Float64)))\n",
    "])\n",
    "\n",
    "# Save to a Parquet file\n",
    "df.write_parquet(\"processed_data.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in data from cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_pickle(path: str) -> pl.DataFrame:\n",
    "    with open(path, \"rb\") as f:\n",
    "        df = pickle.load(f)\n",
    "    return df\n",
    "\n",
    "df = open_pickle(\"./processed_data.pkl\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faster than pickle\n",
    "def open_parquet(path: str) -> pl.DataFrame:\n",
    "    return pl.read_parquet(path)\n",
    "\n",
    "df = open_parquet(\"./processed_data.parquet\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "- Explored shape of df, distributions of various features\n",
    "- Visualized distributions of labels, spectograms across unique labels\n",
    "- Projected audio features into 2D space to detect possible clusters forming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataframe shape: {df.shape}\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YIPEEEEE (no empty data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.null_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = df['Label'].value_counts()\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are discrepencies of how many labels exist per group, where we have a mean of approximately 320, but aving high deviation of nearly 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = plt.cm.viridis(np.linspace(0, 1, len(label_counts))) # pretty colors \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(label_counts['Label'], label_counts['count'], color=colors)\n",
    "plt.title('Distribution of Labels')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=70) # so labels don't overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viridis colors\n",
    "colors = px.colors.sample_colorscale('Viridis', np.linspace(0, 1, len(label_counts)))\n",
    "\n",
    "# Interactive bar chart\n",
    "fig = px.bar(\n",
    "    label_counts,\n",
    "    x='Label',\n",
    "    y='count',\n",
    "    title='Distribution of Labels',\n",
    "    color='Label',\n",
    "    color_discrete_sequence=colors\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='Label',\n",
    "    yaxis_title='Count',\n",
    "    xaxis_tickangle=70,\n",
    "    margin=dict(l=40, r=40, t=60, b=100),\n",
    "    plot_bgcolor='white',\n",
    ")\n",
    "\n",
    "# Save to HTML\n",
    "fig.show()\n",
    "fig.write_html(\"../checkpoint3/website/clarity/images/label_distribution.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Dominant Labels**: Some participants, like P08 and P11, primarily exhibit labels like \"request\" or \"delighted\", indicating specific behaviors or contexts.\n",
    "- **Label Variety**: Participants vary in label diversity; for example, P05 shows a wide array while P11 predominantly shows one label.\n",
    "- **Customized Approaches**: The differences in label distribution across participants suggest that personalized models might be more effective (depending on model type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for each person\n",
    "participant_label_counts = df.group_by(['ID', 'Label']).agg(pl.len().alias('Count'))\n",
    "participant_label_counts = participant_label_counts.to_pandas()\n",
    "\n",
    "participant_ids = participant_label_counts['ID'].unique()\n",
    "n_cols = 4\n",
    "n_rows = 2\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 4), constrained_layout=True)\n",
    "fig.suptitle('Label Distribution per Participant', fontsize=16)\n",
    "\n",
    "for idx, participant_id in enumerate(participant_ids):\n",
    "    ax = axes[idx // n_cols, idx % n_cols]  # next subplot\n",
    "    data = participant_label_counts[participant_label_counts['ID'] == participant_id]\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(data['Label'])))\n",
    "    ax.bar(data['Label'], data['Count'], color=colors)\n",
    "    ax.set_title(f'Participant {participant_id}')\n",
    "    ax.set_xlabel('Label')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.tick_params(axis='x', rotation=70)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure output directory exists\n",
    "output_dir = \"participant_barplots\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Convert Polars groupby to pandas if not already done\n",
    "# participant_label_counts = df.group_by(['ID', 'Label']).agg(pl.len().alias('Count')).to_pandas()\n",
    "\n",
    "participant_ids = participant_label_counts['ID'].unique()\n",
    "\n",
    "for participant_id in participant_ids:\n",
    "    data = participant_label_counts[participant_label_counts['ID'] == participant_id]\n",
    "    \n",
    "    # Generate Viridis colors\n",
    "    colors = px.colors.sample_colorscale('Viridis', np.linspace(0, 1, len(data)))\n",
    "\n",
    "    # Create interactive bar chart\n",
    "    fig = px.bar(\n",
    "        data,\n",
    "        x='Label',\n",
    "        y='Count',\n",
    "        title=f'Label Distribution for Participant {participant_id}',\n",
    "        color='Label',\n",
    "        color_discrete_sequence=colors\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title='Label',\n",
    "        yaxis_title='Count',\n",
    "        xaxis_tickangle=70,\n",
    "        margin=dict(l=40, r=40, t=60, b=100),\n",
    "        plot_bgcolor='white',\n",
    "        autosize=False,\n",
    "        width=400,    \n",
    "        height=400,\n",
    "    )\n",
    "\n",
    "    # Save each plot as HTML\n",
    "    filename = os.path.join(output_dir, f\"participant_{participant_id}.html\")\n",
    "    fig.show()\n",
    "    fig.write_html(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that length of audios vary as well, very important to pad prior to training to prepare input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_columns([\n",
    "    pl.col(\"Audio\").map_elements(lambda a: len(a), return_dtype=pl.Float64).alias(\"Audio Length\")\n",
    "])\n",
    "df['Audio Length'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectrograms of Unique Label Groups\n",
    "\n",
    "This grid displays one **Mel spectrogram** for each unique vocalization label in the dataset. Each spectrogram represents a **single audio sample** randomly selected from that label group.\n",
    "\n",
    "__Why Spectograms:__ A spectrogram is a time-frequency visualization of sound. It shows how energy (brightness) is distributed across frequency bins (y-axis) over time (x-axis). Brighter regions indicate more intensity at that frequency and time.\n",
    "\n",
    "__Flat colors at end of some labels__: You may notice that many spectrograms appear to **suddenly turn into a solid blue color** after a certain point. This occurs because:\n",
    "\n",
    "- All spectrograms have been **padded or cropped to a fixed width** (`target_length`), ensuring uniform input size for modeling.\n",
    "- When the original audio sample is **shorter than the target time length**, the remaining time frames are filled with **zeros** — resulting in that flat, dark blue region on the right.\n",
    "- This is done to make all inputs the same shape for consistent processing and modeling (e.g., embeddings or CNNs).\n",
    "\n",
    "---\n",
    "\n",
    "These spectrograms give an intuitive view of the **acoustic patterns** present in each vocalization type — for example:\n",
    "- \"YES\" shows low-frequency harmonics,\n",
    "- \"FRUSTRATED\" is noisier and denser,\n",
    "- \"SELF-TALK\" often contains repeating patterns,\n",
    "- \"GLEE\" and \"DELIGHTED\" appear more tonal or melodic.\n",
    "\n",
    "This kind of visualization helps validate that **distinct spectral features** exist across labels, supporting downstream classification or clustering tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_unique_label_spectrograms_grid(df, n_rows=4, n_cols=6):\n",
    "    unique_labels = df.select(\"Label\").unique().to_series().to_list()\n",
    "    total_plots = n_rows * n_cols\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(n_cols * 4, n_rows * 3))\n",
    "    axs = axs.flatten()\n",
    "    fig.suptitle(\"Unique Label Spectogram\", fontsize=30)\n",
    "\n",
    "    for idx, label in enumerate(unique_labels):\n",
    "        ax = axs[idx]\n",
    "\n",
    "        # Get the first spectrogram for this label\n",
    "        row = df.filter(pl.col(\"Label\") == label).row(0)\n",
    "        spectrogram = row[df.columns.index(\"Spectrogram\")]\n",
    "        spectrogram_np = np.array(spectrogram, dtype=np.float32)\n",
    "\n",
    "        if spectrogram_np.ndim == 2:\n",
    "            im = ax.imshow(spectrogram_np, aspect=\"auto\", origin=\"lower\", cmap=\"viridis\")\n",
    "            ax.set_title(label.upper(), fontsize=18)\n",
    "            ax.set_xlabel(\"Time\")\n",
    "            ax.set_ylabel(\"Freq\")\n",
    "        else:\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "    # Hide any unused axes\n",
    "    for j in range(len(unique_labels), len(axs)):\n",
    "        axs[j].axis(\"off\")\n",
    "\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_unique_label_spectrograms_grid(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_unique_label_spectrograms_plotly(df, output_dir=\"spectrograms\"):\n",
    "    \"\"\"\n",
    "    Generate individual Plotly spectrograms for each unique label and save as HTML files.\n",
    "    \n",
    "    Args:\n",
    "        df: Polars DataFrame containing 'Label' and 'Spectrogram' columns\n",
    "        output_dir: Directory to save the HTML files (will be created if it doesn't exist)\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get unique labels\n",
    "    unique_labels = df.select(\"Label\").unique().to_series().to_list()\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        # Get the first spectrogram for this label\n",
    "        row = df.filter(pl.col(\"Label\") == label).row(0)\n",
    "        spectrogram = row[df.columns.index(\"Spectrogram\")]\n",
    "        spectrogram_np = np.array(spectrogram, dtype=np.float32)\n",
    "        \n",
    "        if spectrogram_np.ndim == 2:\n",
    "            # Create a Plotly figure\n",
    "            fig = go.Figure()\n",
    "            \n",
    "            # Add the spectrogram as a heatmap\n",
    "            fig.add_trace(go.Heatmap(\n",
    "                z=spectrogram_np,\n",
    "                colorscale='viridis',\n",
    "                showscale=True\n",
    "            ))\n",
    "            \n",
    "            # Update layout with title and axis labels\n",
    "            fig.update_layout(\n",
    "                title=f\"{label.upper()} Spectrogram\",\n",
    "                xaxis_title=\"Time\",\n",
    "                yaxis_title=\"Frequency\",\n",
    "                width=400,\n",
    "                height=400\n",
    "            )\n",
    "            \n",
    "            # Save as HTML file\n",
    "            output_file = os.path.join(output_dir, f\"{label.lower()}_spectrogram.html\")\n",
    "            fig.show()\n",
    "            fig.write_html(output_file)\n",
    "            print(f\"Saved {output_file}\")\n",
    "        else:\n",
    "            print(f\"Skipping {label}: spectrogram is not 2-dimensional\")\n",
    "    \n",
    "    print(f\"All spectrograms saved to '{output_dir}' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_unique_label_spectrograms_plotly(df, output_dir=\"spectrograms_plotly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2D Projection Setup for Audio Data Visualization\n",
    "\n",
    "To visualize complex audio data, we extract key features, specifically Mel Frequency Cepstral Coefficients (MFCCs) and Pitch Variance, from the audio samples. These features are chosen for their ability to encapsulate the essential characteristics of sound. \n",
    "\n",
    "The extracted features are then subjected to dimensionality reduction techniques to project them into a 2D space, facilitating easier visualization and interpretation. The methods used for this purpose include:\n",
    "\n",
    "- **PCA (Principal Component Analysis)**: Linear transformation technique to reduce the dimensionality while attempting to preserve as much variance as possible.\n",
    "- **t-SNE (t-Distributed Stochastic Neighbor Embedding)**: A non-linear approach, t-SNE is effective in visualizing high-dimensional data by maintaining local relationships in a lower-dimensional space.\n",
    "- **UMAP (Uniform Manifold Approximation and Projection)**: Another non-linear method that excels in preserving both local and global data structures, making it ideal for a nuanced exploration of audio features.\n",
    "\n",
    "These projections allow us to visually analyze the clustering and distribution of audio samples, thereby providing insights into the inherent patterns and distinctions within the data.\n",
    "\n",
    "We notice no clear clusters forming, requiring possible kernel tricks to get better seperation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pitch variance as feature for audio projection\n",
    "def get_pitch_var(y: List[float], sr: int=SAMPLE_RATE):\n",
    "    y_np = np.array(y, dtype=np.float64)\n",
    "    f0, voiced_flag, _ = librosa.pyin(\n",
    "        y_np,\n",
    "        sr=sr,\n",
    "        fmin=librosa.note_to_hz('C2'),\n",
    "        fmax=librosa.note_to_hz('C7')\n",
    "    )\n",
    "    if f0 is None:\n",
    "        return 0.0\n",
    "    f0_voiced = f0[voiced_flag]\n",
    "    return float(np.std(f0_voiced)) if len(f0_voiced) > 0 else 0.0\n",
    "\n",
    "# Gets MFCC means of signal (feature for audio projection)\n",
    "def get_mfcc_means(y: List[float], sr: int = 16000, n_mfcc: int = 3) -> List[float]:\n",
    "    y_np = np.array(y, dtype=np.float32)\n",
    "    mfccs = librosa.feature.mfcc(y=y_np, sr=sr, n_mfcc=n_mfcc)\n",
    "    return np.mean(mfccs, axis=1).tolist()  # returns [mfcc-1, mfcc-2, mfcc-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract MFCC-1, MFCC-2, MFCC-3, and Pitch variance as features for PCA\n",
    "df = df.with_columns([\n",
    "    pl.col(\"Audio\").map_elements(lambda y: get_mfcc_means(y)[0], return_dtype=pl.Float64).alias(\"MFCC-1\"),\n",
    "    pl.col(\"Audio\").map_elements(lambda y: get_mfcc_means(y)[1], return_dtype=pl.Float64).alias(\"MFCC-2\"),\n",
    "    pl.col(\"Audio\").map_elements(lambda y: get_mfcc_means(y)[2], return_dtype=pl.Float64).alias(\"MFCC-3\"),\n",
    "    pl.col(\"Audio\").map_elements(lambda y: get_pitch_var(y), return_dtype=pl.Float64).alias(\"PitchVar\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Correlations between Features**:\n",
    "  - Displays varying correlation strengths between audio features.\n",
    "  - Notable strong negative correlation between MFCC-1 and MFCC-3.\n",
    "  - Other pairs show weak to moderate correlations.\n",
    "\n",
    "- **2D PCA of Audio Features**:\n",
    "  - Data points are broadly dispersed.\n",
    "  - No clear clustering by labels, suggesting limited effectiveness of PCA in separating types.\n",
    "\n",
    "- **UMAP Projection of Audio Features**:\n",
    "  - Shows improved clustering compared to PCA.\n",
    "  - Distinct groups suggest better separation of features.\n",
    "\n",
    "- **t-SNE Projection of Audio Features**:\n",
    "  - Exhibits tight clustering and clear separation between different audio labels.\n",
    "  - Indicates strong capability of t-SNE in capturing local relationships in data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features for correlation heatmap\n",
    "features = [\"PitchVar\", \"MFCC-1\", \"MFCC-2\", \"MFCC-3\"]\n",
    "corr = df[features].corr()\n",
    "\n",
    "# Convert Polars DataFrame to numpy array\n",
    "corr_array = corr.to_numpy()\n",
    "corr_rounded = np.round(corr_array, 2)\n",
    "\n",
    "# 1. Correlation Heatmap (Plotly)\n",
    "fig_heatmap = go.Figure(data=go.Heatmap(\n",
    "    z=corr_array,\n",
    "    x=features,\n",
    "    y=features,\n",
    "    zmin=-1, zmax=1,\n",
    "    colorscale='RdBu_r',\n",
    "    text=corr_rounded,\n",
    "    texttemplate='%{text}',\n",
    "    showscale=True,\n",
    "))\n",
    "\n",
    "fig_heatmap.update_layout(\n",
    "        title={\n",
    "            'text': \"Correlation Heatmap of Audio Features\",\n",
    "            'xanchor': 'left',\n",
    "            'yanchor': 'top',\n",
    "        },\n",
    "        margin=dict(l=20, r=220, t=60, b=20),  # More right margin\n",
    "        legend=dict(\n",
    "            orientation=\"v\",\n",
    "            yanchor=\"top\",\n",
    "            y=1,\n",
    "            xanchor=\"left\",\n",
    "            x=1,  # Push legend further right outside the plot\n",
    "            font=dict(size=12),\n",
    "            itemwidth=40,  # Force horizontal space per legend item\n",
    "            title='Correlations between Features'\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig_heatmap.show()\n",
    "fig_heatmap.write_html(\"correlation_heatmap.html\")\n",
    "\n",
    "# Prepare MFCC data\n",
    "mfccs = [\"MFCC-\" + str(i) for i in range(1, 4)]\n",
    "X = df[mfccs].to_numpy()\n",
    "labels = df[\"Label\"].to_numpy()\n",
    "unique_labels = np.unique(labels)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Create a distinct color map for the labels\n",
    "distinct_colors = {\n",
    "    label: color for label, color in zip(\n",
    "        unique_labels, \n",
    "        px.colors.qualitative.D3 + px.colors.qualitative.Bold + px.colors.qualitative.Safe\n",
    "    )\n",
    "}\n",
    "\n",
    "# 2. PCA (Plotly)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create DataFrame for Plotly\n",
    "df_pca = pd.DataFrame({\n",
    "    'Component 1': X_pca[:, 0],\n",
    "    'Component 2': X_pca[:, 1],\n",
    "    'Label': labels.astype(str)\n",
    "})\n",
    "\n",
    "fig_pca = px.scatter(\n",
    "    df_pca, \n",
    "    x='Component 1', \n",
    "    y='Component 2', \n",
    "    color='Label',\n",
    "    color_discrete_map=distinct_colors,\n",
    "    opacity=0.7,\n",
    "    title='2D PCA of Audio Features'\n",
    ")\n",
    "\n",
    "fig_pca.update_layout(\n",
    "        margin=dict(l=20, r=220, t=60, b=20),  # More right margin\n",
    "        legend=dict(\n",
    "            orientation=\"v\",\n",
    "            yanchor=\"top\",\n",
    "            y=1,\n",
    "            xanchor=\"left\",\n",
    "            x=1,  # Push legend further right outside the plot\n",
    "            font=dict(size=12),\n",
    "            itemwidth=40,  # Force horizontal space per legend item\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig_pca.show()\n",
    "fig_pca.write_html(\"pca_projection.html\")\n",
    "\n",
    "# 3. UMAP (Plotly)\n",
    "reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "X_umap = reducer.fit_transform(X_scaled)\n",
    "\n",
    "# Create DataFrame for Plotly\n",
    "df_umap = pd.DataFrame({\n",
    "    'Component 1': X_umap[:, 0],\n",
    "    'Component 2': X_umap[:, 1],\n",
    "    'Label': labels.astype(str)\n",
    "})\n",
    "\n",
    "fig_umap = px.scatter(\n",
    "    df_umap, \n",
    "    x='Component 1', \n",
    "    y='Component 2', \n",
    "    color='Label',\n",
    "    color_discrete_map=distinct_colors,\n",
    "    opacity=0.7,\n",
    "    title='UMAP Projection of Audio Features'\n",
    ")\n",
    "\n",
    "fig_umap.update_layout(\n",
    "        margin=dict(l=20, r=220, t=60, b=20),  # More right margin\n",
    "        legend=dict(\n",
    "            orientation=\"v\",\n",
    "            yanchor=\"top\",\n",
    "            y=1,\n",
    "            xanchor=\"left\",\n",
    "            x=1,  # Push legend further right outside the plot\n",
    "            font=dict(size=12),\n",
    "            itemwidth=40,  # Force horizontal space per legend item\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig_umap.show()\n",
    "fig_umap.write_html(\"umap_projection.html\")\n",
    "\n",
    "# 4. t-SNE (Plotly)\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "\n",
    "# Create DataFrame for Plotly\n",
    "df_tsne = pd.DataFrame({\n",
    "    'Component 1': X_tsne[:, 0],\n",
    "    'Component 2': X_tsne[:, 1],\n",
    "    'Label': labels.astype(str)\n",
    "})\n",
    "\n",
    "fig_tsne = px.scatter(\n",
    "    df_tsne, \n",
    "    x='Component 1', \n",
    "    y='Component 2', \n",
    "    color='Label',\n",
    "    color_discrete_map=distinct_colors,\n",
    "    opacity=0.7,\n",
    "    title='t-SNE Projection of Audio Features'\n",
    ")\n",
    "\n",
    "fig_tsne.update_layout(\n",
    "        margin=dict(l=20, r=220, t=60, b=20),  # More right margin\n",
    "        legend=dict(\n",
    "            orientation=\"v\",\n",
    "            yanchor=\"top\",\n",
    "            y=1,\n",
    "            xanchor=\"left\",\n",
    "            x=1,  # Push legend further right outside the plot\n",
    "            font=dict(size=12),\n",
    "            itemwidth=40,  # Force horizontal space per legend item\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig_tsne.show()\n",
    "fig_tsne.write_html(\"tsne_projection.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"PitchVar\", \"MFCC-1\", \"MFCC-2\", \"MFCC-3\"]\n",
    "\n",
    "# Prepare MFCC data\n",
    "mfccs = [\"MFCC-\" + str(i) for i in range(1, 4)]\n",
    "X = df[mfccs].to_numpy()\n",
    "labels = df[\"Label\"].to_numpy()\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Create a distinct color map for the labels\n",
    "unique_labels = np.unique(labels)\n",
    "# Distinct color palette - high contrast colors that are visually distinguishable\n",
    "distinct_colors = {\n",
    "    # Vibrant, distinct colors that work well for visualization\n",
    "    label: color for label, color in zip(\n",
    "        unique_labels, \n",
    "        px.colors.qualitative.D3 + px.colors.qualitative.Bold + px.colors.qualitative.Safe\n",
    "    )\n",
    "}\n",
    "\n",
    "# Helper to make 3D plot and save HTML with distinct colors\n",
    "def save_3d_plot(X_3d, labels, title, filename):\n",
    "    # Create a dataframe for plotly\n",
    "    import pandas as pd\n",
    "    plot_df = pd.DataFrame({\n",
    "        'x': X_3d[:, 0],\n",
    "        'y': X_3d[:, 1],\n",
    "        'z': X_3d[:, 2],\n",
    "        'Label': labels.astype(str)\n",
    "    })\n",
    "    \n",
    "    fig = px.scatter_3d(\n",
    "        plot_df,\n",
    "        x='x', y='y', z='z',\n",
    "        color='Label',\n",
    "        color_discrete_map=distinct_colors,  # Apply the custom color mapping\n",
    "        title=title,\n",
    "        labels={\"x\": \"Component 1\", \"y\": \"Component 2\", \"z\": \"Component 3\"},\n",
    "        opacity=0.7\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        margin=dict(l=20, r=220, t=60, b=20),  # More right margin\n",
    "        legend=dict(\n",
    "            orientation=\"v\",\n",
    "            yanchor=\"top\",\n",
    "            y=1,\n",
    "            xanchor=\"left\",\n",
    "            x=1,  # Push legend further right outside the plot\n",
    "            font=dict(size=12),\n",
    "            itemwidth=40,  # Force horizontal space per legend item\n",
    "        )\n",
    "    )\n",
    "    fig.show()\n",
    "    fig.write_html(f\"../checkpoint3/website/clarity/images/{filename}\")\n",
    "\n",
    "# 2. PCA (3D)\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "save_3d_plot(X_pca, labels, \"3D PCA of Audio Features\", \"pca_3d.html\")\n",
    "\n",
    "# 3. UMAP (3D)\n",
    "reducer = umap.UMAP(n_components=3, random_state=42)\n",
    "X_umap = reducer.fit_transform(X_scaled)\n",
    "save_3d_plot(X_umap, labels, \"3D UMAP Projection of Audio Features\", \"umap_3d.html\")\n",
    "\n",
    "# 4. t-SNE (3D)\n",
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "save_3d_plot(X_tsne, labels, \"3D t-SNE Projection of Audio Features\", \"tsne_3d.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting MFCCs and Pitch variance as features for 2d projection \n",
    "features = [\"PitchVar\", \"MFCC-1\", \"MFCC-2\", \"MFCC-3\"]\n",
    "corr = df[features].corr()\n",
    "\n",
    "# Correlation matrix\n",
    "plt.figure(figsize=(16, 16))  \n",
    "plt.subplot(221)\n",
    "\n",
    "# Heat map\n",
    "sns.heatmap(data=corr, annot=True, cmap=\"coolwarm\", xticklabels=features, yticklabels=features)\n",
    "plt.title(\"Correlations between Features\")\n",
    "\n",
    "# Prepare data for PCA, UMAP, and t-SNE\n",
    "mfccs = [\"MFCC-\" + str(i) for i in range(1, 4)]\n",
    "X = df[mfccs].to_numpy()\n",
    "labels = df[\"Label\"].to_numpy()\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# UMAP\n",
    "reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "X_umap = reducer.fit_transform(X_scaled)\n",
    "\n",
    "# t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "\n",
    "# Plot PCA\n",
    "plt.subplot(222)\n",
    "unique_labels = np.unique(labels)\n",
    "for label in unique_labels:\n",
    "    idx = labels == label\n",
    "    plt.scatter(X_pca[idx, 0], X_pca[idx, 1], label=label, alpha=0.6)\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.title(\"2D PCA of Audio Features\")\n",
    "plt.legend(loc=\"best\", fontsize=\"small\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot UMAP\n",
    "plt.subplot(223)\n",
    "for label in unique_labels:\n",
    "    idx = labels == label\n",
    "    plt.scatter(X_umap[idx, 0], X_umap[idx, 1], label=label, alpha=0.6)\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "plt.title(\"UMAP Projection of Audio Features\")\n",
    "plt.legend(loc=\"best\", fontsize=\"small\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot t-SNE\n",
    "plt.subplot(224)\n",
    "for label in unique_labels:\n",
    "    idx = labels == label\n",
    "    plt.scatter(X_tsne[idx, 0], X_tsne[idx, 1], label=label, alpha=0.6)\n",
    "plt.xlabel(\"t-SNE Component 1\")\n",
    "plt.ylabel(\"t-SNE Component 2\")\n",
    "plt.title(\"t-SNE Projection of Audio Features\")\n",
    "plt.legend(loc=\"best\", fontsize=\"small\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction & Hypothesis Testing\n",
    "\n",
    "In this section, we extract acoustic features from vocalizations and statistically evaluate whether they differ significantly across expression labels. These tests aim to determine if vocal cues such as **pitch variability**, and **spectral shape** (MFCCs) carry meaningful information that can distinguish between intents like `\"yes\"` and `\"no\"`.\n",
    "\n",
    "---\n",
    "\n",
    "### Tests Conducted:\n",
    "1. **Pitch Variability** – Mann-Whitney U test on pitch standard deviation across samples.\n",
    "2. **MFCC Differences** – Mann-Whitney U test on mean MFCC coefficients (1–3).\n",
    "3. **Spectral Entropy Differences** - ANOVA + Ad-Hoc Pairwise T tests on Spectral Entropy.\n",
    "\n",
    "---\n",
    "\n",
    "### Results:\n",
    "\n",
    "Our statistical tests revealed **significant acoustic differences** between `\"yes\"` and `\"no\"` vocalizations:\n",
    "\n",
    "- **Pitch Variability**:\n",
    "  - \"No\" vocalizations showed **much higher pitch variability** (std = 119.13) compared to \"Yes\" (std = 22.46).\n",
    "  - Mann-Whitney U test confirmed this with **p < 0.001** and a large effect size (**Cohen’s d = -2.38**).\n",
    "\n",
    "- **MFCCs (Spectral Shape)**:\n",
    "  - Significant differences were found in **MFCC-1** and **MFCC-3** (both **p < 0.001**, Cohen’s d > 1.6), indicating strong differences in **spectral slope** and **fine spectral variation**.\n",
    "  - **MFCC-2** showed **no significant difference**, suggesting similar mid-frequency emphasis in both groups.\n",
    "\n",
    "- **Spectral Entropy**:\n",
    "  - Significant differences in spectral entropy were found between various vocalization labels, indicating that certain emotional states could be distinguished based on their spectral characteristics.\n",
    "  - Strong entropy differences were notably present between \"dysregulated\" and \"delighted\", and between \"selftalk\" and \"frustrated\", highlighting that spectral entropy can be a useful feature in distinguishing emotional states in vocalizations.\n",
    "\n",
    "\n",
    "These findings suggest that both **pitch dynamics** and **spectral shape** are promising features for distinguishing vocal intent in non-verbal utterances for the model development phase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Pitch Variability Differences between \"Yes\" and \"No\" Vocalizations (Mann-Whitney U Test)\n",
    "\n",
    "This test evaluates whether there are statistically significant differences in **pitch variability** between vocalizations labeled as `\"yes\"` and `\"no\"`. Pitch variability is measured as the **standard deviation of estimated pitch (f₀)** across time for each audio sample. This metric reflects how much the speaker's pitch varies within a vocalization type, often tied to emotional expressiveness or vocal intent.\n",
    "\n",
    "#### What is Pitch Variability?\n",
    "\n",
    "- Calculated using **Librosa's PYIN algorithm**, which estimates fundamental frequency (f₀) for voiced segments of an audio signal.\n",
    "- We then compute the **standard deviation** of those f₀ values per sample.\n",
    "- A **higher pitch std** generally means more variation in tone, while a lower std suggests more monotonic vocalization.\n",
    "\n",
    "#### Test Setup\n",
    "\n",
    "- **Statistic**: Mann-Whitney U test (non-parametric)\n",
    "- **Effect Size**: Cohen’s *d*\n",
    "- **Input Feature**: Standard deviation of pitch per sample\n",
    "- **Groups Compared**: `\"yes\"` vs `\"no\"` vocalizations\n",
    "- **Sample Size**: 100 samples for \"yes\", 12 samples for \"no\"\n",
    "- **Alpha**: Will use a significance level of 0.05 \n",
    "\n",
    "---\n",
    "\n",
    "### Null Hypothesis (H₀):\n",
    "- There is **no difference** in pitch variability between vocalizations labeled as \"yes\" and \"no\". The distributions of pitch standard deviation are the same for both groups.\n",
    "\n",
    "### Alternative Hypothesis (H₁):\n",
    "- There is a **difference** in pitch variability between vocalizations labeled as \"yes\" and \"no\". The distributions of pitch standard deviation are not the same for both groups, indicating that one group may exhibit more pitch variation than the other.\n",
    "\n",
    "\n",
    "#### Group Means & Standard Deviations\n",
    "\n",
    "| Label      | Pitch Std (Mean ± Std) |\n",
    "|------------|------------------------|\n",
    "| **Yes**    | 19.818 ± 20.91          |\n",
    "| **No**     | 114.964 ± 110.339        |\n",
    "\n",
    "#### Statistical Results Summary\n",
    "\n",
    "| Metric             | Value                         |\n",
    "|--------------------|-------------------------------|\n",
    "| **U Statistic**     | 260.0                         |\n",
    "| **p-value**         | 0.001                       |\n",
    "| **Cohen’s d**       | -2.370                         |\n",
    "| **Mean Difference** | -95.147                        |\n",
    "| **Significant**     |   Yes                         |\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "Since our p = 0.01 < alpha, we __reject__ the null hypothesis. We interpret that:\n",
    "\n",
    "- The **\"no\"** vocalizations exhibit **dramatically higher pitch variability** than \"yes\" samples — almost **5× higher on average**.\n",
    "- The test yields a **low p-value (0.01)** and a **large negative effect size (Cohen’s d = -2.38)**, indicating a strong and statistically significant difference.\n",
    "- This suggests that **pitch dynamics** could be a powerful feature in differentiating certain types of vocal intent, especially when classifying expressive vs. flat responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_pitch_extraction(audio_list: List,\n",
    "                           max_samples_per_batch: int=50,\n",
    "                           sr: int=SAMPLE_RATE) -> List[float]:\n",
    "    # Randomly sample if batch is too large\n",
    "    if len(audio_list) > max_samples_per_batch:\n",
    "        sample_indices = np.random.choice(len(audio_list), max_samples_per_batch, replace=False)\n",
    "        audio_list = [audio_list[i] for i in sample_indices]\n",
    "    \n",
    "    pitch_stds = []\n",
    "    for audio_array in audio_list:\n",
    "        audio_array = np.asarray(audio_array, dtype=np.float64)\n",
    "        \n",
    "        # Extract pitch using PYIN\n",
    "        f0, voiced_flag, _ = librosa.pyin(\n",
    "            audio_array, \n",
    "            fmin=librosa.note_to_hz('C2'),\n",
    "            fmax=librosa.note_to_hz('C7'),\n",
    "            sr=sr\n",
    "        )\n",
    "        \n",
    "        # Filter for voiced segments\n",
    "        f0_voiced = f0[voiced_flag]\n",
    "        \n",
    "        # Calculate pitch std, handle empty case\n",
    "        pitch_std = float(np.std(f0_voiced)) if len(f0_voiced) > 0 else 0.0\n",
    "        pitch_stds.append(pitch_std)\n",
    "    \n",
    "    return pitch_stds\n",
    "\n",
    "def pitch_variability_test(df: pl.DataFrame,\n",
    "                           max_batch_size: int=50,\n",
    "                           target_labels: List[str]=['frustrated', 'delighted']) -> Dict[str, float]:\n",
    "    # Group audio by label\n",
    "    label_audio_groups = {}\n",
    "    for label in target_labels:\n",
    "        # Extract audio for each label\n",
    "        label_audio_groups[label] = df.filter(pl.col(\"Label\") == label)[\"Audio\"].to_list()\n",
    "    \n",
    "    # Batch pitch extraction\n",
    "    label_pitch_stds = {}\n",
    "    for label, audio_list in label_audio_groups.items():\n",
    "        label_pitch_stds[label] = batch_pitch_extraction(audio_list=audio_list, max_samples_per_batch=max_batch_size)\n",
    "        \n",
    "        # Print basic stats\n",
    "        pitch_array = np.array(label_pitch_stds[label])\n",
    "        print(f\"{label} samples: {len(pitch_array)}\")\n",
    "        print(f\"  Mean pitch std: {np.mean(pitch_array):.4f}\")\n",
    "        print(f\"  Std of pitch std: {np.std(pitch_array):.4f}\")\n",
    "    \n",
    "    # Perform statistical tests\n",
    "    label1_data = label_pitch_stds[target_labels[0]]\n",
    "    label2_data = label_pitch_stds[target_labels[1]]\n",
    "    \n",
    "    # Mann-Whitney U Test\n",
    "    u_statistic, p_value = scipy.stats.mannwhitneyu(\n",
    "        label1_data, \n",
    "        label2_data, \n",
    "        alternative='two-sided'\n",
    "    )\n",
    "    \n",
    "    # Effect size calculation (Cohen's d)\n",
    "    mean1, std1 = np.mean(label1_data), np.std(label1_data)\n",
    "    mean2, std2 = np.mean(label2_data), np.std(label2_data)\n",
    "    \n",
    "    # Pooled standard deviation\n",
    "    pooled_std = np.sqrt(((len(label1_data) - 1) * std1**2 + \n",
    "                          (len(label2_data) - 1) * std2**2) / \n",
    "                         (len(label1_data) + len(label2_data) - 2))\n",
    "    \n",
    "    # Cohen's d\n",
    "    cohens_d = (mean1 - mean2) / pooled_std\n",
    "    \n",
    "    # Prepare results\n",
    "    results = {\n",
    "        'Mann-Whitney U Statistic': u_statistic,\n",
    "        'p-value': p_value,\n",
    "        'Cohen\\'s d': cohens_d,\n",
    "        'Mean Difference': mean1 - mean2,\n",
    "        'Significant': p_value < 0.05\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n=== Hypothesis Test Results ===\")\n",
    "    for key, value in results.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectogram plotting functions to compare labels \n",
    "def plot_spectrogram_comparison(df, label1=\"yes\", label2=\"no\", sr=SAMPLE_RATE, n_examples=2):\n",
    "    fig, axes = plt.subplots(n_examples, 2, figsize=(12, 4 * n_examples))\n",
    "    label_map = {0: label1, 1: label2}\n",
    "\n",
    "    for i, label in enumerate([label1, label2]):\n",
    "        examples = df.filter(pl.col(\"Label\") == label).head(n_examples).iter_rows(named=True)\n",
    "        for j, row in enumerate(examples):\n",
    "            y = np.array(row[\"Audio\"])\n",
    "            S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "            S_db = librosa.power_to_db(S, ref=np.max)\n",
    "            ax = axes[j, i] if n_examples > 1 else axes[i]\n",
    "            librosa.display.specshow(S_db, sr=sr, x_axis='time', y_axis='mel', ax=ax)\n",
    "            ax.set_title(f\"{label_map[i].upper()} Sample #{j+1}\")\n",
    "            ax.set_xlabel(\"\")\n",
    "            ax.set_ylabel(\"\")\n",
    "\n",
    "    plt.suptitle(\"Mel Spectrogram Comparison: YES vs NO\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "results = pitch_variability_test(df=df, max_batch_size=100, target_labels=[\"yes\", \"no\"])\n",
    "print(f\"\\n🎶 Pitch Variability Test completed in {time.time() - t0:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **\"YES\" Samples**: Show varied frequency patterns with bright spots indicating dynamic changes in pitch.\n",
    "- **\"NO\" Samples**: Display consistent and flat energy patterns with fewer changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrogram_comparison(df, label1=\"yes\", label2=\"no\", sr=SAMPLE_RATE, n_examples=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_plotly_spectrogram_comparison(df, label1=\"yes\", label2=\"no\", sr=SAMPLE_RATE, n_examples=4, output_dir=\"comparison_spectrograms\"):\n",
    "    \"\"\"\n",
    "    Generate side-by-side Plotly spectrograms comparing 'yes' and 'no' labels and save as HTML files.\n",
    "    \n",
    "    Args:\n",
    "        df: Polars DataFrame containing 'Label' and 'Audio' columns\n",
    "        label1: First label to compare (default: \"yes\")\n",
    "        label2: Second label to compare (default: \"no\")\n",
    "        sr: Sample rate (default: 16000)\n",
    "        n_examples: Number of examples to plot (default: 4)\n",
    "        output_dir: Directory to save the HTML files (will be created if it doesn't exist)\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get examples for each label\n",
    "    label1_examples = df.filter(pl.col(\"Label\") == label1).head(n_examples).to_pandas()\n",
    "    label2_examples = df.filter(pl.col(\"Label\") == label2).head(n_examples).to_pandas()\n",
    "    \n",
    "    # Create a separate HTML file for each pair of examples\n",
    "    for j in range(n_examples):\n",
    "        if j < len(label1_examples) and j < len(label2_examples):\n",
    "            # Create subplot figure with 1 row and 2 columns\n",
    "            fig = make_subplots(\n",
    "                rows=1, cols=2,\n",
    "                subplot_titles=(f\"{label1.upper()} Sample #{j+1}\", f\"{label2.upper()} Sample #{j+1}\")\n",
    "            )\n",
    "            \n",
    "            # Process first label example\n",
    "            y1 = np.array(label1_examples.iloc[j][\"Audio\"])\n",
    "            S1 = librosa.feature.melspectrogram(y=y1, sr=sr, n_mels=128)\n",
    "            S_db1 = librosa.power_to_db(S1, ref=np.max)\n",
    "            \n",
    "            # Process second label example\n",
    "            y2 = np.array(label2_examples.iloc[j][\"Audio\"])\n",
    "            S2 = librosa.feature.melspectrogram(y=y2, sr=sr, n_mels=128)\n",
    "            S_db2 = librosa.power_to_db(S2, ref=np.max)\n",
    "            \n",
    "            # Add spectrograms as heatmaps\n",
    "            fig.add_trace(\n",
    "                go.Heatmap(\n",
    "                    z=S_db1,\n",
    "                    colorscale='viridis',\n",
    "                    colorbar=dict(title=\"dB\", x=0.46),\n",
    "                    name=label1.upper()\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Heatmap(\n",
    "                    z=S_db2,\n",
    "                    colorscale='viridis',\n",
    "                    colorbar=dict(title=\"dB\", x=1.0),\n",
    "                    name=label2.upper()\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "            \n",
    "            # Update layout\n",
    "            fig.update_layout(\n",
    "                title_text=f\"Mel Spectrogram Comparison: {label1.upper()} vs {label2.upper()} (Sample #{j+1})\",\n",
    "                height=600,\n",
    "                width=1800\n",
    "            )\n",
    "            \n",
    "            # Update axes\n",
    "            fig.update_xaxes(title_text=\"Time\", row=1, col=1)\n",
    "            fig.update_xaxes(title_text=\"Time\", row=1, col=2)\n",
    "            fig.update_yaxes(title_text=\"Mel Frequency\", row=1, col=1)\n",
    "            \n",
    "            # Save as HTML file\n",
    "            output_file = os.path.join(output_dir, f\"comparison_{label1}_vs_{label2}_sample{j+1}.html\")\n",
    "            fig.show()\n",
    "            fig.write_html(output_file)\n",
    "            print(f\"Saved {output_file}\")\n",
    "    \n",
    "    print(f\"All comparison spectrograms saved to '{output_dir}' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_plotly_spectrogram_comparison(df, label1=\"yes\", label2=\"no\", sr=SAMPLE_RATE, n_examples=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Mel Frequency Cepstral Coefficients (MFCCs) Mean Differences between \"Yes\" and \"No\" Vocalizations (Pairwise Mann-Whitney U Test)\n",
    "\n",
    "This test evaluates whether there are statistically significant differences in **spectral shape** between the vocalizations labeled as `\"yes\"` and `\"no\"`, focusing on the **mean values of the first three MFCCs**.\n",
    "\n",
    "#### What are MFCCs?\n",
    "\n",
    "- **MFCC-1**: Captures the overall **spectral slope** — indicates the energy balance between low and high frequencies.\n",
    "- **MFCC-2**: Captures the **curvature** of the spectral envelope — flat vs. peaked energy in the mid frequencies.\n",
    "- **MFCC-3**: Represents **fine-grained variation** — subtle changes or \"ripples\" in the spectral shape.\n",
    "- Higher-order MFCCs (4, 5, …) capture increasingly localized detail and high-frequency texture.\n",
    "\n",
    "---\n",
    "\n",
    "#### Test Setup\n",
    "\n",
    "- **Statistic**: Pairwise Mann-Whitney U test, a non-parametric test used to compare differences between two independent groups when the data cannot be assumed to be normally distributed.\n",
    "- **Effect Size**: Cohen’s *d*\n",
    "- **Input Features**: Mean of MFCC-1, MFCC-2, and MFCC-3 per sample\n",
    "- **Groups Compared**: `\"yes\"` vs `\"no\"` vocalizations\n",
    "- **Alpha**: We will use a significance level of 0.05\n",
    "\n",
    "---\n",
    "\n",
    "### Null Hypothesis (H₀):\n",
    "- There are **no significant differences** in the mean values of the first three MFCCs (MFCC-1, MFCC-2, and MFCC-3) between vocalizations labeled as \"yes\" and \"no\". The distributions of these spectral shape measures are the same across both groups.\n",
    "\n",
    "### Alternative Hypothesis (H₁):\n",
    "- There are **significant differences** in the mean values of the first three MFCCs (MFCC-1, MFCC-2, and MFCC-3) between vocalizations labeled as \"yes\" and \"no\". The distributions of these spectral shape measures vary between the two groups, indicating discriminative spectral characteristics.\n",
    "\n",
    "#### Results\n",
    "\n",
    "__Group Means & Standard Deviations__\n",
    "\n",
    "| Label | MFCC-1 Mean ± Std | MFCC-2 Mean ± Std | MFCC-3 Mean ± Std |\n",
    "|-------|-------------------|-------------------|-------------------|\n",
    "| **Yes** | -323.365 ± 32.903 | 122.607 ± 18.428 | -16.314 ± 15.998 |\n",
    "| **No**  | -262.130 ± 42.377 | 114.124 ± 25.801 | -52.627 ± 27.668 |\n",
    "\n",
    "__Statistical Results Summary__\n",
    "\n",
    "| MFCC      | U Statistic | p-value        | Cohen’s *d* | Mean Diff | Significant |\n",
    "|-----------|-------------|----------------|-------------|-----------|-------------|\n",
    "| **MFCC-1** | 158.0       | 3.277e-05      | -1.803      | -61.235   | Yes         |\n",
    "| **MFCC-2** | 708.0       | 0.312          | 0.440       | 8.483     | No          |\n",
    "| **MFCC-3** | 1048.0      | 2.557e-05      | 2.073       | 36.312    | Yes         |\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "Since our p value for MFCC-1 (p = 3.277e-05) and MFCC-3 (p = 2.557e-05) are below alpha = 0.05, we reject the null hypothesis for MFCC-1 and MFCC-3. This indicates there are signficant differences between spectral slope and fine-grain variation between different labels, aiding in discerning label groups and serving as important features. \n",
    "\n",
    "- **MFCC-1** and **MFCC-3** show statistically significant differences between \"yes\" and \"no\" samples, with Cohen’s *d* indicating large effect sizes, which suggests these features are powerful discriminators.\n",
    "- **MFCC-2** does not show a statistically significant difference, suggesting it might not carry as much discriminative power between these two vocalization types.\n",
    "- The results lead to the rejection of the null hypothesis for MFCC-1 and MFCC-3, confirming significant differences in spectral shape that could be instrumental in classifying or clustering intent in non-verbal vocalizations.\n",
    "- These results indicate that **low and mid-frequency spectral properties** (slope and variation) carry meaningful differences between vocalizations, while overall mid-frequency curvature does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_mfcc_extraction(audio_list: List,\n",
    "                          max_samples_per_batch: int=50,\n",
    "                          sr: int=SAMPLE_RATE,\n",
    "                          n_coeffs: int=3) -> List[float]:\n",
    "    if len(audio_list) > max_samples_per_batch:\n",
    "        sample_indices = np.random.choice(len(audio_list), max_samples_per_batch, replace=False)\n",
    "        audio_list = [audio_list[i] for i in sample_indices]\n",
    "    \n",
    "    mfcc_means = []\n",
    "    for audio_array in audio_list:\n",
    "        audio_array = np.asarray(audio_array, dtype=np.float32)\n",
    "        mfccs = librosa.feature.mfcc(y=audio_array, sr=sr, n_mfcc=n_coeffs)\n",
    "        mfcc_mean = np.mean(mfccs, axis=1)\n",
    "        mfcc_means.append(mfcc_mean)\n",
    "    \n",
    "    return mfcc_means\n",
    "\n",
    "def mfcc_significance_test(df, max_batch_size=50, target_labels=[\"frustrated\", \"delighted\"], n_coeffs=3):\n",
    "    label_audio_groups = {}\n",
    "    for label in target_labels:\n",
    "        label_audio_groups[label] = df.filter(pl.col(\"Label\") == label)[\"Audio\"].to_list()\n",
    "    \n",
    "    label_mfcc_means = {}\n",
    "    for label, audio_list in label_audio_groups.items():\n",
    "        label_mfcc_means[label] = batch_mfcc_extraction(\n",
    "            audio_list,\n",
    "            max_samples_per_batch=max_batch_size,\n",
    "            n_coeffs=n_coeffs,\n",
    "            sr=SAMPLE_RATE\n",
    "        )\n",
    "        mfcc_array = np.array(label_mfcc_means[label])\n",
    "        print(f\"{label} samples: {len(mfcc_array)}\")\n",
    "        \n",
    "        for i in range(n_coeffs):\n",
    "            print(f\"  MFCC-{i+1} Mean: {np.mean(mfcc_array[:, i]):.4f}, Std: {np.std(mfcc_array[:, i]):.4f}\")\n",
    "\n",
    "    results = {}\n",
    "    for i in range(n_coeffs):\n",
    "        data1 = [x[i] for x in label_mfcc_means[target_labels[0]]]\n",
    "        data2 = [x[i] for x in label_mfcc_means[target_labels[1]]]\n",
    "\n",
    "        u_statistic, p_value = scipy.stats.mannwhitneyu(data1, data2, alternative='two-sided')\n",
    "        mean1, std1 = np.mean(data1), np.std(data1)\n",
    "        mean2, std2 = np.mean(data2), np.std(data2)\n",
    "\n",
    "        pooled_std = np.sqrt(((len(data1) - 1) * std1**2 + (len(data2) - 1) * std2**2) /\n",
    "                             (len(data1) + len(data2) - 2))\n",
    "        cohens_d = (mean1 - mean2) / pooled_std\n",
    "\n",
    "        results[f\"MFCC-{i+1}\"] = {\n",
    "            'U Statistic': u_statistic,\n",
    "            'p-value': p_value,\n",
    "            'Cohen\\'s d': cohens_d,\n",
    "            'Mean Difference': mean1 - mean2,\n",
    "            'Significant': p_value < 0.05\n",
    "        }\n",
    "\n",
    "    print(\"\\n=== MFCC Significance Test Results ===\")\n",
    "    for k, v in results.items():\n",
    "        print(f\"\\n{k}\")\n",
    "        for stat, val in v.items():\n",
    "            print(f\"  {stat}: {val}\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_and_heat_mfcc_comparison(df, labels=[\"yes\", \"no\"], sr=22050, n_mfcc=3):\n",
    "    # Step 1: Prepare data\n",
    "    data = []\n",
    "    mfcc_data = {label: [] for label in labels}\n",
    "\n",
    "    for label in labels:\n",
    "        for row in df.filter(pl.col(\"Label\") == label).iter_rows(named=True):\n",
    "            y = np.array(row[\"Audio\"])\n",
    "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "            mfcc_mean = np.mean(mfccs, axis=1)\n",
    "\n",
    "            # For boxplot\n",
    "            for i in range(n_mfcc):\n",
    "                data.append({\n",
    "                    \"MFCC\": f\"MFCC-{i+1}\",\n",
    "                    \"Value\": float(mfcc_mean[i]),\n",
    "                    \"Label\": label\n",
    "                })\n",
    "\n",
    "            # For heatmap\n",
    "            mfcc_data[label].append(mfcc_mean)\n",
    "\n",
    "    # Convert to pandas DataFrame for Plotly\n",
    "    df_plot = pd.DataFrame(data)\n",
    "\n",
    "    # Prepare heatmap data\n",
    "    heat_data = []\n",
    "    for label in labels:\n",
    "        means = np.mean(np.stack(mfcc_data[label]), axis=0)\n",
    "        row = [float(means[i]) for i in range(n_mfcc)]\n",
    "        heat_data.append(row)\n",
    "\n",
    "    # Color mapping\n",
    "    colors = px.colors.qualitative.D3[:len(labels)]\n",
    "    color_dict = {label: color for label, color in zip(labels, colors)}\n",
    "\n",
    "    # Step 2: Create Boxplot Figure\n",
    "    box_fig = go.Figure()\n",
    "    for label in labels:\n",
    "        label_data = df_plot[df_plot[\"Label\"] == label]\n",
    "        box_fig.add_trace(\n",
    "            go.Box(\n",
    "                x=label_data[\"MFCC\"],\n",
    "                y=label_data[\"Value\"],\n",
    "                name=label,\n",
    "                marker_color=color_dict[label],\n",
    "                boxmean=True\n",
    "            )\n",
    "        )\n",
    "    box_fig.update_layout(\n",
    "        title=\"MFCC Distribution (Boxplot)\",\n",
    "        xaxis_title=\"MFCC Coefficient\",\n",
    "        yaxis_title=\"Value\",\n",
    "        boxmode='group',\n",
    "        margin=dict(l=20, r=220, t=60, b=20),  # More right margin\n",
    "        legend=dict(\n",
    "            orientation=\"v\",\n",
    "            yanchor=\"top\",\n",
    "            y=1,\n",
    "            xanchor=\"left\",\n",
    "            x=1,  # Push legend further right outside the plot\n",
    "            font=dict(size=12),\n",
    "            itemwidth=40,  # Force horizontal space per legend item\n",
    "        ),\n",
    "        width=800,\n",
    "        height=600\n",
    "    )\n",
    "\n",
    "    # Step 3: Create Heatmap Figure\n",
    "    heatmap_fig = go.Figure(\n",
    "        go.Heatmap(\n",
    "            z=heat_data,\n",
    "            x=[f\"MFCC-{i+1}\" for i in range(n_mfcc)],\n",
    "            y=[label.upper() for label in labels],\n",
    "            colorscale='Viridis',\n",
    "            text=[[f\"{val:.1f}\" for val in row] for row in heat_data],\n",
    "            texttemplate=\"%{text}\",\n",
    "            colorbar=dict(title=\"Value\")\n",
    "        )\n",
    "    )\n",
    "    heatmap_fig.update_layout(\n",
    "        title=\"MFCC Mean Comparison (Heatmap)\",\n",
    "        xaxis_title=\"MFCC Coefficient\",\n",
    "        yaxis_title=\"Label\",\n",
    "        margin=dict(l=20, r=220, t=60, b=20),  # More right margin\n",
    "        legend=dict(\n",
    "            orientation=\"v\",\n",
    "            yanchor=\"top\",\n",
    "            y=1,\n",
    "            xanchor=\"left\",\n",
    "            x=1,  # Push legend further right outside the plot\n",
    "            font=dict(size=12),\n",
    "            itemwidth=40,  # Force horizontal space per legend item\n",
    "        ),\n",
    "        width=800,\n",
    "        height=600\n",
    "    )\n",
    "\n",
    "    box_fig.show()\n",
    "    box_fig.write_html(\"mfcc_boxplot.html\")\n",
    "    heatmap_fig.show()\n",
    "    heatmap_fig.write_html(\"mfcc_heatmap.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "results = mfcc_significance_test(df, max_batch_size=100, target_labels=[\"yes\", \"no\"], n_coeffs=3)\n",
    "print(f\"\\n🎛️ MFCC Significance Test completed in {time.time() - t0:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visual representations show the distribution and mean comparisons of the first three MFCCs for vocalizations labeled \"Yes\" and \"No\". The boxplot illustrates variability within each MFCC across the two vocalization types, while the heatmap provides a direct comparison of their mean values (easier for the eye to notice the notable differences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_and_heat_mfcc_comparison(df, labels=[\"yes\", \"no\"], n_mfcc=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3: Spectral Entropy Differences Across Vocalization Labels (ANOVA & T-Tests)\n",
    "\n",
    "This analysis investigates whether there are statistically significant differences in **spectral entropy** between different vocalization labels (`\"dysregulated\"`, `\"hunger\"`, `\"delighted\"`).\n",
    "\n",
    "\n",
    "#### What is Spectral Entropy?\n",
    "\n",
    "Spectral entropy measures the **disorder** or **randomness** in an audio signal's frequency distribution. A higher entropy indicates a more uniform spectral distribution, while lower entropy suggests a more structured or tonal signal.\n",
    "\n",
    "---\n",
    "\n",
    "#### Test Setup\n",
    "\n",
    "- **Statistic**: One-way ANOVA & Pairwise T-tests\n",
    "- **Effect Size**: Cohen’s *d* (for pairwise comparisons)\n",
    "- **Input Feature**: Spectral entropy computed from short-time Fourier transform (STFT)\n",
    "- **Groups Compared**: `\"dysregulated\"`, `\"hunger\"`, `\"delighted\"`\n",
    "- **Sample Size**: Maximum of 100 samples per label\n",
    "\n",
    "---\n",
    "\n",
    "### Null Hypothesis (H₀):\n",
    "- There are **no significant differences** in spectral entropy among the vocalization labels `\"dysregulated\"`, `\"hunger\"`, and `\"delighted\"`. All groups exhibit similar entropy distributions.\n",
    "\n",
    "### Alternative Hypothesis (H₁):\n",
    "- There are **significant differences** in spectral entropy among the vocalization labels `\"dysregulated\"`, `\"hunger\"`, and `\"delighted\"`. At least one of these groups exhibits a different entropy distribution compared to the others.\n",
    "\n",
    "#### Results\n",
    "\n",
    "__ANOVA Results__\n",
    "\n",
    "| Test | F-Statistic | p-value | Significant |\n",
    "|------|------------|---------|-------------|\n",
    "| **Spectral Entropy** | 22.914 | 1.067e-13 | Yes |\n",
    "\n",
    "A significant ANOVA result suggests that at least one group has a different spectral entropy distribution.\n",
    "\n",
    "---\n",
    "\n",
    "#### Pairwise T-Test Summary\n",
    "\n",
    "| Comparison                 | T-Statistic | p-value      | Significant |\n",
    "|----------------------------|-------------|--------------|-------------|\n",
    "| **Dysregulated vs Selftalk** | -15.48      | 5.76e-50      | Yes         |\n",
    "| **Dysregulated vs Delighted** | -11.94      | 1.19e-31      | Yes         |\n",
    "| **Dysregulated vs Frustrated** | -2.10       | 0.036         | No          |\n",
    "| **Selftalk vs Delighted**     | +1.65       | 0.100         | No          |\n",
    "| **Selftalk vs Frustrated**    | +14.18      | 3.37e-44      | Yes         |\n",
    "| **Delighted vs Frustrated**   | +10.49      | 3.05e-25      | Yes         |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- **Strong differences** in spectral entropy are observed between:\n",
    "  - `\"dysregulated\"` and both `\"selftalk\"` & `\"delighted\"`\n",
    "  - `\"selftalk\"` and `\"frustrated\"`\n",
    "  - `\"delighted\"` and `\"frustrated\"`\n",
    "\n",
    "- No significant difference between:\n",
    "  - `\"dysregulated\"` and `\"frustrated\"`\n",
    "  - `\"selftalk\"` and `\"delighted\"`\n",
    "\n",
    "These results suggest that **spectral entropy can effectively differentiate between some vocal states**, particularly those on emotional extremes. However, overlaps exist, indicating entropy may not capture all acoustic nuance across labels.\n",
    "\n",
    "Given the significant results from the ANOVA (p = 1.067e-13) and most pairwise T-tests, we **reject the null hypothesis**. This indicates there are significant differences in spectral entropy among the vocalization labels tested.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spectral_entropy(y):\n",
    "    # Compute power spectral density\n",
    "    S = np.abs(librosa.stft(y))**2\n",
    "    \n",
    "    # Normalize each frame to create a probability distribution\n",
    "    S_norm = S / (np.sum(S, axis=0) + 1e-10)\n",
    "    \n",
    "    # Compute Shannon entropy using log base 2 (information-theoretic interpretation)\n",
    "    spectral_entropy = -np.sum(S_norm * np.log2(S_norm + 1e-10), axis=0)\n",
    "    \n",
    "    # Normalize by maximum possible entropy for the given frequency bins\n",
    "    max_entropy = np.log2(S.shape[0])  # log2(n_bins)\n",
    "    normalized_entropy = spectral_entropy / max_entropy\n",
    "    \n",
    "    return float(np.mean(normalized_entropy))\n",
    "\n",
    "def spectral_entropy_anova_test(df, target_labels=[\"dysregulated\", \"hunger\", \"delighted\"], max_samples_per_label=50):\n",
    "    \"\"\"Perform ANOVA on spectral entropy differences.\"\"\"\n",
    "    label_audio_groups = {label: df.filter(pl.col(\"Label\") == label)[\"Audio\"].to_list() for label in target_labels}\n",
    "    label_entropy_means = {label: [compute_spectral_entropy(np.array(y)) for y in audio_list[:max_samples_per_label]]\n",
    "                            for label, audio_list in label_audio_groups.items()}\n",
    "    data = [label_entropy_means[label] for label in target_labels]\n",
    "\n",
    "    # One-way ANOVA test\n",
    "    f_statistic, p_value = scipy.stats.f_oneway(*data)\n",
    "\n",
    "    results = {\n",
    "        'ANOVA F-Statistic': f_statistic,\n",
    "        'p-value': p_value,\n",
    "        'Significant': p_value < 0.05\n",
    "    }\n",
    "\n",
    "    print(\"\\n=== Spectral Entropy ANOVA Test Results ===\")\n",
    "    for key, value in results.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "target_labels = [\"dysregulated\", \"selftalk\", \"delighted\", \"frustrated\"]\n",
    "t0 = time.time()\n",
    "results = spectral_entropy_anova_test(df, target_labels=target_labels, max_samples_per_label=100)\n",
    "print(f\"\\nSpectral Entropy ANOVA Test completed in {time.time() - t0:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Must run ad-hoc tests to test which pairings are significantly different__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_t_test(df, target_labels, feature=\"Spectral Entropy\"):\n",
    "    label_data = {label: df.filter(pl.col(\"Label\") == label)[feature].to_list() for label in target_labels}\n",
    "\n",
    "    results = {}\n",
    "    alpha = 0.05 / (len(target_labels) * (len(target_labels) - 1) / 2)\n",
    "\n",
    "    for l1, l2 in list(combinations(target_labels, 2)):\n",
    "            data1, data2 = label_data[l1], label_data[l2]\n",
    "\n",
    "            t_statistic, p_value = scipy.stats.ttest_ind(data1, data2, equal_var=False)\n",
    "\n",
    "            results[f\"{l1} vs {l2}\"] = {\n",
    "                \"T-Statistic\": t_statistic,\n",
    "                \"P-Value\": p_value,\n",
    "                \"Significant\": p_value < alpha\n",
    "            }\n",
    "\n",
    "    return results\n",
    "\n",
    "def format_t_test_results(results_dict):\n",
    "    df_results = pd.DataFrame.from_dict(results_dict, orient=\"index\")\n",
    "    df_results.rename(columns={\"T-Statistic\": \"T-Statistic\", \"P-Value\": \"P-Value\", \"Significant\": \"Significant\"}, inplace=True)\n",
    "    \n",
    "    # Apply scientific notation for small p-values\n",
    "    df_results[\"P-Value\"] = df_results[\"P-Value\"].apply(lambda x: f\"{x:.15e}\" if x < 1e-5 else f\"{x:.15f}\")\n",
    "\n",
    "    print(\"\\n=== Pairwise T-Test Results ===\\n\")\n",
    "    print(df_results.to_string(index=True))\n",
    "\n",
    "def plot_spectral_entropy_comparison(df, target_labels, feature=\"Spectral Entropy\"):\n",
    "    data = [\n",
    "        {\"Label\": label, feature: val}\n",
    "        for label in target_labels\n",
    "        for val in df.filter(pl.col(\"Label\") == label)[feature].to_list()\n",
    "    ]\n",
    "    df_plot = pd.DataFrame(data)\n",
    "\n",
    "    fig = px.box(\n",
    "        df_plot,\n",
    "        x=\"Label\",\n",
    "        y=feature,\n",
    "        color=\"Label\",\n",
    "        title=f\"{feature} Comparison Across Labels\",\n",
    "        points=\"all\",\n",
    "        color_discrete_sequence=px.colors.qualitative.Set2\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        yaxis_title=feature,\n",
    "        xaxis_title=\"Label\",\n",
    "        width=1000,\n",
    "        height=600,\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "    fig.write_html(\"spectral_entropy_boxplot.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate spectral entropy for t test\n",
    "df = df.with_columns([\n",
    "    pl.col(\"Audio\").map_elements(lambda y: compute_spectral_entropy(np.array(y)), return_dtype=pl.Float64).alias(\"Spectral Entropy\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[\"Label\"].unique()\n",
    "t_test_results = pairwise_t_test(df, target_labels=target_labels, feature=\"Spectral Entropy\")\n",
    "format_t_test_results(t_test_results)\n",
    "plot_spectral_entropy_comparison(df, target_labels=target_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
