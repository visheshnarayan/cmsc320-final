{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c09b41d",
   "metadata": {},
   "source": [
    "### Assigned to __Deval and Eshan__\n",
    "\n",
    "#### Main goal: \n",
    "1. Use features to create some form of input vector appropriate to model used \n",
    "2. try to do: \n",
    "    - clustering over a projection (look at checkpoint 2 and how i used pca/umap/t-sne)\n",
    "    - SVMs on singular features (an array of only floats, don't input another array inside all your inputs as another feature); also see if you need to kernel trick or something \n",
    "    - ensemble method: have binary classifier for this label or not this label and trickle down\n",
    "        - YES or (NO or (HAPPY or ...)) kinda like if else in ocaml \n",
    "3. give me the dihta on confusion matrix and precision recall f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c711e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import umap\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6ecaaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (8102, 7)\n",
      "Columns: ['Filename', 'Audio', 'ID', 'Label', 'Duration', 'Index', 'Spectrogram']\n",
      "Label distribution:\n",
      "shape: (22, 2)\n",
      "┌────────────────────┬───────┐\n",
      "│ Label              ┆ count │\n",
      "│ ---                ┆ ---   │\n",
      "│ str                ┆ u32   │\n",
      "╞════════════════════╪═══════╡\n",
      "│ selftalk           ┆ 1885  │\n",
      "│ frustrated         ┆ 1536  │\n",
      "│ delighted          ┆ 1272  │\n",
      "│ dysregulated       ┆ 704   │\n",
      "│ social             ┆ 634   │\n",
      "│ …                  ┆ …     │\n",
      "│ tablet             ┆ 100   │\n",
      "│ no                 ┆ 100   │\n",
      "│ laugh              ┆ 100   │\n",
      "│ more               ┆ 100   │\n",
      "│ dysregulation-sick ┆ 100   │\n",
      "└────────────────────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "df = pl.read_parquet(\"balanced_audio_data.parquet\")\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns}\")\n",
    "print(f\"Label distribution:\")\n",
    "print(df.group_by(\"Label\").count().sort(\"count\", descending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cf4ccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfccs(spec_data, n_mfccs=13):\n",
    "    mfccs = []\n",
    "\n",
    "    for spec in spec_data:\n",
    "        if isinstance(spec, list):\n",
    "            spec_array = np.array(spec)\n",
    "            \n",
    "            # time x frequency\n",
    "            if spec_array.ndim == 2:\n",
    "                mfcc_features = np.mean(spec_array, axis=0)[:n_mfccs]\n",
    "                if len(mfcc_features) < n_mfccs:\n",
    "                    mfcc_features = np.pad(mfcc_features, (0, n_mfccs - len(mfcc_features)))\n",
    "                mfccs.append(mfcc_features)\n",
    "\n",
    "            else:\n",
    "                # Reshape if 1D (shouldn't be the case)\n",
    "                feat = spec_array[:n_mfccs] if len(spec_array) >= n_mfccs else np.pad(spec_array, (0, n_mfccs - len(spec_array)))\n",
    "                mfccs.append(feat)\n",
    "    \n",
    "    # numpy array with shape (n_samples, n_mfccs)\n",
    "    result = np.array(mfccs)\n",
    "    print(f\"Extracted MFCC shape: {result.shape}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03105a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_stats(audio_data):\n",
    "    \n",
    "    features = []\n",
    "    for audio in audio_data:\n",
    "        if isinstance(audio, list):\n",
    "            audio_array = np.array(audio)\n",
    "            \n",
    "            # Extract usual features + rms, zcr\n",
    "            mean = np.mean(audio_array)\n",
    "            std = np.std(audio_array)\n",
    "            max_val = np.max(audio_array)\n",
    "            min_val = np.min(audio_array)\n",
    "            rms = np.sqrt(np.mean(np.square(audio_array)))\n",
    "            zcr = np.sum(np.abs(np.diff(np.signbit(audio_array)))) / len(audio_array)\n",
    "            \n",
    "            features.append([mean, std, max_val, min_val, rms, zcr])\n",
    "        else:\n",
    "            # Fallback for non-list (also shouldn't be the case)\n",
    "            features.append([0, 0, 0, 0, 0, 0])\n",
    "    \n",
    "    # numpy array with shape (n_samples, 6)\n",
    "    result = np.array(features)\n",
    "    print(f\"Extracted audio stats shape: {result.shape}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c2f74a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted spectrograms: 8102\n",
      "Extracted audio data: 8102\n",
      "Extracted labels: 8102\n",
      "Extracting features.\n",
      "Extracted MFCC shape: (8102, 13)\n",
      "Extracted audio stats shape: (8102, 6)\n",
      "Feature vector shape: (8102, 19)\n",
      "Scaling features now.\n"
     ]
    }
   ],
   "source": [
    "# Data extraction\n",
    "spectrograms = df[\"Spectrogram\"].to_list()\n",
    "audio_data = df[\"Audio\"].to_list()\n",
    "labels = df[\"Label\"].to_list()\n",
    "\n",
    "print(f\"Extracted spectrograms: {len(spectrograms)}\")\n",
    "print(f\"Extracted audio data: {len(audio_data)}\")\n",
    "print(f\"Extracted labels: {len(labels)}\")\n",
    "\n",
    "# Feature extraction\n",
    "print(\"Extracting features.\")\n",
    "mfcc_features = extract_mfccs(spectrograms)\n",
    "audio_features = extract_audio_stats(audio_data)\n",
    "\n",
    "X_combined = np.hstack((mfcc_features, audio_features))\n",
    "print(f\"Feature vector shape: {X_combined.shape}\")\n",
    "\n",
    "print(\"Scaling features now.\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04ead236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying reduction techniques.\n",
      "PCA explained variance: 0.71\n",
      "PCA contains NaN: False\n",
      "PCA contains inf: False\n",
      "PCA min: -9.597823512844759, max: 13.001468678488116\n",
      "t-SNE contains NaN: False\n",
      "t-SNE contains inf: False\n",
      "t-SNE min: -86.40396881103516, max: 79.43026733398438\n",
      "UMAP contains NaN: False\n",
      "UMAP contains inf: False\n",
      "UMAP min: -2.158806324005127, max: 14.844462394714355\n",
      "Labels shape: 8102\n",
      "Unique labels: ['affectionate' 'bathroom' 'delighted' 'dysregulated'\n",
      " 'dysregulation-bathroom' 'dysregulation-sick' 'frustrated' 'glee'\n",
      " 'greeting' 'happy' 'help' 'hunger' 'laugh' 'laughter' 'more' 'no'\n",
      " 'protest' 'request' 'selftalk' 'social' 'tablet' 'yes']\n",
      "Label counts: [(np.str_('affectionate'), np.int64(129)), (np.str_('bathroom'), np.int64(100)), (np.str_('delighted'), np.int64(1272)), (np.str_('dysregulated'), np.int64(704)), (np.str_('dysregulation-bathroom'), np.int64(100)), (np.str_('dysregulation-sick'), np.int64(100)), (np.str_('frustrated'), np.int64(1536)), (np.str_('glee'), np.int64(100)), (np.str_('greeting'), np.int64(100)), (np.str_('happy'), np.int64(100)), (np.str_('help'), np.int64(100)), (np.str_('hunger'), np.int64(100)), (np.str_('laugh'), np.int64(100)), (np.str_('laughter'), np.int64(100)), (np.str_('more'), np.int64(100)), (np.str_('no'), np.int64(100)), (np.str_('protest'), np.int64(100)), (np.str_('request'), np.int64(419)), (np.str_('selftalk'), np.int64(1885)), (np.str_('social'), np.int64(634)), (np.str_('tablet'), np.int64(100)), (np.str_('yes'), np.int64(123))]\n"
     ]
    }
   ],
   "source": [
    "print(\"Applying reduction techniques.\")\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(X_scaled)\n",
    "print(f\"PCA explained variance: {pca.explained_variance_ratio_.sum():.2f}\")\n",
    "\n",
    "# t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "tsne_result = tsne.fit_transform(X_scaled)\n",
    "\n",
    "# UMAP\n",
    "reducer = umap.UMAP(random_state=42)\n",
    "umap_result = reducer.fit_transform(X_scaled)\n",
    "\n",
    "projections = {\n",
    "    'PCA': pca_result,\n",
    "    't-SNE': tsne_result,\n",
    "    'UMAP': umap_result\n",
    "}\n",
    "\n",
    "for name, result in projections.items():\n",
    "    print(f\"{name} contains NaN: {np.isnan(result).any()}\")\n",
    "    print(f\"{name} contains inf: {np.isinf(result).any()}\")\n",
    "    print(f\"{name} min: {np.min(result)}, max: {np.max(result)}\")\n",
    "\n",
    "print(f\"Labels shape: {len(labels)}\")\n",
    "print(f\"Unique labels: {np.unique(labels)}\")\n",
    "print(f\"Label counts: {[(l, np.sum(np.array(labels) == l)) for l in np.unique(labels)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d201420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying 2D dimensionality reduction...\n"
     ]
    }
   ],
   "source": [
    "print(\"Applying 2D dimensionality reduction...\")\n",
    "\n",
    "for name, result in projections.items():\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Remove NaN/inf values\n",
    "    valid_indices = np.where(~np.isnan(result).any(axis=1) & ~np.isinf(result).any(axis=1))[0]\n",
    "    result_clean = result[valid_indices]\n",
    "    labels_clean = [labels[idx] for idx in valid_indices]\n",
    "    \n",
    "    unique_labels_clean = np.unique(labels_clean)\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels_clean)))\n",
    "    \n",
    "    for j, label in enumerate(unique_labels_clean):\n",
    "        mask = np.array(labels_clean) == label\n",
    "        if np.sum(mask) > 0:  # Only plot if we have points\n",
    "            plt.scatter(\n",
    "                result_clean[mask, 0], \n",
    "                result_clean[mask, 1], \n",
    "                c=[colors[j]], \n",
    "                label=label, \n",
    "                alpha=0.7, \n",
    "                s=50\n",
    "            )\n",
    "    \n",
    "    plt.title(f\"2D {name} Projection\")\n",
    "    plt.grid(True)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{name}_2d_projection.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78977bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 2D, now starting 3D.\n",
      "3D PCA explained variance: 0.78\n"
     ]
    }
   ],
   "source": [
    "# Apply 3D dimensionality reduction techniques\n",
    "print(\"Done with 2D, now starting 3D.\")\n",
    "\n",
    "# PCA 3D\n",
    "pca_3d = PCA(n_components=3)\n",
    "pca_result_3d = pca_3d.fit_transform(X_scaled)\n",
    "print(f\"3D PCA explained variance: {pca_3d.explained_variance_ratio_.sum():.2f}\")\n",
    "\n",
    "# t-SNE 3D\n",
    "tsne_3d = TSNE(n_components=3, random_state=42, perplexity=30)\n",
    "tsne_result_3d = tsne_3d.fit_transform(X_scaled)\n",
    "\n",
    "# UMAP 3D\n",
    "reducer_3d = umap.UMAP(n_components=3, random_state=42)\n",
    "umap_result_3d = reducer_3d.fit_transform(X_scaled)\n",
    "\n",
    "projections_3d = {\n",
    "    'PCA': pca_result_3d,\n",
    "    't-SNE': tsne_result_3d,\n",
    "    'UMAP': umap_result_3d\n",
    "}\n",
    "\n",
    "for name, result in projections_3d.items():\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    valid_indices = np.where(~np.isnan(result).any(axis=1) & ~np.isinf(result).any(axis=1))[0]\n",
    "    result_clean = result[valid_indices]\n",
    "    labels_clean = [labels[idx] for idx in valid_indices]\n",
    "    \n",
    "    unique_labels_clean = np.unique(labels_clean)\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels_clean)))\n",
    "    \n",
    "    for j, label in enumerate(unique_labels_clean):\n",
    "        mask = np.array(labels_clean) == label\n",
    "        if np.sum(mask) > 0:  # Only plot if we have points\n",
    "            ax.scatter(\n",
    "                result_clean[mask, 0], \n",
    "                result_clean[mask, 1], \n",
    "                result_clean[mask, 2],\n",
    "                c=[colors[j]], \n",
    "                label=label, \n",
    "                alpha=0.7, \n",
    "                s=50\n",
    "            )\n",
    "    \n",
    "    ax.set_title(f\"3D {name} Projection\")\n",
    "    ax.grid(True)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    ax.set_xlabel(f\"{name} Component 1\")\n",
    "    ax.set_ylabel(f\"{name} Component 2\")\n",
    "    ax.set_zlabel(f\"{name} Component 3\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{name}_3d_projection.png', bbox_inches='tight', dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4ed27e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating interactive 2D visualizations...\n",
      "Created interactive 2D plots (HTML files)\n"
     ]
    }
   ],
   "source": [
    "def create_interactive_2d_plot(result, labels_list, name):\n",
    "    df_plot = pd.DataFrame({\n",
    "        'x': result[:, 0],\n",
    "        'y': result[:, 1],\n",
    "        'label': labels_list\n",
    "    })\n",
    "    \n",
    "    # Using Plotly Express\n",
    "    fig = px.scatter(\n",
    "        df_plot, x='x', y='y', color='label',\n",
    "        title=f\"Interactive 2D {name} Projection\",\n",
    "        labels={'x': f\"{name} Component 1\", 'y': f\"{name} Component 2\"},\n",
    "        opacity=0.7,\n",
    "        height=800,\n",
    "        width=1000\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            yanchor=\"top\",\n",
    "            y=0.99,\n",
    "            xanchor=\"left\",\n",
    "            x=0.01\n",
    "        ),\n",
    "        margin=dict(l=20, r=20, b=40, t=60),\n",
    "        plot_bgcolor='#F8F9FA'\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray')\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray')\n",
    "    \n",
    "    # Save as HTML\n",
    "    fig.write_html(f\"{name}_2d_interactive.html\")\n",
    "\n",
    "print(\"Creating interactive 2D visualizations...\")\n",
    "for name, result in projections.items():\n",
    "    create_interactive_2d_plot(result, labels, name)\n",
    "\n",
    "print(\"Created interactive 2D plots (HTML files)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "676fc0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created interactive 3D plots (HTML files)\n"
     ]
    }
   ],
   "source": [
    "def create_interactive_3d_plot(result, labels_list, name):\n",
    "    df_plot = pd.DataFrame({\n",
    "        'x': result[:, 0],\n",
    "        'y': result[:, 1],\n",
    "        'z': result[:, 2],\n",
    "        'label': labels_list\n",
    "    })\n",
    "    \n",
    "    fig = px.scatter_3d(\n",
    "        df_plot, x='x', y='y', z='z', color='label',\n",
    "        title=f\"Interactive 3D {name} Projection\",\n",
    "        labels={'x': f\"{name} Component 1\", 'y': f\"{name} Component 2\", 'z': f\"{name} Component 3\"},\n",
    "        opacity=0.7\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            yanchor=\"top\",\n",
    "            y=0.99,\n",
    "            xanchor=\"left\",\n",
    "            x=0.01\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, b=0, t=40)\n",
    "    )\n",
    "    \n",
    "    fig.write_html(f\"{name}_3d_interactive.html\")\n",
    "\n",
    "for name, result in projections_3d.items():\n",
    "    create_interactive_3d_plot(result, labels, name)\n",
    "\n",
    "print(\"Created interactive 3D plots (HTML files)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "603c19c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: classifier_pics\n"
     ]
    }
   ],
   "source": [
    "save_dir = \"classifier_pics\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    print(f\"Created directory: {save_dir}\")\n",
    "else:\n",
    "    print(f\"Directory already exists: {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5e8bdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_kmeans_clustering(projection_data, true_labels, projection_name, n_clusters):\n",
    "\n",
    "    print(f\"Applying K-means clustering on {projection_name} projection...\")\n",
    "    \n",
    "    # Create and fit K-means model\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(projection_data)\n",
    "    \n",
    "    silhouette = silhouette_score(projection_data, cluster_labels)\n",
    "    ari = adjusted_rand_score(true_labels, cluster_labels)\n",
    "    nmi = normalized_mutual_info_score(true_labels, cluster_labels)\n",
    "    \n",
    "    print(f\"  Silhouette Score: {silhouette:.4f}\")\n",
    "    print(f\"  ARI (vs true labels): {ari:.4f}\")\n",
    "    print(f\"  NMI (vs true labels): {nmi:.4f}\")\n",
    "    \n",
    "    results = {\n",
    "        'labels': cluster_labels,\n",
    "        'silhouette': silhouette,\n",
    "        'ari': ari,\n",
    "        'nmi': nmi\n",
    "    }\n",
    "    \n",
    "    create_cluster_visualization(projection_data, cluster_labels, true_labels, \n",
    "                               projection_name, results)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57c7f630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cluster_visualization(data, cluster_labels, true_labels, projection_name, metrics):\n",
    "\n",
    "    is_3d = data.shape[1] == 3\n",
    "    # Create static plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    if is_3d:\n",
    "        ax = plt.subplot(111, projection='3d')\n",
    "    else:\n",
    "        ax = plt.subplot(111)\n",
    "    \n",
    "    unique_clusters = sorted(set(cluster_labels))\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, max(10, len(unique_clusters))))\n",
    "    \n",
    "    # Plot each unique cluster\n",
    "    for i, cluster in enumerate(unique_clusters):\n",
    "        cluster_mask = cluster_labels == cluster\n",
    "        \n",
    "        if is_3d:\n",
    "            ax.scatter(data[cluster_mask, 0], data[cluster_mask, 1], data[cluster_mask, 2], \n",
    "                      c=[colors[i % 10]], label=f'Cluster {cluster}', alpha=0.7, s=40)\n",
    "        else:\n",
    "            ax.scatter(data[cluster_mask, 0], data[cluster_mask, 1], \n",
    "                      c=[colors[i % 10]], label=f'Cluster {cluster}', alpha=0.7, s=40)\n",
    "    \n",
    "    dimension = \"3D\" if is_3d else \"2D\"\n",
    "    ax.set_title(f\"{dimension} {projection_name} with K-means Clustering\")\n",
    "    ax.set_xlabel(f\"{projection_name} Component 1\")\n",
    "    ax.set_ylabel(f\"{projection_name} Component 2\")\n",
    "    if is_3d:\n",
    "        ax.set_zlabel(f\"{projection_name} Component 3\")\n",
    "    \n",
    "    title_metrics = [\n",
    "        f\"Silhouette: {metrics['silhouette']:.3f}\",\n",
    "        f\"ARI: {metrics['ari']:.3f}\",\n",
    "        f\"NMI: {metrics['nmi']:.3f}\"\n",
    "    ]\n",
    "    plt.figtext(0.5, 0.01, \", \".join(title_metrics), ha='center', fontsize=10)\n",
    "    \n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    dim_str = \"3d\" if is_3d else \"2d\"\n",
    "    file_path = os.path.join(save_dir, f\"{projection_name}_kmeans_clustering_{dim_str}.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # more plotly\n",
    "    if is_3d:\n",
    "        create_interactive_cluster_3d(data, cluster_labels, true_labels, projection_name, metrics)\n",
    "    else:\n",
    "        create_interactive_cluster_2d(data, cluster_labels, true_labels, projection_name, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee7f5303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_cluster_2d(data, cluster_labels, true_labels, projection_name, metrics):\n",
    "    df_plot = pd.DataFrame({\n",
    "        'x': data[:, 0],\n",
    "        'y': data[:, 1],\n",
    "        'Cluster': [f'Cluster {label}' for label in cluster_labels],\n",
    "        'True Label': true_labels\n",
    "    })\n",
    "    \n",
    "    title_metrics = [\n",
    "        f\"Silhouette: {metrics['silhouette']:.3f}\",\n",
    "        f\"ARI: {metrics['ari']:.3f}\",\n",
    "        f\"NMI: {metrics['nmi']:.3f}\"\n",
    "    ]\n",
    "    \n",
    "    title = f\"Interactive 2D {projection_name} with K-means Clustering<br><sub>{', '.join(title_metrics)}</sub>\"\n",
    "    \n",
    "    fig = px.scatter(\n",
    "        df_plot, x='x', y='y', color='Cluster',\n",
    "        title=title,\n",
    "        labels={'x': f\"{projection_name} Component 1\", 'y': f\"{projection_name} Component 2\"},\n",
    "        opacity=0.7,\n",
    "        height=700,\n",
    "        width=1000,\n",
    "        hover_data=['True Label']\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            yanchor=\"top\",\n",
    "            y=0.99,\n",
    "            xanchor=\"left\",\n",
    "            x=0.01\n",
    "        ),\n",
    "        margin=dict(l=20, r=20, b=40, t=80),\n",
    "        plot_bgcolor='#F8F9FA'\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray')\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=0.5, gridcolor='lightgray')\n",
    "    \n",
    "    file_path = os.path.join(save_dir, f\"{projection_name}_kmeans_clustering_2d_interactive.html\")\n",
    "    fig.write_html(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f65dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_cluster_3d(data, cluster_labels, true_labels, projection_name, metrics):\n",
    "    df_plot = pd.DataFrame({\n",
    "        'x': data[:, 0],\n",
    "        'y': data[:, 1],\n",
    "        'z': data[:, 2],\n",
    "        'Cluster': [f'Cluster {label}' for label in cluster_labels],\n",
    "        'True Label': true_labels\n",
    "    })\n",
    "    \n",
    "    title_metrics = [\n",
    "        f\"Silhouette: {metrics['silhouette']:.3f}\",\n",
    "        f\"ARI: {metrics['ari']:.3f}\",\n",
    "        f\"NMI: {metrics['nmi']:.3f}\"\n",
    "    ]\n",
    "    \n",
    "    title = f\"Interactive 3D {projection_name} with K-means Clustering<br><sub>{', '.join(title_metrics)}</sub>\"\n",
    "    \n",
    "    fig = px.scatter_3d(\n",
    "        df_plot, x='x', y='y', z='z', color='Cluster',\n",
    "        title=title,\n",
    "        labels={\n",
    "            'x': f\"{projection_name} Component 1\", \n",
    "            'y': f\"{projection_name} Component 2\", \n",
    "            'z': f\"{projection_name} Component 3\"\n",
    "        },\n",
    "        opacity=0.7,\n",
    "        hover_data=['True Label']\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            yanchor=\"top\",\n",
    "            y=0.99,\n",
    "            xanchor=\"left\",\n",
    "            x=0.01\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, b=0, t=60)\n",
    "    )\n",
    "    \n",
    "    file_path = os.path.join(save_dir, f\"{projection_name}_kmeans_clustering_3d_interactive.html\")\n",
    "    fig.write_html(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f1fa8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying K-means clustering with 22 clusters...\n",
      "\n",
      "Clustering on 2D PCA projection:\n",
      "Applying K-means clustering on PCA projection...\n",
      "  Silhouette Score: 0.3273\n",
      "  ARI (vs true labels): 0.0143\n",
      "  NMI (vs true labels): 0.1113\n",
      "\n",
      "Clustering on 2D t-SNE projection:\n",
      "Applying K-means clustering on t-SNE projection...\n",
      "  Silhouette Score: 0.3729\n",
      "  ARI (vs true labels): 0.0296\n",
      "  NMI (vs true labels): 0.1753\n",
      "\n",
      "Clustering on 2D UMAP projection:\n",
      "Applying K-means clustering on UMAP projection...\n",
      "  Silhouette Score: 0.3777\n",
      "  ARI (vs true labels): 0.0315\n",
      "  NMI (vs true labels): 0.1748\n",
      "\n",
      "Clustering on 3D PCA projection:\n",
      "Applying K-means clustering on PCA projection...\n",
      "  Silhouette Score: 0.2319\n",
      "  ARI (vs true labels): 0.0173\n",
      "  NMI (vs true labels): 0.1200\n",
      "\n",
      "Clustering on 3D t-SNE projection:\n",
      "Applying K-means clustering on t-SNE projection...\n",
      "  Silhouette Score: 0.2963\n",
      "  ARI (vs true labels): 0.0308\n",
      "  NMI (vs true labels): 0.1741\n",
      "\n",
      "Clustering on 3D UMAP projection:\n",
      "Applying K-means clustering on UMAP projection...\n",
      "  Silhouette Score: 0.3312\n",
      "  ARI (vs true labels): 0.0286\n",
      "  NMI (vs true labels): 0.1727\n"
     ]
    }
   ],
   "source": [
    "unique_labels = np.unique(labels)\n",
    "n_clusters = len(unique_labels)\n",
    "\n",
    "print(f\"Applying K-means clustering with {n_clusters} clusters...\")\n",
    "\n",
    "# For 2D\n",
    "clustering_results_2d = {}\n",
    "for name, projection in projections.items():\n",
    "    print(f\"\\nClustering on 2D {name} projection:\")\n",
    "    clustering_results_2d[name] = apply_kmeans_clustering(projection, labels, name, n_clusters)\n",
    "\n",
    "# For 3D\n",
    "clustering_results_3d = {}\n",
    "for name, projection in projections_3d.items():\n",
    "    print(f\"\\nClustering on 3D {name} projection:\")\n",
    "    clustering_results_3d[name] = apply_kmeans_clustering(projection, labels, name, n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "140bb124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table of results\n",
    "def create_clustering_summary(results_2d, results_3d):\n",
    "    summary_data = []\n",
    "    \n",
    "    for proj_name, metrics in results_2d.items():\n",
    "        row = {\n",
    "            'Projection': proj_name,\n",
    "            'Dimensions': '2D',\n",
    "            'Silhouette': metrics['silhouette'],\n",
    "            'ARI': metrics['ari'],\n",
    "            'NMI': metrics['nmi']\n",
    "        }\n",
    "        summary_data.append(row)\n",
    "    \n",
    "    for proj_name, metrics in results_3d.items():\n",
    "        row = {\n",
    "            'Projection': proj_name,\n",
    "            'Dimensions': '3D',\n",
    "            'Silhouette': metrics['silhouette'],\n",
    "            'ARI': metrics['ari'],\n",
    "            'NMI': metrics['nmi']\n",
    "        }\n",
    "        summary_data.append(row)\n",
    "    \n",
    "    # CSort df by ARI (higher is better)\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df = summary_df.sort_values(by=['ARI', 'NMI'], ascending=False)\n",
    "    \n",
    "    csv_path = os.path.join(save_dir, \"kmeans_clustering_results_summary.csv\")\n",
    "    summary_df.to_csv(csv_path, index=False)\n",
    "    print(f\"\\nSummary saved to {csv_path}\")\n",
    "    \n",
    "    # Bar chart\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ax = sns.barplot(\n",
    "        data=summary_df.melt(id_vars=['Projection', 'Dimensions'], \n",
    "                            value_vars=['Silhouette', 'ARI', 'NMI']),\n",
    "        x='Projection', y='value', hue='variable',\n",
    "        palette='viridis'\n",
    "    )\n",
    "    plt.title('K-means Clustering Performance across Projections')\n",
    "    plt.xlabel('Projection Method')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Metric')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"kmeans_performance_comparison.png\"), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96ef2324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary saved to classifier_pics\\kmeans_clustering_results_summary.csv\n",
      "\n",
      "==================================================\n",
      "Best K-means Clustering Method:\n",
      "Projection: UMAP 2D\n",
      "ARI: 0.0315\n",
      "NMI: 0.1748\n",
      "Silhouette Score: 0.3777\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "summary_df = create_clustering_summary(clustering_results_2d, clustering_results_3d)\n",
    "\n",
    "best_method = summary_df.iloc[0]\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Best K-means Clustering Method:\")\n",
    "print(f\"Projection: {best_method['Projection']} {best_method['Dimensions']}\")\n",
    "print(f\"ARI: {best_method['ARI']:.4f}\")\n",
    "print(f\"NMI: {best_method['NMI']:.4f}\")\n",
    "print(f\"Silhouette Score: {best_method['Silhouette']:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4649438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_feature_importance(X_combined, labels, feature_names):\n",
    "\n",
    "    print(\"Now: evaluating individual feature performance with SVM.\")\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(labels)\n",
    "    feature_performances = {}\n",
    "    kernels = ['linear', 'rbf', 'sigmoid']\n",
    "    \n",
    "    # Test each feature individually\n",
    "    for i in range(X_combined.shape[1]):\n",
    "        feature_data = X_combined[:, i].reshape(-1, 1)  # Extract single feature as column vector\n",
    "        feature_name = feature_names[i] if i < len(feature_names) else f\"Feature_{i}\"\n",
    "        \n",
    "        print(f\"\\nTesting feature: {feature_name}\")\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        feature_scaled = scaler.fit_transform(feature_data)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            feature_scaled, y_encoded, test_size=0.25, random_state=42, stratify=y_encoded\n",
    "        )\n",
    "        \n",
    "        best_kernel = None\n",
    "        best_accuracy = 0\n",
    "        best_model = None\n",
    "        best_predictions = None\n",
    "        \n",
    "        for kernel in kernels:\n",
    "            # Grid search for RBF kernel\n",
    "            if kernel == 'rbf':\n",
    "                param_grid = {\n",
    "                    'C': [0.1, 1, 10, 100],\n",
    "                    'gamma': ['scale', 'auto', 0.1, 0.01]\n",
    "                }\n",
    "                svm = GridSearchCV(\n",
    "                    SVC(kernel=kernel, random_state=42),\n",
    "                    param_grid=param_grid,\n",
    "                    scoring='accuracy',\n",
    "                    cv=3,\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "                svm.fit(X_train, y_train)\n",
    "                model = svm.best_estimator_\n",
    "            else:\n",
    "                model = SVC(kernel=kernel, random_state=42)\n",
    "                model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred = model.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            \n",
    "            print(f\"  {kernel.upper()} kernel accuracy: {accuracy:.4f}\")\n",
    "            \n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_kernel = kernel\n",
    "                best_model = model\n",
    "                best_predictions = y_pred\n",
    "        \n",
    "        feature_performances[feature_name] = {\n",
    "            'best_kernel': best_kernel,\n",
    "            'best_accuracy': best_accuracy,\n",
    "            'confusion_matrix': confusion_matrix(y_test, best_predictions),\n",
    "            'classification_report': classification_report(y_test, best_predictions, \n",
    "                                                         target_names=le.classes_, \n",
    "                                                         output_dict=True),\n",
    "            'model': best_model\n",
    "        }\n",
    "        \n",
    "        print(f\"  Best kernel for {feature_name}: {best_kernel.upper()} with accuracy {best_accuracy:.4f}\")\n",
    "    \n",
    "    best_feature = max(feature_performances.items(), \n",
    "                      key=lambda x: x[1]['best_accuracy'])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Best single feature: {best_feature[0]} with {best_feature[1]['best_kernel']} kernel\")\n",
    "    print(f\"Accuracy: {best_feature[1]['best_accuracy']:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    plot_feature_performance(feature_performances, le.classes_)\n",
    "    \n",
    "    return feature_performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37d1935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_performance(feature_performances, class_names):\n",
    "\n",
    "    features = list(feature_performances.keys())\n",
    "    accuracies = [data['best_accuracy'] for data in feature_performances.values()]\n",
    "    kernels = [data['best_kernel'] for data in feature_performances.values()]\n",
    "    \n",
    "    plot_data = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'Accuracy': accuracies,\n",
    "        'Kernel': kernels\n",
    "    })\n",
    "    \n",
    "    plot_data = plot_data.sort_values('Accuracy', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    ax = sns.barplot(x='Accuracy', y='Feature', hue='Kernel', data=plot_data)\n",
    "    plt.title('SVM Classification Accuracy by Individual Feature', fontsize=15)\n",
    "    plt.xlabel('Accuracy', fontsize=12)\n",
    "    plt.ylabel('Feature', fontsize=12)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance_svm.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    best_feature = plot_data.iloc[0]['Feature']\n",
    "    best_perf = feature_performances[best_feature]\n",
    "    \n",
    "    # Confusion matrix for the best feature\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        best_perf['confusion_matrix'], \n",
    "        annot=True if len(class_names) <= 10 else False, \n",
    "        fmt='d', \n",
    "        cmap='Blues',\n",
    "        xticklabels=class_names, \n",
    "        yticklabels=class_names\n",
    "    )\n",
    "    plt.title(f'Confusion Matrix for Best Feature: {best_feature}\\nKernel: {best_perf[\"best_kernel\"]}', fontsize=13)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'best_feature_confusion_matrix.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # F1 scores for the best feature\n",
    "    class_f1 = {\n",
    "        class_name: report['f1-score'] \n",
    "        for class_name, report in best_perf['classification_report'].items() \n",
    "        if class_name not in ['accuracy', 'macro avg', 'weighted avg']\n",
    "    }\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    pd.Series(class_f1).sort_values().plot(kind='barh', color='teal')\n",
    "    plt.title(f'F1-Scores by Class for Best Feature ({best_feature})', fontsize=13)\n",
    "    plt.xlabel('F1-Score')\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'best_feature_f1_scores.png', dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4c4eded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing SVM on individual features...\n",
      "Now: evaluating individual feature performance with SVM.\n",
      "\n",
      "Testing feature: MFCC_1\n",
      "  LINEAR kernel accuracy: 0.2325\n",
      "  RBF kernel accuracy: 0.2493\n",
      "  SIGMOID kernel accuracy: 0.1831\n",
      "  Best kernel for MFCC_1: RBF with accuracy 0.2493\n",
      "\n",
      "Testing feature: MFCC_2\n",
      "  LINEAR kernel accuracy: 0.2502\n",
      "  RBF kernel accuracy: 0.2537\n",
      "  SIGMOID kernel accuracy: 0.1989\n",
      "  Best kernel for MFCC_2: RBF with accuracy 0.2537\n",
      "\n",
      "Testing feature: MFCC_3\n",
      "  LINEAR kernel accuracy: 0.2517\n",
      "  RBF kernel accuracy: 0.2547\n",
      "  SIGMOID kernel accuracy: 0.1747\n",
      "  Best kernel for MFCC_3: RBF with accuracy 0.2547\n",
      "\n",
      "Testing feature: MFCC_4\n",
      "  LINEAR kernel accuracy: 0.2562\n",
      "  RBF kernel accuracy: 0.2552\n",
      "  SIGMOID kernel accuracy: 0.1520\n",
      "  Best kernel for MFCC_4: LINEAR with accuracy 0.2562\n",
      "\n",
      "Testing feature: MFCC_5\n",
      "  LINEAR kernel accuracy: 0.2577\n",
      "  RBF kernel accuracy: 0.2547\n",
      "  SIGMOID kernel accuracy: 0.1787\n",
      "  Best kernel for MFCC_5: LINEAR with accuracy 0.2577\n",
      "\n",
      "Testing feature: MFCC_6\n",
      "  LINEAR kernel accuracy: 0.2572\n",
      "  RBF kernel accuracy: 0.2577\n",
      "  SIGMOID kernel accuracy: 0.1663\n",
      "  Best kernel for MFCC_6: RBF with accuracy 0.2577\n",
      "\n",
      "Testing feature: MFCC_7\n",
      "  LINEAR kernel accuracy: 0.2621\n",
      "  RBF kernel accuracy: 0.2606\n",
      "  SIGMOID kernel accuracy: 0.1584\n",
      "  Best kernel for MFCC_7: LINEAR with accuracy 0.2621\n",
      "\n",
      "Testing feature: MFCC_8\n",
      "  LINEAR kernel accuracy: 0.2621\n",
      "  RBF kernel accuracy: 0.2631\n",
      "  SIGMOID kernel accuracy: 0.1930\n",
      "  Best kernel for MFCC_8: RBF with accuracy 0.2631\n",
      "\n",
      "Testing feature: MFCC_9\n",
      "  LINEAR kernel accuracy: 0.2636\n",
      "  RBF kernel accuracy: 0.2641\n",
      "  SIGMOID kernel accuracy: 0.1565\n",
      "  Best kernel for MFCC_9: RBF with accuracy 0.2641\n",
      "\n",
      "Testing feature: MFCC_10\n",
      "  LINEAR kernel accuracy: 0.2631\n",
      "  RBF kernel accuracy: 0.2537\n",
      "  SIGMOID kernel accuracy: 0.1441\n",
      "  Best kernel for MFCC_10: LINEAR with accuracy 0.2631\n",
      "\n",
      "Testing feature: MFCC_11\n",
      "  LINEAR kernel accuracy: 0.2591\n",
      "  RBF kernel accuracy: 0.2468\n",
      "  SIGMOID kernel accuracy: 0.1476\n",
      "  Best kernel for MFCC_11: LINEAR with accuracy 0.2591\n",
      "\n",
      "Testing feature: MFCC_12\n",
      "  LINEAR kernel accuracy: 0.2591\n",
      "  RBF kernel accuracy: 0.2443\n",
      "  SIGMOID kernel accuracy: 0.1787\n",
      "  Best kernel for MFCC_12: LINEAR with accuracy 0.2591\n",
      "\n",
      "Testing feature: MFCC_13\n",
      "  LINEAR kernel accuracy: 0.2552\n",
      "  RBF kernel accuracy: 0.2488\n",
      "  SIGMOID kernel accuracy: 0.1619\n",
      "  Best kernel for MFCC_13: LINEAR with accuracy 0.2552\n",
      "\n",
      "Testing feature: Mean\n",
      "  LINEAR kernel accuracy: 0.2325\n",
      "  RBF kernel accuracy: 0.2335\n",
      "  SIGMOID kernel accuracy: 0.1841\n",
      "  Best kernel for Mean: RBF with accuracy 0.2335\n",
      "\n",
      "Testing feature: StdDev\n",
      "  LINEAR kernel accuracy: 0.2384\n",
      "  RBF kernel accuracy: 0.2409\n",
      "  SIGMOID kernel accuracy: 0.1678\n",
      "  Best kernel for StdDev: RBF with accuracy 0.2409\n",
      "\n",
      "Testing feature: Max\n",
      "  LINEAR kernel accuracy: 0.2325\n",
      "  RBF kernel accuracy: 0.2453\n",
      "  SIGMOID kernel accuracy: 0.1737\n",
      "  Best kernel for Max: RBF with accuracy 0.2453\n",
      "\n",
      "Testing feature: Min\n",
      "  LINEAR kernel accuracy: 0.2325\n",
      "  RBF kernel accuracy: 0.2325\n",
      "  SIGMOID kernel accuracy: 0.1678\n",
      "  Best kernel for Min: LINEAR with accuracy 0.2325\n",
      "\n",
      "Testing feature: RMS\n",
      "  LINEAR kernel accuracy: 0.2384\n",
      "  RBF kernel accuracy: 0.2409\n",
      "  SIGMOID kernel accuracy: 0.1678\n",
      "  Best kernel for RMS: RBF with accuracy 0.2409\n",
      "\n",
      "Testing feature: ZCR\n",
      "  LINEAR kernel accuracy: 0.2512\n",
      "  RBF kernel accuracy: 0.2552\n",
      "  SIGMOID kernel accuracy: 0.1550\n",
      "  Best kernel for ZCR: RBF with accuracy 0.2552\n",
      "\n",
      "============================================================\n",
      "Best single feature: MFCC_9 with rbf kernel\n",
      "Accuracy: 0.2641\n",
      "============================================================\n",
      "\n",
      "Top 5 Individual Features:\n",
      "1. MFCC_9 with rbf kernel: 0.2641\n",
      "2. MFCC_8 with rbf kernel: 0.2631\n",
      "3. MFCC_10 with linear kernel: 0.2631\n",
      "4. MFCC_7 with linear kernel: 0.2621\n",
      "5. MFCC_11 with linear kernel: 0.2591\n"
     ]
    }
   ],
   "source": [
    "mfcc_names = [f\"MFCC_{i+1}\" for i in range(13)]\n",
    "audio_stat_names = [\"Mean\", \"StdDev\", \"Max\", \"Min\", \"RMS\", \"ZCR\"]\n",
    "feature_names = mfcc_names + audio_stat_names\n",
    "\n",
    "print(\"Testing SVM on individual features...\")\n",
    "feature_performances = evaluate_feature_importance(X_scaled, labels, feature_names)\n",
    "\n",
    "# Details about the best features\n",
    "top_features = sorted(\n",
    "    [(name, data['best_kernel'], data['best_accuracy']) \n",
    "     for name, data in feature_performances.items()],\n",
    "    key=lambda x: x[2],\n",
    "    reverse=True\n",
    ")[:5]  # Top 5 features\n",
    "\n",
    "print(\"\\nTop 5 Individual Features:\")\n",
    "for i, (feature, kernel, accuracy) in enumerate(top_features):\n",
    "    print(f\"{i+1}. {feature} with {kernel} kernel: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1909df9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hierarchical_ensemble(X_scaled, labels):\n",
    "\n",
    "    print(\"\\nNext: hierarchical trickle-down ensemble classification.\")\n",
    "    \n",
    "    labels_array = np.array(labels)\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    encoded_labels = le.fit_transform(labels_array)\n",
    "    \n",
    "    unique_labels = np.unique(labels_array)\n",
    "    label_counts = {label: np.sum(labels_array == label) for label in unique_labels}\n",
    "    \n",
    "    # Sort labels by frequency (most common first)\n",
    "    ordered_labels = sorted(label_counts.keys(), key=lambda x: label_counts[x], reverse=True)\n",
    "    print(f\"Label hierarchy will be: {ordered_labels}\")\n",
    "    \n",
    "    kernel = 'rbf'\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, labels_array, test_size=0.25, random_state=42, stratify=labels_array\n",
    "    )\n",
    "    \n",
    "    binary_classifiers = {}\n",
    "    remaining_labels = ordered_labels.copy()\n",
    "    \n",
    "    # For each level (except the last one which doesn't need a classifier)\n",
    "    for i in range(len(ordered_labels) - 1):\n",
    "        current_label = remaining_labels[0]\n",
    "        remaining_labels = remaining_labels[1:]\n",
    "        \n",
    "        train_indices = [idx for idx, label in enumerate(y_train) if label == current_label or label in remaining_labels]\n",
    "        binary_X = X_train[train_indices]\n",
    "        binary_y = np.array([1 if y_train[idx] == current_label else 0 for idx in train_indices])\n",
    "        \n",
    "        print(f\"Training binary classifier for: '{current_label}' vs rest\")\n",
    "        classifier = SVC(kernel=kernel, probability=True, random_state=42)\n",
    "        classifier.fit(binary_X, binary_y)\n",
    "        \n",
    "        binary_classifiers[current_label] = classifier\n",
    "    \n",
    "    # Trickle-down prediction\n",
    "    def predict_trickle_down(X):\n",
    "        predictions = []\n",
    "        \n",
    "        for sample in X:\n",
    "            remaining = ordered_labels.copy()\n",
    "            sample_reshaped = sample.reshape(1, -1)\n",
    "            predicted = False\n",
    "            \n",
    "            # Trickle down through the binary classifiers\n",
    "            for level, label in enumerate(ordered_labels[:-1]):\n",
    "                classifier = binary_classifiers[label]\n",
    "                if classifier.predict(sample_reshaped)[0] == 1:\n",
    "                    predictions.append(label)\n",
    "                    predicted = True\n",
    "                    break\n",
    "            \n",
    "            if not predicted:\n",
    "                predictions.append(ordered_labels[-1])\n",
    "        \n",
    "        return np.array(predictions)\n",
    "    \n",
    "    print(\"\\nEvaluating trickle-down ensemble...\")\n",
    "    y_pred = predict_trickle_down(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Ensemble accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "    report_text = classification_report(y_test, y_pred)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report_text)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=unique_labels, yticklabels=unique_labels)\n",
    "    plt.title(\"Confusion Matrix - Hierarchical Ensemble Classification\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"ensemble_confusion_matrix.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    precision = {}\n",
    "    recall = {}\n",
    "    f1 = {}\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        # Get class metrics from the classification report dict\n",
    "        if label in report_dict:\n",
    "            precision[label] = report_dict[label]['precision']\n",
    "            recall[label] = report_dict[label]['recall']\n",
    "            f1[label] = report_dict[label]['f1-score']\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    f1_series = pd.Series(f1)\n",
    "    f1_series.sort_values().plot(kind='barh', color='teal')\n",
    "    plt.title(\"F1-Scores by Class - Hierarchical Ensemble\")\n",
    "    plt.xlabel(\"F1-Score\")\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"ensemble_f1_scores.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        'classifiers': binary_classifiers,\n",
    "        'accuracy': accuracy, \n",
    "        'label_hierarchy': ordered_labels,\n",
    "        'confusion_matrix': cm,\n",
    "        'classification_report': report_dict,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e207354c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Next: hierarchical trickle-down ensemble classification.\n",
      "Label hierarchy will be: [np.str_('selftalk'), np.str_('frustrated'), np.str_('delighted'), np.str_('dysregulated'), np.str_('social'), np.str_('request'), np.str_('affectionate'), np.str_('yes'), np.str_('bathroom'), np.str_('dysregulation-bathroom'), np.str_('dysregulation-sick'), np.str_('glee'), np.str_('greeting'), np.str_('happy'), np.str_('help'), np.str_('hunger'), np.str_('laugh'), np.str_('laughter'), np.str_('more'), np.str_('no'), np.str_('protest'), np.str_('tablet')]\n",
      "Training binary classifier for: 'selftalk' vs rest\n",
      "Training binary classifier for: 'frustrated' vs rest\n",
      "Training binary classifier for: 'delighted' vs rest\n",
      "Training binary classifier for: 'dysregulated' vs rest\n",
      "Training binary classifier for: 'social' vs rest\n",
      "Training binary classifier for: 'request' vs rest\n",
      "Training binary classifier for: 'affectionate' vs rest\n",
      "Training binary classifier for: 'yes' vs rest\n",
      "Training binary classifier for: 'bathroom' vs rest\n",
      "Training binary classifier for: 'dysregulation-bathroom' vs rest\n",
      "Training binary classifier for: 'dysregulation-sick' vs rest\n",
      "Training binary classifier for: 'glee' vs rest\n",
      "Training binary classifier for: 'greeting' vs rest\n",
      "Training binary classifier for: 'happy' vs rest\n",
      "Training binary classifier for: 'help' vs rest\n",
      "Training binary classifier for: 'hunger' vs rest\n",
      "Training binary classifier for: 'laugh' vs rest\n",
      "Training binary classifier for: 'laughter' vs rest\n",
      "Training binary classifier for: 'more' vs rest\n",
      "Training binary classifier for: 'no' vs rest\n",
      "Training binary classifier for: 'protest' vs rest\n",
      "\n",
      "Evaluating trickle-down ensemble...\n",
      "Ensemble accuracy: 0.1732\n",
      "\n",
      "Classification Report:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "          affectionate       0.23      0.09      0.13        32\n",
      "              bathroom       1.00      0.28      0.44        25\n",
      "             delighted       0.42      0.06      0.10       318\n",
      "          dysregulated       0.33      0.42      0.37       176\n",
      "dysregulation-bathroom       0.29      0.64      0.40        25\n",
      "    dysregulation-sick       0.22      0.44      0.29        25\n",
      "            frustrated       0.00      0.00      0.00       384\n",
      "                  glee       0.00      0.00      0.00        25\n",
      "              greeting       0.95      0.76      0.84        25\n",
      "                 happy       0.00      0.00      0.00        25\n",
      "                  help       0.67      0.08      0.14        25\n",
      "                hunger       0.66      0.92      0.77        25\n",
      "                 laugh       0.48      0.52      0.50        25\n",
      "              laughter       0.02      0.40      0.03        25\n",
      "                  more       0.14      0.52      0.22        25\n",
      "                    no       0.24      0.56      0.34        25\n",
      "               protest       0.09      0.48      0.15        25\n",
      "               request       0.15      0.33      0.21       105\n",
      "              selftalk       0.00      0.00      0.00       471\n",
      "                social       0.19      0.33      0.24       159\n",
      "                tablet       0.27      0.68      0.39        25\n",
      "                   yes       0.12      0.35      0.18        31\n",
      "\n",
      "              accuracy                           0.17      2026\n",
      "             macro avg       0.29      0.36      0.26      2026\n",
      "          weighted avg       0.19      0.17      0.14      2026\n",
      "\n",
      "\n",
      "Trickle-down hierarchy: selftalk → frustrated → delighted → dysregulated → social → request → affectionate → yes → bathroom → dysregulation-bathroom → dysregulation-sick → glee → greeting → happy → help → hunger → laugh → laughter → more → no → protest → tablet\n",
      "Final ensemble accuracy: 0.1732\n",
      "\n",
      "=== Performance Metrics Summary ===\n",
      "Macro Avg Precision: 0.2942\n",
      "Macro Avg Recall: 0.3577\n",
      "Macro Avg F1-Score: 0.2614\n",
      "Weighted Avg F1-Score: 0.1389\n"
     ]
    }
   ],
   "source": [
    "ensemble_results = create_hierarchical_ensemble(X_scaled, labels)\n",
    "\n",
    "print(f\"\\nTrickle-down hierarchy: {' → '.join(ensemble_results['label_hierarchy'])}\")\n",
    "print(f\"Final ensemble accuracy: {ensemble_results['accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\n=== Performance Metrics Summary ===\")\n",
    "print(f\"Macro Avg Precision: {ensemble_results['classification_report']['macro avg']['precision']:.4f}\")\n",
    "print(f\"Macro Avg Recall: {ensemble_results['classification_report']['macro avg']['recall']:.4f}\")  \n",
    "print(f\"Macro Avg F1-Score: {ensemble_results['classification_report']['macro avg']['f1-score']:.4f}\")\n",
    "print(f\"Weighted Avg F1-Score: {ensemble_results['classification_report']['weighted avg']['f1-score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "202d53b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_performance_metrics(model_name, confusion_mat, class_report, labels):\n",
    "\n",
    "    # Normalize confusion matrix (true labels)\n",
    "    cm_normalized = confusion_mat.astype('float') / confusion_mat.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap=\"Blues\",\n",
    "                xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(f\"Normalized Confusion Matrix - {model_name}\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{model_name.lower().replace(' ', '_')}_norm_confusion_matrix.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    support = []\n",
    "    \n",
    "    for label in labels:\n",
    "        if label in class_report:\n",
    "            precision.append(class_report[label]['precision'])\n",
    "            recall.append(class_report[label]['recall'])\n",
    "            f1.append(class_report[label]['f1-score'])\n",
    "            support.append(class_report[label]['support'])\n",
    "    \n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Class': labels,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Support': support\n",
    "    })\n",
    "    \n",
    "    plt.figure(figsize=(14, 12))\n",
    "    \n",
    "    metrics_df_sorted = metrics_df.sort_values('F1 Score')\n",
    "    \n",
    "    metrics_plot = metrics_df_sorted.melt(\n",
    "        id_vars=['Class', 'Support'],\n",
    "        value_vars=['Precision', 'Recall', 'F1 Score'],\n",
    "        var_name='Metric', value_name='Value'\n",
    "    )\n",
    "    \n",
    "    g = sns.barplot(x='Value', y='Class', hue='Metric', data=metrics_plot, palette=['#5DADE2', '#58D68D', '#F4D03F'])\n",
    "    \n",
    "    # Support count annotations\n",
    "    for i, (_, row) in enumerate(metrics_df_sorted.iterrows()):\n",
    "        plt.text(0.02, i, f\"n={int(row['Support'])}\", va='center', color='black', fontsize=8)\n",
    "    \n",
    "    plt.title(f\"Classification Metrics by Class - {model_name}\", fontsize=16)\n",
    "    plt.xlabel('Score', fontsize=12)\n",
    "    plt.ylabel('Class', fontsize=12)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.legend(title='Metric')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{model_name.lower().replace(' ', '_')}_metrics_by_class.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    error_counts = {}\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            if i != j and confusion_mat[i, j] > 0:\n",
    "                error_type = f\"{labels[i]} → {labels[j]}\"\n",
    "                error_counts[error_type] = confusion_mat[i, j]\n",
    "    \n",
    "    if error_counts:\n",
    "        # Sort by count and get top errors\n",
    "        error_df = pd.DataFrame(list(error_counts.items()), columns=['Error Type', 'Count'])\n",
    "        error_df = error_df.sort_values('Count', ascending=False).head(15)  # Top 15 errors\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='Count', y='Error Type', data=error_df, palette='Reds_r')\n",
    "        plt.title(f\"Top Misclassifications - {model_name}\", fontsize=16)\n",
    "        plt.xlabel('Count', fontsize=12)\n",
    "        plt.ylabel('Error Type (True → Predicted)', fontsize=12)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{model_name.lower().replace(' ', '_')}_top_errors.png\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98aed478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Model Performance Comparison ====\n",
      "Hierarchical Ensemble Accuracy: 0.1732\n",
      "Best Single Feature (MFCC_9) Accuracy: 0.2641\n",
      "\n",
      "Macro F1-Scores:\n",
      "Hierarchical Ensemble: 0.2614\n",
      "Best Single Feature: 0.0326\n"
     ]
    }
   ],
   "source": [
    "unique_labels = np.unique(labels)\n",
    "ensemble_metrics = analyze_performance_metrics(\n",
    "    \"Hierarchical Ensemble\",\n",
    "    ensemble_results['confusion_matrix'],\n",
    "    ensemble_results['classification_report'],\n",
    "    unique_labels\n",
    ")\n",
    "\n",
    "best_feature_name = sorted([(name, data['best_accuracy']) for name, data in feature_performances.items()],\n",
    "                         key=lambda x: x[1], reverse=True)[0][0]\n",
    "\n",
    "# Fix for TypeError\n",
    "best_feature_metrics = analyze_performance_metrics(\n",
    "    f\"SVM with {best_feature_name}\",\n",
    "    feature_performances[best_feature_name]['confusion_matrix'],\n",
    "    feature_performances[best_feature_name]['classification_report'],\n",
    "    sorted(list(feature_performances[best_feature_name]['classification_report'].keys() - \n",
    "               {'accuracy', 'macro avg', 'weighted avg'}))\n",
    ")\n",
    "\n",
    "print(\"\\n==== Model Performance Comparison ====\")\n",
    "print(f\"Hierarchical Ensemble Accuracy: {ensemble_results['accuracy']:.4f}\")\n",
    "print(f\"Best Single Feature ({best_feature_name}) Accuracy: {feature_performances[best_feature_name]['best_accuracy']:.4f}\")\n",
    "print(\"\\nMacro F1-Scores:\")\n",
    "print(f\"Hierarchical Ensemble: {ensemble_results['classification_report']['macro avg']['f1-score']:.4f}\")\n",
    "print(f\"Best Single Feature: {feature_performances[best_feature_name]['classification_report']['macro avg']['f1-score']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
