<html lang="en-GB">

<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Clarity: A Minimalist Website Template for AI Research</title>
  <meta name="description"
    content="We've presented Clarity, a minimalist and elegant website template for AI research.">
  <meta name="referrer" content="no-referrer-when-downgrade">
  <meta name="robots" content="all">
  <meta content="en_EN" property="og:locale">
  <meta content="website" property="og:type">
  <meta content="https://shikun.io/projects/clarity" property="og:url">
  <meta content="Clarity" property="og:title">
  <meta content="Website Template for AI Research" property="og:description">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@your_twitter_id">
  <meta name="twitter:description" content="Clarity: A Minimalist Website Template for AI Research">
  <meta name="twitter:image:src" content="assets/figures/clarity.png">

  <link rel="stylesheet" type="text/css" media="all" href="assets/stylesheets/main_free.css" />
  <link rel="stylesheet" type="text/css" media="all" href="clarity/clarity.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/foundation.min.css">
  <link href="assets/fontawesome-free-6.6.0-web/css/all.min.css" rel="stylesheet">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/styles.css" />
  <script defer src="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/index.js"></script>
  <script src="assets/scripts/navbar.js"></script> <!-- Comment to remove table of content. -->
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            "HTML-CSS": {
              scale: 95,
              fonts: ["Gyre-Pagella"],
              imageFont: null,
              undefinedFamily: "'Arial Unicode MS', cmbright"
            },
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                processEscapes: true
              }
          });
    </script>
  <script type="text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>

  <style>
    /* Data table styling for Clarity template */
    .data-table-container {
      margin: 2rem 0;
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", sans-serif;
      overflow-x: auto;
      border-radius: 8px;
      box-shadow: 0 4px 8px rgba(0, 0, 0, 0.05);
    }

    .data-table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.9rem;
      border: 1px solid #e0e4e6;
      background-color: white;
    }

    .data-table-header {
      padding: 0.75rem 1rem;
      background-color: #f5f7f8;
      color: #505a5f;
      text-align: left;
      font-size: 0.85rem;
      font-weight: 600;
      letter-spacing: 0.02em;
      border-bottom: 2px solid #e0e4e6;
    }

    .data-table-datatype {
      padding: 0.5rem 1rem;
      color: #667883;
      font-size: 0.75rem;
      font-weight: normal;
      background-color: #f9fafb;
      border-bottom: 1px solid #e0e4e6;
      font-family: Menlo, Monaco, "Courier New", monospace;
    }

    .data-table-cell {
      padding: 0.5rem 1rem;
      border-top: 1px solid #e0e4e6;
      color: #2a3439;
      vertical-align: top;
      font-size: 0.85rem;
    }

    .data-table-cell code {
      font-family: Menlo, Monaco, "Courier New", monospace;
      font-size: 0.8rem;
      color: #505a5f;
      background-color: #f5f7f8;
      padding: 0.2rem;
      border-radius: 3px;
    }

    .data-table-footer {
      font-size: 0.75rem;
      color: #667883;
      padding: 0.75rem 1rem;
      text-align: left;
      background-color: #f9fafb;
      border-top: 1px solid #e0e4e6;
    }

    .data-table tbody tr:nth-child(even) {
      background-color: #f9fafb;
    }

    .data-table tbody tr:hover {
      background-color: #f0f4f7;
    }

    .data-table-truncated {
      color: #8b9aa4;
      font-style: italic;
    }
    
    .plot-container {
      margin-bottom: 40px;
      border: 1px solid #ccc;
      background-color: white;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
      padding: 10px;
      width: 100%
    }
        
    iframe {
      width: 100%;
      height: 600px;
      border: none;
    }
  </style>
</head>

<>
  <!-- Title Page -->
  <!-- Dark Theme Example: Change the background colour dark and change the following div "blog-title" into "blog-title white". -->
  <div class="container blog" id="first-content" style="background-color: #E0E4E6;">
    <!-- If you don't have a project cover: Change "blog-title" into "blog-title no-cover"  -->
    <div class="blog-title">
      <div class="blog-intro">
        <div>
          <!-- <h1 class="title">
                        Learning to Listen: Decoding Nonverbal Vocal Intent in Autism Through Feature-Based Modeling
                    </h1> -->
          <h1 class="title">
            Machine Learning the Unspoken: Acoustic Feature Analysis for Classifying Intent in Nonverbal Vocalizations
          </h1>
          <p>Spring 2025 Data Science Project</p>
          <p class="author">Vishesh Narayan, Shivam Amin, Deval Bansal, Eric Yao, Eshan Khan</p>
          <p class="abstract">
            This project explores the use of machine learning to classify non-verbal vocalizations from autistic
            individuals into expressive intent categories (e.g., “yes”, “no”, “frustrated”, “delighted”). We develop a
            preprocessing pipeline to clean raw audio, extract acoustic features (pitch, MFCCs, spectral entropy), and
            generate normalized Mel spectrograms. Statistical analysis confirms that these features vary meaningfully
            across intent labels, motivating their use for supervised classification.

            We experiment with classical models (logistic regression, random forests) and deep learning architectures
            (CNNs, attention-based models) to assess classification performance and interpretability. The tutorial
            provides a reproducible walkthrough from data preparation to model evaluation, with a supplemental PDF
            detailing our exploratory data analysis. Our goal is to contribute toward tools that help decode
            communicative intent in nonverbal autism contexts.
          </p>

        </div>

        <div class="info">
          <div>
            <a href="https://github.com/visheshnarayan/cmsc320-final" class="button icon"
              style="background-color: rgba(255, 255, 255, 0.25); margin-bottom: 0;">Github <i
                class="fa-solid fa-code"></i></a>
          </div>
        </div>
      </div>

      <div class="blog-cover">
        <img class="foreground" src="assets/figures/clarity.png">
        <img class="background" src="assets/figures/clarity.png">
      </div>
    </div>
  </div>

  <div class="container blog main first" id="blog-main">

    <!-- CONTRIBUTIONS -->

    <h1>
      Contributions
    </h1>
    <ol style="padding: 2em; display: flex; flex-direction: column; gap: 1em;">
      <li>
        <p class="text"><span style="font-weight: bold">Vishesh Narayan</span>: Helped conceive and define the project
          scope and objective (A). He developed the data preprocessing pipeline and extracted key audio features (B),
          conducted exploratory data analysis and visualizations (C), and contributed to the design and implementation
          of ML models including CNN, classical classifiers, and Attention-based models (D). Vishesh also ran training
          experiments and model evaluations (E), helped interpret results and refine insights (F), and contributed
          heavily to writing and formatting the final tutorial report (G).</p>
      </li>
      <li>
        <p class="text"><span style="font-weight: bold">Shivam Amin</span>: Improved dataset loading by building
          parallel processing functions for faster and more efficient data handling along with waveform cleaning,
          spectrogram generation, and acoustic feature extraction (B). He contributed to exploratory data analysis and
          interpretation (C), helped design and implement ML models (D), participated in interpreting visualizations and
          results (F), and helped write and polish the final tutorial (G).</p>
      </li>
      <li>
        <p class="text"><span style="font-weight: bold">Deval Bansal</span>: Contributed to EDA through feature summary
          statistics and comparative plots (C), helped build classification models and optimize hyperparameters using
          the elbow method (D), ran training and testing procedures on classical (E), created supporting visualizations
          and analysis summaries (F), and co-authored the final report (G).</p>
      </li>
      <li>
        <p class="text"><span style="font-weight: bold">Eric Yao</span>: Assisted in audio feature extraction and
          comparative analysis of spectral signatures (C), developed deep learning models including CNN variants and
          preprocessing logic (D), supported model training and hyperparameter tuning (E), helped interpret results and
          plot visual comparisons (F), and contributed to the overall report structure and clarity (G).</p>
      </li>
      <li>
        <p class="text"><span style="font-weight: bold">Eshan Khan</span>: Analyzed MFCC and pitch statistics across
          label groups and visualized feature correlations (C), contributed to classifier experimentation and CNN
          architecture selection (D), supported training runs and validation of model outputs (E), assisted in
          visualizing trends and summarizing results (F), and contributed to writing key sections of the final tutorial
          report (G).</p>
      </li>
    </ol>
  </div>

  <div class="container blog main">
    <!-- DATASET CURATION -->
    <h1>Data Curation</h1>

    <p class="text">
      In this section, we will go over details of the dataset and transforming our data into a indexable interactive
      data frame.
    </p>

    <h2>Dataset</h2>

    <p class="text">
      For this project, we use the <cite>ReCANVo</cite> dataset, which contains real-world vocalizations of non-verbal
      autistic children and young adults. Each vocalization is labeled with its intended expressive category—such as
      <i>happy</i>, <i>frustrated</i>, <i>hungry</i>, or <i>self-talk</i>—allowing for supervised learning approaches to
      intent classification.
    </p>
    <p class="text">
      The dataset was compiled by Dr. Kristine Johnson at MIT as part of a study exploring how machine learning
      techniques can be used to interpret communicative vocal cues in autistic individuals. Audio samples were recorded
      in naturalistic settings, making this dataset especially valuable for research on real-world assistive
      technologies.
    </p>

    <p class="text">
      Dataset citation: <br>
      <cite>Narain, J., & Johnson, K. T. (2021). ReCANVo: A Dataset of Real-World Communicative and Affective Nonverbal
        Vocalizations</cite> [Data set]. Zenodo. <a href="https://doi.org/10.5281/zenodo.5786860"
        target="_blank">https://doi.org/10.5281/zenodo.5786860</a>
    </p>

    <h2>DataFrame creation</h2>

    <p class="text">
      We loaded the audio files for our dataset using <code>librosa</code>, along with associated metadata from a CSV
      file that included labels, participant IDs, and file indices. All of this information was organized into a
      structured <code>polars</code> DataFrame. Because audio loading is computationally intensive and initially caused
      RAM issues, Shivam implemented a multi-threaded approach to parallelize the loading process. This optimization
      significantly reduced loading times and improved memory efficiency (and so our kernel stopped crashing 🥀).
    </p>
  </div>

  <div class="container blog main">
    <details>
      <summary style="padding-top: 1em; font-family:Arial, Helvetica, sans-serif;">Show/Hide Full Loading code</summary>
      <pre><code class="python">
def load_audio_metadata(csv_path: str,
            audio_dir: str,
            limit: Union[int, None] = None,
            clean_audio_params: dict = None,
            save_comparisons: bool = False,
            comparison_dir: str = 'audio_comparisons') -> pl.DataFrame:
    """
    Loads audio metadata and processes files in parallel.
    
    Args:
        csv_path (str): Path to CSV file with metadata.
        audio_dir (str): Directory where audio files are stored.
        limit (int, optional): Number of rows to load.
        clean_audio_params (dict, optional): Parameters for cleaning.
        save_comparisons (bool): Save original vs cleaned audio files.
        comparison_dir (str): Directory for saved audio comparisons.
    
    Returns:
        pl.DataFrame: DataFrame with processed audio metadata.
    """
    
    df = pl.read_csv(csv_path).drop_nulls(subset=['Filename'])

    if limit:
        df = df.head(limit)

    # Default audio cleaning parameters
    default_clean_params = {
        'denoise': True,
        'remove_silence': True,
        'normalize': True,
        'min_silence_duration': 0.3,
        'silence_threshold': -40
    }
    clean_params = {**default_clean_params, **(clean_audio_params or {})}

    # Prepare file processing queue 
    file_info_list = [
        (row['Filename'], 
        os.path.join(audio_dir, row['Filename']), 
        clean_params, 
        save_comparisons, 
        comparison_dir, 
        row['ID'],
        row['Label'],  
        row['Index']) 
        for row in df.iter_rows(named=True)
    ]

    # Modify process_audio_file to handle the additional parameters
    def process_audio_file(
        file_info: Tuple[str, str, dict, bool, str, int, str, int]
    ) -> Union[Tuple[str, List[float], int, str, float, int], None]:
        """
        Loads and processes an audio file.

        Args:
            file_info (Tuple): Contains filename, full path, cleaning params,
            saving options, ID, Label, and Index.

        Returns:
            Tuple[str, List[float], int, str, float, int] | None: Processed
            audio metadata or None if failed.
        """
        (
            file_name, file_path, clean_params,
            save_comparisons, comparison_dir,
            file_id, label, index 
        ) = file_info

        y, sr = librosa.load(file_path, sr=SAMPLE_RATE)  
        cleaned_y = clean_audio(y, sr, **clean_params)

        if save_comparisons:
            save_audio_comparison(y, cleaned_y, sr, file_name, comparison_dir)

        duration = len(cleaned_y) / sr
        return file_name, cleaned_y.tolist(), file_id, label, duration, index  

    # Use ThreadPoolExecutor for parallel processing
    with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:
        results = list(executor.map(process_audio_file, file_info_list))

    # Filter out None values from failed processing
    audio_data = [res for res in results if res]

    return pl.DataFrame(
        audio_data,
        schema=["Filename", "Audio", "ID", "Label", "Duration", "Index"], orient='row'
    )
        </code></pre>
    </details>
  </div>

  <div class="container blog main">
    <!-- EDA -->

    <h1 style="padding-top: 1em">Exploratory Data Analysis</h1>

    <p class="text">
      In this, we will overview all of our Exploratory Data Analysis (EDA) done to perform statistical tests on our
      features, develop assumption about signals in our data, and visualize it too of course.
    </p>
  </div>
  <div class="container blog main gray">
    <h2>Audio Preprocessing Pipeline Overview</h2>
    <p class="text">
      Our preprocessing pipeline for audio data follows a structured and modular sequence to prepare high-quality inputs
      for downstream tasks. The steps are as follows:
    </p>

    <img src="clarity/images/preprocessing_pipeline.png" alt="Preprocessing Pipeline Flowchart">

    <p class="text">
      This end-to-end pipeline ensures that raw audio recordings are systematically cleaned, transformed, and
      structured, making them ready for efficient modeling and analysis. We have provided the preprocessing code as
      well:
    </p>
  </div>
  <div class="container blog main">
    <details>
      <summary style="padding-top: 1em; font-family:Arial, Helvetica, sans-serif;">Show/Hide Full Pipeline Code
      </summary>
      <pre><code class="python">
# ------------------- optional preprocessing ------------------- #
def rename_audio_files(csv_path: str,
                       audio_dir: str,
                       output_csv: str = "renamed_metadata.csv") -> None:
    """
    Renames audio files based on Participant and Label and saves new metadata.

    Args:
        csv_path (str): Path to the input metadata CSV.
        audio_dir (str): Directory containing audio files.
        output_csv (str): Filename for the output metadata CSV.
    """
    df = pl.read_csv(csv_path)
    renamed_files = []
    file_counts = {}

    for file in df.iter_rows(named=True):
        org_name = file['Filename']
        id = file['Participant']
        label = file['Label']

        key = (id, label)
        file_counts[key] = file_counts.get(key, 0) + 1
        index = file_counts[key]

        new_name = f"{id}_{label}_{index}.wav"
        old_path = os.path.join(audio_dir, org_name)
        new_path = os.path.join(audio_dir, new_name)

        if not os.path.exists(old_path):
            print(f"❌ File not found: {old_path}. Skipping renaming process.")
            return  # Exit the function immediately if any file is missing

        os.rename(old_path, new_path)
        renamed_files.append((new_name, id, label, index))

    # If renaming was successful, save the updated metadata
    renamed_df = pl.DataFrame(renamed_files, schema=["Filename", "ID", "Label", "Index"], orient="row")
    output_path = os.path.join(audio_dir, output_csv)
    renamed_df.write_csv(output_path)
    
def save_audio_comparison(original_y: np.ndarray, 
                           cleaned_y: np.ndarray, 
                           sr: int, 
                           filename: str, 
                           output_dir: str = 'audio_comparisons') -> None:
    
    os.makedirs(output_dir, exist_ok=True)
    base_name = os.path.splitext(filename)[0]
    original_path = os.path.join(output_dir, f"{base_name}_original.wav")
    cleaned_path = os.path.join(output_dir, f"{base_name}_cleaned.wav")

    sf.write(original_path, original_y, sr)
    sf.write(cleaned_path, cleaned_y, sr)


def clean_audio(y: np.ndarray, 
                sr: int, 
                denoise: bool = True, 
                remove_silence: bool = True,
                normalize: bool = True,
                min_silence_duration: float = 0.3,
                silence_threshold: float = -40) -> np.ndarray:
    """
    Enhanced audio cleaning function tailored for voice recordings of autistic individuals.

    Parameters:
        y (np.ndarray): Input audio time series
        sr (int): Sampling rate
        denoise (bool): Apply noise reduction
        remove_silence (bool): Remove long silent segments
        normalize (bool): Normalize audio amplitude
        min_silence_duration (float): Minimum duration of silence to remove (in seconds)
        silence_threshold (float): Decibel threshold for silence detection

    Returns:
        np.ndarray: Cleaned audio time series
    """
    if len(y) == 0:
        return y  # Return empty if the input is empty

    cleaned_audio = y.copy()

    if normalize:
        cleaned_audio = librosa.util.normalize(cleaned_audio)

    # Noise reduction using spectral gating
    if denoise:
        stft = librosa.stft(cleaned_audio)                # Compute STFT with valid n_fft
        mag, phase = librosa.magphase(stft)               # Magnitude and phase
        noise_threshold = np.median(mag) * 0.5
        mask = mag > noise_threshold                      # Apply noise threshold mask
        cleaned_stft = stft * mask                        
        cleaned_audio = librosa.istft(cleaned_stft)       # Convert back to time domain

    # Remove long silent segments
    if remove_silence:
        frame_length = int(sr * min_silence_duration)
        hop_length = max(1, frame_length // 2)  # Ensure hop_length is at least 1

        non_silent_frames = librosa.effects.split(
            cleaned_audio, 
            top_db=abs(silence_threshold), 
            frame_length=frame_length, 
            hop_length=hop_length
        )

        if len(non_silent_frames) == 0:
            return np.array([])  # Return empty if all frames are silent

        cleaned_audio = np.concatenate([
            cleaned_audio[start:end] for start, end in non_silent_frames
        ])

    # Apply high-pass filter to reduce low-frequency noise
    b, a = signal.butter(6, 80 / (sr/2), btype='high')
    cleaned_audio = signal.filtfilt(b, a, cleaned_audio)

    return cleaned_audio


def compute_or_load_global_stats(ys: List[np.ndarray],
                                 sr: int=SAMPLE_RATE,
                                 n_mels: int = 128,
                                 method: str = "zscore",
                                 stats_file: str = "global_stats.json",
                                 force_recompute: bool = False) -> Dict[str, float]:
    """
    Computes or loads global normalization stats for Mel spectrograms.

    Parameters:
        ys (List[np.ndarray]): List of raw audio waveforms.
        sr (int): Sample rate.
        n_mels (int): Number of Mel bands.
        method (str): 'zscore' or 'minmax'.
        stats_file (str): Path to save/load stats JSON.
        force_recompute (bool): If True, recomputes even if file exists.

    Returns:
        Dict[str, float]: Stats dictionary (mean/std or min/max).
    """

    if not force_recompute and os.path.exists(stats_file):
        print(f"🗂️ Loading global stats from {stats_file}")
        with open(stats_file, "r") as f:
            return json.load(f)

    print(f"📊 Computing global stats with method '{method}'...")
    all_values = []

    for y in ys:
        S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)
        S_db = librosa.power_to_db(S, ref=np.max)
        all_values.append(S_db.flatten())

    all_values = np.concatenate(all_values)
    stats = {}

    if method == "zscore":
        stats = {
            "mean": float(np.mean(all_values)),
            "std": float(np.std(all_values))
        }
    elif method == "minmax":
        stats = {
            "min": float(np.min(all_values)),
            "max": float(np.max(all_values))
        }
    else:
        raise ValueError("Unsupported method. Use 'zscore' or 'minmax'.")

    # Save stats to file
    with open(stats_file, "w") as f:
        json.dump(stats, f)
        print(f"💾 Saved global stats to {stats_file}")

    return stats


def audio_to_spectrogram(y: np.ndarray,
                         sr: int=SAMPLE_RATE,
                         n_mels: int = 128,
                         target_length: int = 128,
                         normalization: str = "minmax",
                         normalize_scope: str = "sample",  # "sample" or "global"
                         global_stats: dict = None) -> np.ndarray:
    """
    Converts a raw audio waveform into a normalized, fixed-size Mel spectrogram.

    Parameters:
        y (np.ndarray): Raw audio waveform.
        sr (int): Sample rate of the audio.
        n_mels (int): Number of Mel bands.
        target_length (int): Number of time steps to pad/crop to.
        normalization (str): 'minmax' or 'zscore'.
        normalize_scope (str): 'sample' for per-sample normalization,
                               'global' for dataset-wide using global_stats.
        global_stats (dict): Required if normalize_scope='global'. Should contain
                             'mean' and 'std' or 'min' and 'max'.

    Returns:
        np.ndarray: Mel spectrogram of shape (n_mels, target_length).
    """

    def _normalize(S_db: np.ndarray, method: str, scope: str, stats: dict = None):
        if scope == "sample":
            if method == "minmax":
                return (S_db - S_db.min()) / (S_db.max() - S_db.min())
            elif method == "zscore":
                mean = np.mean(S_db)
                std = np.std(S_db)
                return (S_db - mean) / std
        else:
            if method == "minmax":
                return (S_db - stats["min"]) / (stats["max"] - stats["min"])
            elif method == "zscore":
                return (S_db - stats["mean"]) / stats["std"]

    def _pad_or_crop(S: np.ndarray, target_len: int):
        current_len = S.shape[1]
        if current_len < target_len:
            pad_width = target_len - current_len
            return np.pad(S, ((0, 0), (0, pad_width)), mode='constant')
        else:
            return S[:, :target_len]
    
    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)
    S_db = librosa.power_to_db(S, ref=np.max)

    S_norm = _normalize(S_db, method=normalization, scope=normalize_scope, stats=global_stats)
    S_fixed = _pad_or_crop(S_norm, target_len=target_length)

    return S_fixed

    # ----------------------- pipeline ----------------------- #
    def pipeline(rename: bool = False, 
                 limit: Union[int, None] = None,
                 clean_audio_params: dict = None,
                 save_comparisons: bool = False,
                 ) -> pl.DataFrame:
        """
        Pipeline to run all preprocessing functions with timing and optional audio cleaning.
        Only supports saving to .parquet (not CSV) to handle arrays properly.
        """
        print("🚀 Starting preprocessing pipeline...")
        start = time.time()
        
        if rename:
            t0 = time.time()
            rename_audio_files(
                csv_path=ORG_CSV_PATH,
                audio_dir=AUDIO_DIR,
            )
            print(f"📝 rename_audio_files completed in {time.time() - t0:.2f} seconds")
    
        t0 = time.time()
        df = load_audio_metadata(
            csv_path=RENAME_CSV_PATH,
            audio_dir=AUDIO_DIR,
            limit=limit,
            clean_audio_params=clean_audio_params,
            save_comparisons=save_comparisons
        )
        print(f"⏳ load_audio_metadata completed in {time.time() - t0:.2f} seconds")
    
        t0 = time.time()
        stats = compute_or_load_global_stats(df["Audio"].to_numpy(), sr=SAMPLE_RATE)
        print(f"🧮 compute_or_load_global_stats completed in {time.time() - t0:.2f} seconds")
        
        print("\n📈 Computed Statistics:")
        for k, v in stats.items(): 
            print(f"  {k}: {v}")
        print()
    
        t0 = time.time()
        df = df.with_columns([
            pl.col("Audio").map_elements(lambda y: audio_to_spectrogram(
                y=np.array(y),
                sr=SAMPLE_RATE,
                normalization='zscore',
                normalize_scope='global',
                global_stats=stats
            ), return_dtype=pl.Object).alias("Spectrogram")
        ])
        print(f"🔊 Spectrogram generation completed in {time.time() - t0:.2f} seconds")
        
        print(f"🏁 Full pipeline completed in {time.time() - start:.2f} seconds\n")
        print(df)
        
        return df
    
        custom_clean_params = {
            'denoise': True,
            'remove_silence': True,
            'normalize': True,
            'min_silence_duration': 0.3,
            'silence_threshold': -40
        }
        
        df = pipeline(
            rename=False, 
            limit=None,
            clean_audio_params=custom_clean_params,
            save_comparisons=False
        )
        
        # Convert data to numpy arrays for serialization
        df = df.with_columns([
            pl.col("Audio").map_elements(
                lambda y: np.array(y, dtype=np.float64).tolist(), return_dtype=pl.List(pl.Float64)
            ),
            pl.col("Spectrogram").map_elements(
                lambda s: np.array(s, dtype=np.float64).tolist(), return_dtype=pl.List(pl.List(pl.Float64))
            )
        ])
        # saves df to a pkl file to be cached and used later
        with open("processed_data.pkl", "wb") as f:
            pickle.dump(df, f)
        </code></pre>
    </details>

    <p class="text">
      This process results in the following DataFrame:
    </p>
    <p style="font-family: 'Courier New', Courier, monospace;">🚀 Starting preprocessing pipeline...</p>
    <p style="font-family: 'Courier New', Courier, monospace;">⏳ load_audio_metadata completed in 138.89 seconds</p>
    <p style="font-family: 'Courier New', Courier, monospace;">🗂️ Loading global stats from global_stats.json</p>
    <p style="font-family: 'Courier New', Courier, monospace;">🧮 compute_or_load_global_stats completed in 0.16 seconds
    </p>

    <p style="font-family: 'Courier New', Courier, monospace;">📈 Computed Statistics:</p>
    <p style="font-family: 'Courier New', Courier, monospace;">mean: -55.975612227106474</p>
    <p style="font-family: 'Courier New', Courier, monospace;">std: 18.55726476893056</p>

    <p style="font-family: 'Courier New', Courier, monospace;">🔊 Spectrogram generation completed in 29.24 seconds</p>
    <p style="font-family: 'Courier New', Courier, monospace;">🏁 Full pipeline completed in 168.32 seconds</p>
    </p>

    <p class="text">
      We have also created a loading function for the <code>.pkl</code> so we don't have to rerun the whole pipeline
      every time we want to jump back into the notebook:
    </p>
    <pre><code class="python">
def open_pickle(path: str) -> pl.DataFrame:
    with open(path, "rb") as f:
        df = pickle.load(f)
    return df
    </code></pre>

    <h2>DataFrame Overview</h2>

    <p class="text">
      The Dataframe is stored as a <code>.pkl</code> containing key information about the audio including: Filename,
      Audio, ID, Label, Duration, Index, and Spectrogram. The audio is stored as a list of floats, the spectrogram is
      stored as a list of lists of floats, and the rest are strings or floats.
    </p>
  </div>

  <div class="container blog main gray">
    <div class="data-table-container">
      <table class="data-table">
        <thead>
          <tr>
            <th class="data-table-header">Filename</th>
            <th class="data-table-header">Audio</th>
            <th class="data-table-header">ID</th>
            <th class="data-table-header">Label</th>
            <th class="data-table-header">Duration</th>
            <th class="data-table-header">Index</th>
            <th class="data-table-header">Spectrogram</th>
          </tr>
          <tr>
            <td class="data-table-datatype">str</td>
            <td class="data-table-datatype">list[f64]</td>
            <td class="data-table-datatype">str</td>
            <td class="data-table-datatype">str</td>
            <td class="data-table-datatype">f64</td>
            <td class="data-table-datatype">i64</td>
            <td class="data-table-datatype">list[list[f64]]</td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td class="data-table-cell">P01_dysregulation-sick_1.wav</td>
            <td class="data-table-cell">[-0.107705, -0.120444, ...]</td>
            <td class="data-table-cell">P01</td>
            <td class="data-table-cell">dysregulation-sick</td>
            <td class="data-table-cell">0.25542</td>
            <td class="data-table-cell">1</td>
            <td class="data-table-cell">[[1.065977, 0.518101, ...], ...]</td>
          </tr>
          <tr>
            <td class="data-table-cell">P01_dysregulation-sick_2.wav</td>
            <td class="data-table-cell">[0.145759, 0.148596, ...]</td>
            <td class="data-table-cell">P01</td>
            <td class="data-table-cell">dysregulation-sick</td>
            <td class="data-table-cell">0.928798</td>
            <td class="data-table-cell">2</td>
            <td class="data-table-cell">[[1.004109, 0.631097, ...], ...]</td>
          </tr>
          <tr>
            <td class="data-table-cell">P01_dysregulation-sick_3.wav</td>
            <td class="data-table-cell">[0.034167, 0.022343, ...]</td>
            <td class="data-table-cell">P01</td>
            <td class="data-table-cell">dysregulation-sick</td>
            <td class="data-table-cell">1.137778</td>
            <td class="data-table-cell">3</td>
            <td class="data-table-cell">[[0.113385, -0.084511, ...], ...]</td>
          </tr>
          <tr>
            <td class="data-table-cell">P01_dysregulation-sick_4.wav</td>
            <td class="data-table-cell">[-0.005172, -0.009896, ...]</td>
            <td class="data-table-cell">P01</td>
            <td class="data-table-cell">dysregulation-sick</td>
            <td class="data-table-cell">3.645533</td>
            <td class="data-table-cell">4</td>
            <td class="data-table-cell">[[-0.463286, -0.999457, ...], ...]</td>
          </tr>
          <tr>
            <td class="data-table-cell">P01_dysregulation-sick_5.wav</td>
            <td class="data-table-cell">[-0.0023, -0.001397, ...]</td>
            <td class="data-table-cell">P01</td>
            <td class="data-table-cell">dysregulation-sick</td>
            <td class="data-table-cell">0.394739</td>
            <td class="data-table-cell">5</td>
            <td class="data-table-cell">[[0.945787, 0.609868, ...], ...]</td>
          </tr>
          <tr>
            <td class="data-table-cell" colspan="7" class="data-table-truncated">...</td>
          </tr>
          <tr>
            <td class="data-table-cell">P16_delighted_135.wav</td>
            <td class="data-table-cell">[0.000027, 0.000085, ...]</td>
            <td class="data-table-cell">P16</td>
            <td class="data-table-cell">delighted</td>
            <td class="data-table-cell">1.044898</td>
            <td class="data-table-cell">135</td>
            <td class="data-table-cell">[[-1.28051, -1.294608, ...], ...]</td>
          </tr>
          <tr>
            <td class="data-table-cell">P16_delighted_136.wav</td>
            <td class="data-table-cell">[0.016696, 0.013343, ...]</td>
            <td class="data-table-cell">P16</td>
            <td class="data-table-cell">delighted</td>
            <td class="data-table-cell">0.638549</td>
            <td class="data-table-cell">136</td>
            <td class="data-table-cell">[[0.801103, 0.513365, ...], ...]</td>
          </tr>
          <tr>
            <td class="data-table-cell">P16_delighted_137.wav</td>
            <td class="data-table-cell">[0.008781, 0.005037, ...]</td>
            <td class="data-table-cell">P16</td>
            <td class="data-table-cell">delighted</td>
            <td class="data-table-cell">0.766259</td>
            <td class="data-table-cell">137</td>
            <td class="data-table-cell">[[0.40735, 0.053851, ...], ...]</td>
          </tr>
          <tr>
            <td class="data-table-cell">P16_delighted_138.wav</td>
            <td class="data-table-cell">[0.015408, 0.010745, ...]</td>
            <td class="data-table-cell">P16</td>
            <td class="data-table-cell">delighted</td>
            <td class="data-table-cell">0.743039</td>
            <td class="data-table-cell">138</td>
            <td class="data-table-cell">[[0.439509, 0.102873, ...], ...]</td>
          </tr>
          <tr>
            <td class="data-table-cell">P16_delighted_139.wav</td>
            <td class="data-table-cell">[-0.00114, -0.003822, ...]</td>
            <td class="data-table-cell">P16</td>
            <td class="data-table-cell">delighted</td>
            <td class="data-table-cell">1.277098</td>
            <td class="data-table-cell">139</td>
            <td class="data-table-cell">[[0.294103, -0.048475, ...], ...]</td>
          </tr>
        </tbody>
        <tfoot>
          <tr>
            <td colspan="7" class="data-table-footer">
              shape: (7,077, 7) | Dataset contains audio samples from multiple participants with various intent labels
            </td>
          </tr>
        </tfoot>
      </table>
    </div>
  </div>

  <div class="container blog main">
    <h2>Data Exploration</h2>

    <p class="text">
      We explored the dataset to understand the distribution of labels and the characteristics of the audio samples.
      We visualized the distribution of labels using a bar plot, which showed that the dataset is relatively balanced
      across different intent categories.
    </p>
    <pre><code class="python">
print(f"Dataframe shape: {df.shape}")
df.describe()
        </code></pre>
  </div>
  <div class="container blog main gray">
    <div class="data-table-container">
      Dataframe shape: (7077, 7)
      <table class="data-table">
        <thead>
          <tr>
            <th class="data-table-header">statistic</th>
            <th class="data-table-header">Filename</th>
            <th class="data-table-header">Audio</th>
            <th class="data-table-header">ID</th>
            <th class="data-table-header">Label</th>
            <th class="data-table-header">Duration</th>
            <th class="data-table-header">Index</th>
            <th class="data-table-header">Spectrogram</th>
          </tr>
          <tr>
            <td class="data-table-datatype">str</td>
            <td class="data-table-datatype">str</td>
            <td class="data-table-datatype">f64</td>
            <td class="data-table-datatype">str</td>
            <td class="data-table-datatype">str</td>
            <td class="data-table-datatype">f64</td>
            <td class="data-table-datatype">f64</td>
            <td class="data-table-datatype">f64</td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td class="data-table-cell">"count"</td>
            <td class="data-table-cell">"7077"</td>
            <td class="data-table-cell">7077.0</td>
            <td class="data-table-cell">"7077"</td>
            <td class="data-table-cell">"7077"</td>
            <td class="data-table-cell">7077.0</td>
            <td class="data-table-cell">7077.0</td>
            <td class="data-table-cell">7077.0</td>
          </tr>
          <tr>
            <td class="data-table-cell">"null_count"</td>
            <td class="data-table-cell">"0"</td>
            <td class="data-table-cell">0.0</td>
            <td class="data-table-cell">"0"</td>
            <td class="data-table-cell">"0"</td>
            <td class="data-table-cell">0.0</td>
            <td class="data-table-cell">0.0</td>
            <td class="data-table-cell">0.0</td>
          </tr>
          <tr>
            <td class="data-table-cell">"mean"</td>
            <td class="data-table-cell">null</td>
            <td class="data-table-cell">null</td>
            <td class="data-table-cell">null</td>
            <td class="data-table-cell">null</td>
            <td class="data-table-cell">1.240378</td>
            <td class="data-table-cell">154.396637</td>
            <td class="data-table-cell">null</td>
          </tr>
          <tr>
            <td class="data-table-cell">"std"</td>
            <td class="data-table-cell">null</td>
            <td class="data-table-cell">null</td>
            <td class="data-table-cell">null</td>
            <td class="data-table-cell">null</td>
            <td class="data-table-cell">1.012603</td>
            <td class="data-table-cell">158.147559</td>
            <td class="data-table-cell">null</td>
          </tr>
          <tr>
            <td class="data-table-cell">"min"</td>
            <td class="data-table-cell">"P01_bathroom_1.wav"</td>
            <td class="data-table-cell">null</td>
            <td class="data-table-cell">"P01"</td>
            <td class="data-table-cell">"affectionate"</td>
            <td class="data-table-cell">0.08127</td>
            <td class="data-table-cell">1.0</td>
            <td class="data-table-cell">null</td>
          </tr>
          <tr>
            <td class="data-table-cell">"25%"</td>
            <td class="data-table-cell">null</td>
            <td class="data-table-cell">null</td>
            <td class="data-table-cell">null</td>
            <td class="data-table-cell">null</td>
            <td class="data-table-cell">0.592109</td>
            <td class="data-table-cell">36.0</td>
            <td class="data-table-cell">null</td>
          </tr>
          <tr>
            <td class="data-table-cell">"50%"</td>
            <td class="data-table-cell">null</td>
            <td class="data-table-cell">null</td>
            <td class="data-table-cell">null</td>
            <td class="data-table-cell">null</td>
            <td class="data-table-cell">0.940408</td>
            <td class="data-table-cell">104.0</td>
            <td class="data-table-cell">null</td>
          </tr>
          <tr>
            <td class="data-table-cell">"75%"</td>
            <td class="data-table-cell">null</td>
            <td class="data-table-cell">null</td>
            <td class="data-table-cell">null</td>
            <td class="data-table-cell">null</td>
            <td class="data-table-cell">1.520907</td>
            <td class="data-table-cell">216.0</td>
            <td class="data-table-cell">null</td>
          </tr>
          <tr>
            <td class="data-table-cell">"max"</td>
            <td class="data-table-cell">"P16_social_9.wav"</td>
            <td class="data-table-cell">null</td>
            <td class="data-table-cell">"P16"</td>
            <td class="data-table-cell">"yes"</td>
            <td class="data-table-cell">14.048073</td>
            <td class="data-table-cell">781.0</td>
            <td class="data-table-cell">null</td>
          </tr>
        </tbody>
        <tfoot>
          <tr>
            <td colspan="8" class="data-table-footer">
              shape: (9, 8) | Dataset statistics summary for audio samples collection
            </td>
          </tr>
        </tfoot>
      </table>
    </div>
  </div>
  <div class="container blog main">
    <p class="text">
      YIPEEEEE 🎉
      The DataFrame contains no null values, indicating that all audio files are present and correctly labeled.
    </p>

    <pre><code class="python">
df.null_count()
        </code></pre>
  </div>
  <div class="container blog main gray">
    <div class="data-table-container">
      <table class="data-table">
        <thead>
          <tr>
            <th class="data-table-header">Filename</th>
            <th class="data-table-header">Audio</th>
            <th class="data-table-header">ID</th>
            <th class="data-table-header">Label</th>
            <th class="data-table-header">Duration</th>
            <th class="data-table-header">Index</th>
            <th class="data-table-header">Spectrogram</th>
          </tr>
          <tr>
            <td class="data-table-datatype">u32</td>
            <td class="data-table-datatype">u32</td>
            <td class="data-table-datatype">u32</td>
            <td class="data-table-datatype">u32</td>
            <td class="data-table-datatype">u32</td>
            <td class="data-table-datatype">u32</td>
            <td class="data-table-datatype">u32</td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td class="data-table-cell">0</td>
            <td class="data-table-cell">0</td>
            <td class="data-table-cell">0</td>
            <td class="data-table-cell">0</td>
            <td class="data-table-cell">0</td>
            <td class="data-table-cell">0</td>
            <td class="data-table-cell">0</td>
          </tr>
        </tbody>
        <tfoot>
          <tr>
            <td colspan="7" class="data-table-footer">
              shape: (1, 7) | Zero count table
            </td>
          </tr>
        </tfoot>
      </table>
    </div>
  </div>
  <div class="container blog main">
    <p class="text">We will now examine label distributions: </p>

    <pre><code class="python">
label_counts = df['Label'].value_counts()
label_counts
        </code></pre>
  </div>
  <div class="container blog main gray">
    <div class="data-table-container">
      <table class="data-table">
        <thead>
          <tr>
            <th class="data-table-header">Label</th>
            <th class="data-table-header">count</th>
          </tr>
          <tr>
            <td class="data-table-datatype">str</td>
            <td class="data-table-datatype">u32</td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td class="data-table-cell">"bathroom"</td>
            <td class="data-table-cell">20</td>
          </tr>
          <tr>
            <td class="data-table-cell">"more"</td>
            <td class="data-table-cell">22</td>
          </tr>
          <tr>
            <td class="data-table-cell">"protest"</td>
            <td class="data-table-cell">21</td>
          </tr>
          <tr>
            <td class="data-table-cell">"social"</td>
            <td class="data-table-cell">634</td>
          </tr>
          <tr>
            <td class="data-table-cell">"request"</td>
            <td class="data-table-cell">419</td>
          </tr>
          <tr>
            <td class="data-table-cell" colspan="2" class="data-table-truncated">...</td>
          </tr>
          <tr>
            <td class="data-table-cell">"dysregulated"</td>
            <td class="data-table-cell">704</td>
          </tr>
          <tr>
            <td class="data-table-cell">"happy"</td>
            <td class="data-table-cell">61</td>
          </tr>
          <tr>
            <td class="data-table-cell">"delighted"</td>
            <td class="data-table-cell">1272</td>
          </tr>
          <tr>
            <td class="data-table-cell">"laugh"</td>
            <td class="data-table-cell">8</td>
          </tr>
          <tr>
            <td class="data-table-cell">"frustrated"</td>
            <td class="data-table-cell">1536</td>
          </tr>
        </tbody>
        <tfoot>
          <tr>
            <td colspan="2" class="data-table-footer">
              shape: (22, 2) | Count of audio samples by label category
            </td>
          </tr>
        </tfoot>
      </table>
    </div>
  </div>

  <div class="container blog main">
    <p class="text">
      There are discrepencies of how many labels exist per group. The mean is approximately 320, but there is a high
      deviation of nearly 500
    </p>

    <pre><code class="python">
label_counts.describe()
        </code></pre>
  </div>

  <div class="container blog main gray">
    <div class="data-table-container">
      <table class="data-table">
        <thead>
          <tr>
            <th class="data-table-header">statistic</th>
            <th class="data-table-header">Label</th>
            <th class="data-table-header">count</th>
          </tr>
          <tr>
            <td class="data-table-datatype">str</td>
            <td class="data-table-datatype">str</td>
            <td class="data-table-datatype">f64</td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td class="data-table-cell">"count"</td>
            <td class="data-table-cell">"22"</td>
            <td class="data-table-cell">22.0</td>
          </tr>
          <tr>
            <td class="data-table-cell">"null_count"</td>
            <td class="data-table-cell">"0"</td>
            <td class="data-table-cell">0.0</td>
          </tr>
          <tr>
            <td class="data-table-cell">"mean"</td>
            <td class="data-table-cell">null</td>
            <td class="data-table-cell">321.681818</td>
          </tr>
          <tr>
            <td class="data-table-cell">"std"</td>
            <td class="data-table-cell">null</td>
            <td class="data-table-cell">551.158208</td>
          </tr>
          <tr>
            <td class="data-table-cell">"min"</td>
            <td class="data-table-cell">"affectionate"</td>
            <td class="data-table-cell">3.0</td>
          </tr>
          <tr>
            <td class="data-table-cell">"25%"</td>
            <td class="data-table-cell">null</td>
            <td class="data-table-cell">12.0</td>
          </tr>
          <tr>
            <td class="data-table-cell">"50%"</td>
            <td class="data-table-cell">null</td>
            <td class="data-table-cell">61.0</td>
          </tr>
          <tr>
            <td class="data-table-cell">"75%"</td>
            <td class="data-table-cell">null</td>
            <td class="data-table-cell">419.0</td>
          </tr>
          <tr>
            <td class="data-table-cell">"max"</td>
            <td class="data-table-cell">"yes"</td>
            <td class="data-table-cell">1885.0</td>
          </tr>
        </tbody>
        <tfoot>
          <tr>
            <td colspan="3" class="data-table-footer">
              shape: (9, 3) | Statistical summary for label distribution
            </td>
          </tr>
        </tfoot>
      </table>
    </div>
  </div>

  <div class="container blog main">
    <p class="text">
      The plot shows us that there is imbalance between labels. We should keep this into consideration during analysis
      and training.
    </p>

    <pre><code class="python">
colors = plt.cm.viridis(np.linspace(0, 1, len(label_counts))) # pretty colors 
plt.figure(figsize=(10, 6))
plt.bar(label_counts['Label'], label_counts['count'], color=colors)
plt.title('Distribution of Labels')
plt.xlabel('Label')
plt.ylabel('Count')
plt.xticks(rotation=70) # so labels don't overlap
plt.tight_layout()
plt.show()
        </code></pre>
  </div>

  <div class="container blog main">
    <img src="clarity/images/LabelDistribution.png" alt="LabelDistribution Distribution Bar Graph">

    <p class="text">
      Let us examine the distribution of labels by each participant:
    </p>
  </div>
  <div class="container blog main">
    <pre><code class="python">
# Plot for each person
participant_label_counts = df.group_by(['ID', 'Label']).agg(pl.len().alias('Count'))
participant_label_counts = participant_label_counts.to_pandas()

participant_ids = participant_label_counts['ID'].unique()
n_cols = 4
n_rows = 2

# Create subplots
fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 4), constrained_layout=True)
fig.suptitle('Label Distribution per Participant', fontsize=16)

for idx, participant_id in enumerate(participant_ids):
ax = axes[idx // n_cols, idx % n_cols]  # next subplot
data = participant_label_counts[participant_label_counts['ID'] == participant_id]
colors = plt.cm.viridis(np.linspace(0, 1, len(data['Label'])))
ax.bar(data['Label'], data['Count'], color=colors)
ax.set_title(f'Participant {participant_id}')
ax.set_xlabel('Label')
ax.set_ylabel('Count')
ax.tick_params(axis='x', rotation=70)
          
plt.show()
        </code></pre>
  </div>

  <div class="container blog max gray">
    <div class="slideshow">
      <div class="navigation">
        <!-- Using FontAwesome Pro -->
        <!-- <a class="button icon" id="prev_btn"><i class="fa-solid fa-left" ></i></a>
              <a class="button icon" id="next_btn"><i class="fa-solid fa-right"></i></a> -->
        <!-- Using FontAwesome Free -->
        <a class="button icon" id="prev_btn"><i class="fa-solid fa-arrow-left"></i></a>
        <a class="button icon" id="next_btn"><i class="fa-solid fa-arrow-right"></i></a>
      </div>
      <div class="slider">
        <div class="slider-item">
          <img src="clarity/images/P01LabelDistribution.png" alt="P01 Label Distribution">
        </div>
        <div class="slider-item">
          <img src="clarity/images/P02LabelDistribution.png" alt="P02 Label Distribution">
        </div>
        <div class="slider-item">
          <img src="clarity/images/P03LabelDistribution.png" alt="P03 Label Distribution">
        </div>
        <div class="slider-item">
          <img src="clarity/images/P05LabelDistribution.png" alt="P05 Label Distribution">
        </div>
        <div class="slider-item">
          <img src="clarity/images/P06LabelDistribution.png" alt="P06 Label Distribution">
        </div>
        <div class="slider-item">
          <img src="clarity/images/P08LabelDistribution.png" alt="P08 Label Distribution">
        </div>
        <div class="slider-item">
          <img src="clarity/images/P11LabelDistribution.png" alt="P11 Label Distribution">
        </div>
        <div class="slider-item">
          <img src="clarity/images/P16LabelDistribution.png" alt="P16 Label Distribution">
        </div>
      </div>
    </div>
  </div>

  <div class="container blog main">
    <p class="text">
      Consider the distribution of labels by participant above. We find the following:
    </p>
    <ul style="padding: 2em; display: flex; flex-direction: column; gap: 1em;" class="text">
      <li>
        <p class="text">Label Distribution: The dataset contains a variety of labels, with "frustrated" and "delighted"
          being the most common.</p>
      </li>
      <li>
        <p class="text">Label Variety: Some participants exhibit a wide range of labels, while others are more
          consistent in their responses.</p>
      </li>
      <li>
        <p class="text">Customized Approaches: The differences in label distribution across participants suggest that
          personalized models might be more effective (depending on model type).</p>
      </li>
    </ul>

    <p class="text">
      We also examine the distribution in audio lengths. The variation in lengths tells us we must pad our audio prior
      to feature analysis.
    </p>

    <pre><code class="python">
df = df.with_columns([
    pl.col("Audio").map_elements(
      lambda a: len(a), return_dtype=pl.Float64
    ).alias("Audio Length")
])
df['Audio Length'].describe()
      </code></pre>
  </div>

  <div class="container blog main gray">
    <div class="data-table-container">
      <table class="data-table">
        <thead>
          <tr>
            <th class="data-table-header">statistic</th>
            <th class="data-table-header">value</th>
          </tr>
          <tr>
            <td class="data-table-datatype">str</td>
            <td class="data-table-datatype">f64</td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td class="data-table-cell">"count"</td>
            <td class="data-table-cell">7077.0</td>
          </tr>
          <tr>
            <td class="data-table-cell">"null_count"</td>
            <td class="data-table-cell">0.0</td>
          </tr>
          <tr>
            <td class="data-table-cell">"mean"</td>
            <td class="data-table-cell">54700.662993</td>
          </tr>
          <tr>
            <td class="data-table-cell">"std"</td>
            <td class="data-table-cell">44655.798259</td>
          </tr>
          <tr>
            <td class="data-table-cell">"min"</td>
            <td class="data-table-cell">3584.0</td>
          </tr>
          <tr>
            <td class="data-table-cell">"25%"</td>
            <td class="data-table-cell">26112.0</td>
          </tr>
          <tr>
            <td class="data-table-cell">"50%"</td>
            <td class="data-table-cell">41472.0</td>
          </tr>
          <tr>
            <td class="data-table-cell">"75%"</td>
            <td class="data-table-cell">67072.0</td>
          </tr>
          <tr>
            <td class="data-table-cell">"max"</td>
            <td class="data-table-cell">619520.0</td>
          </tr>
        </tbody>
        <tfoot>
          <tr>
            <td colspan="3" class="data-table-footer">
              shape: (9, 3) | Statistical summary for label distribution
            </td>
          </tr>
        </tfoot>
      </table>
    </div>
  </div>

  <div class="container blog main">
    <h2>Spectograms of Unique Labels</h2>
  
    <p class="text">
      This grid displays one <span style="font-weight: bold">Mel spectrogram</span> for each unique vocalization label in the dataset. Each spectrogram represents a <span style="font-weight: bold">single audio sample</span> randomly selected from that label group.
    </p>
  
    <h3>What is a Spectrogram?</h3>
  
    <p class="text">
      A spectrogram is a time-frequency visualization of sound. It shows how energy (brightness) is distributed across frequency bins (y-axis) over time (x-axis). Brighter regions indicate more intensity at that frequency and time.
    </p>
  
    <hr>
  
    <h3>Interpretation Notes</h3>
  
    <ul style="padding-left: 20px; list-style-type: disc;">
      <li>
        <p class="text"><span style="font-weight: bold">Flat colors at end of some labels</span>: You may notice that many spectrograms appear to <span style="font-weight: bold">suddenly turn into a solid blue color</span> after a certain point. This occurs because:</p>
        <ul style="padding-left: 20px; list-style-type: circle;">
          <li>
            <p class="text">All spectrograms have been <span style="font-weight: bold">padded or cropped to a fixed width</span> (<code>target_length</code>), ensuring uniform input size for modeling.</p>
          </li>
          <li>
            <p class="text">When the original audio sample is <span style="font-weight: bold">shorter than the target time length</span>, the remaining time frames are filled with <span style="font-weight: bold">zeros</span> — resulting in that flat, dark blue region on the right.</p>
          </li>
          <li>
            <p class="text">This is done to make all inputs the same shape for consistent processing and modeling (e.g., embeddings or CNNs).</p>
          </li>
        </ul>
      </li>
    </ul>
  
    <p class="text">
      These spectrograms give an intuitive view of the <span style="font-weight: bold">acoustic patterns</span> present in each vocalization type — for example:
    </p>
    <ul style="padding-left: 20px; list-style-type: disc;">
      <li>
        <p class="text"><code>"YES"</code> shows low-frequency harmonics</p>
      </li>
      <li>
        <p class="text"><code>"FRUSTRATED"</code> is noisier and denser</p>
      </li>
      <li>
        <p class="text"><code>"SELF-TALK"</code> often contains repeating patterns</p>
      </li>
      <li>
        <p class="text"><code>"GLEE"</code> and <code>"DELIGHTED"</code> appear more tonal or melodic</p>
      </li>
    </ul>
  
    <p class="text">
      This kind of visualization helps validate that <span style="font-weight: bold">distinct spectral features</span> exist across labels, supporting downstream classification or clustering tasks.
    </p>
  
    <hr>
  
    <details>
      <summary style="padding-top: 1em; font-family:Arial, Helvetica, sans-serif;">Show/Hide Spectrogram Plot Code</summary>
      <pre><code class="python">
def plot_unique_label_spectrograms_grid(df, n_rows=4, n_cols=6):
    unique_labels = df.select("Label").unique().to_series().to_list()
    total_plots = n_rows * n_cols
    fig, axs = plt.subplots(n_rows, n_cols, figsize=(n_cols * 4, n_rows * 3))
    axs = axs.flatten()
    fig.suptitle("Unique Label Spectogram", fontsize=30)
    
    for idx, label in enumerate(unique_labels):
        ax = axs[idx]
        
        # Get the first spectrogram for this label
        row = df.filter(pl.col("Label") == label).row(0)
        spectrogram = row[df.columns.index("Spectrogram")]
        spectrogram_np = np.array(spectrogram, dtype=np.float32)
        
        if spectrogram_np.ndim == 2:
            im = ax.imshow(spectrogram_np, aspect="auto", origin="lower", cmap="viridis")
            ax.set_title(label.upper(), fontsize=18)
            ax.set_xlabel("Time")
            ax.set_ylabel("Freq")
        else:
            ax.axis("off")
    
    # Hide any unused axes
    for j in range(len(unique_labels), len(axs)):
        axs[j].axis("off")
    
    fig.tight_layout(rect=[0, 0, 1, 0.95])
    plt.show()

plot_unique_label_spectrograms_grid(df)
      </code></pre>
    </details>
    <pre><code class="python">
plot_unique_label_spectrograms_grid(df)
    </code></pre> 
  </div>

  <div class="container blog max gray">
    <div class="slideshow">
      <div class="navigation">
        <!-- Using FontAwesome Pro -->
        <!-- <a class="button icon" id="prev_btn"><i class="fa-solid fa-left" ></i></a>
              <a class="button icon" id="next_btn"><i class="fa-solid fa-right"></i></a> -->
        <!-- Using FontAwesome Free -->
        <a class="button icon" id="prev_btn"><i class="fa-solid fa-arrow-left"></i></a>
        <a class="button icon" id="next_btn"><i class="fa-solid fa-arrow-right"></i></a>
      </div>
      <div class="slider">
        <div class="slider-item">
          <img src="clarity/images/spectrograms/affectionate.png" alt="Affectionate Spectogram">
        </div>
        <div class="slider-item">
          <img src="clarity/images/spectrograms/bathroom.png" alt="Bathroom Spectogram">
      </div>
        <div class="slider-item">
          <img src="clarity/images/spectrograms/delighted.png" alt="Delighted Spectogram">
        </div>
        <div class="slider-item">
          <img src="clarity/images/spectrograms/dysregulated.png" alt="Dysregulated Spectogram">
        </div>
        <div class="slider-item">
          <img src="clarity/images/spectrograms/dysregulation-bathroom.png" alt="Dysregulation-Bathroom Spectogram">
        </div>
        <div class="slider-item">
          <img src="clarity/images/spectrograms/dysregulation-sick.png" alt="Dysregulation-Sick Spectogram">
        </div>
        <div class="slider-item">
          <img src="clarity/images/spectrograms/frustrated.png" alt="Frustrated Spectogram">
        </div>
        <div class="slider-item">
          <img src="clarity/images/spectrograms/glee.png" alt="Frustrated Glee">
        </div>
        <div class="slider-item">
          <img src="clarity/images/spectrograms/greeting.png" alt="Frustrated Greeting">
        </div>
        <div class="slider-item">
          <img src="clarity/images/spectrograms/happy.png" alt="Happy Spectogram">
        </div>
        <div class="slider-item">
          <img src="clarity/images/spectrograms/help.png" alt="Help Spectogram">
        </div>
        <div class="slider-item">
          <img src="clarity/images/spectrograms/hunger.png" alt="Hunger Spectogram">
        </div>
        <div class="slider-item">
          <img src="clarity/images/spectrograms/laugh.png" alt="Laugh Spectogram">
        </div>
        <div class="slider-item">
          <img src="clarity/images/spectrograms/laughter.png" alt="Laughter Spectogram">
        </div>
        <div class="slider-item">
          <img src="clarity/images/spectrograms/more.png" alt="More Spectogram">
        </div>
        <div class="slider-item">
          <img src="clarity/images/spectrograms/no.png" alt="No Spectogram">
        </div>
        <div class="slider-item">
          <img src="clarity/images/spectrograms/protest.png" alt="Protest Spectogram">
        </div>
        <div class="slider-item">
          <img src="clarity/images/spectrograms/request.png" alt="Request Spectogram">
        </div>
        <div class="slider-item">
          <img src="clarity/images/spectrograms/selftalk.png" alt="Selftalk Spectogram">
        </div>
        <div class="slider-item">
          <img src="clarity/images/spectrograms/social.png" alt="Social Spectogram">
        </div>
        <div class="slider-item">
          <img src="clarity/images/spectrograms/tablet.png" alt="Tablet Spectogram">
        </div>
        <div class="slider-item">
          <img src="clarity/images/spectrograms/yes.png" alt="Yes Spectogram">
        </div>
      </div>
  </div>
  </div>
  <div class="container blog main">
    <h2>2D Projection Setup for Audio Data Visualization</h2>
  
    <p class="text">
      To visualize complex audio data, we extract key features, specifically Mel Frequency Cepstral Coefficients (MFCCs) and Pitch Variance, from the audio samples. These features are chosen for their ability to encapsulate the essential characteristics of sound.
    </p>
    
    <p class="text">
      The extracted features are then subjected to dimensionality reduction techniques to project them into a 2D space, facilitating easier visualization and interpretation. The methods used for this purpose include:
    </p>
  
    <ul style="padding-left: 20px; list-style-type: disc;">
      <li>
        <p class="text"><span style="font-weight: bold">PCA (Principal Component Analysis)</span>: Linear transformation technique to reduce the dimensionality while attempting to preserve as much variance as possible.</p>
      </li>
      <li>
        <p class="text"><span style="font-weight: bold">t-SNE (t-Distributed Stochastic Neighbor Embedding)</span>: A non-linear approach, t-SNE is effective in visualizing high-dimensional data by maintaining local relationships in a lower-dimensional space.</p>
      </li>
      <li>
        <p class="text"><span style="font-weight: bold">UMAP (Uniform Manifold Approximation and Projection)</span>: Another non-linear method that excels in preserving both local and global data structures, making it ideal for a nuanced exploration of audio features.</p>
      </li>
    </ul>
  
    <p class="text">
      These projections allow us to visually analyze the clustering and distribution of audio samples, thereby providing insights into the inherent patterns and distinctions within the data.
    </p>
    
    <p class="text">
      We notice no clear clusters forming, requiring possible kernel tricks to get better separation.
    </p>
  
    <details>
      <summary style="padding-top: 1em; font-family:Arial, Helvetica, sans-serif;">Show/Hide Feature Extraction Code</summary>
      <pre><code class="python">
# Get pitch variance as feature for audio projection
def get_pitch_var(y: List[float], sr: int=SAMPLE_RATE):
    y_np = np.array(y, dtype=np.float64)
    f0, voiced_flag, _ = librosa.pyin(
        y_np,
        sr=sr,
        fmin=librosa.note_to_hz('C2'),
        fmax=librosa.note_to_hz('C7')
    )
    if f0 is None:
        return 0.0
    f0_voiced = f0[voiced_flag]
    return float(np.std(f0_voiced)) if len(f0_voiced) > 0 else 0.0

# Gets MFCC means of signal (feature for audio projection)
def get_mfcc_means(y: List[float], sr: int = 16000, n_mfcc: int = 3) -> List[float]:
    y_np = np.array(y, dtype=np.float32)
    mfccs = librosa.feature.mfcc(y=y_np, sr=sr, n_mfcc=n_mfcc)
    return np.mean(mfccs, axis=1).tolist()  # returns [mfcc-1, mfcc-2, mfcc-3]

# Extract MFCC-1, MFCC-2, MFCC-3, and Pitch variance as features for PCA
df = df.with_columns([
    pl.col("Audio").map_elements(lambda y: get_mfcc_means(y)[0], return_dtype=pl.Float64).alias("MFCC-1"),
    pl.col("Audio").map_elements(lambda y: get_mfcc_means(y)[1], return_dtype=pl.Float64).alias("MFCC-2"),
    pl.col("Audio").map_elements(lambda y: get_mfcc_means(y)[2], return_dtype=pl.Float64).alias("MFCC-3"),
    pl.col("Audio").map_elements(lambda y: get_pitch_var(y), return_dtype=pl.Float64).alias("PitchVar")
])
      </code></pre>
    </details>
  
    <h3>Key Observations from 2D Projections</h3>

    <table class="data-table">
      <thead>
        <tr>
          <th class="data-table-header">Projection</th>
          <th class="data-table-header">Dimensionality</th>
          <th class="data-table-header">Clustering Quality</th>
          <th class="data-table-header">Separation of Labels</th>
          <th class="data-table-header">Notes</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td class="data-table-cell">PCA</td>
          <td class="data-table-cell">2D</td>
          <td class="data-table-cell">Low</td>
          <td class="data-table-cell">Poor</td>
          <td class="data-table-cell">Broad dispersion, little label separation</td>
        </tr>
        <tr>
          <td class="data-table-cell">PCA</td>
          <td class="data-table-cell">3D</td>
          <td class="data-table-cell">Low</td>
          <td class="data-table-cell">Poor</td>
          <td class="data-table-cell">Added depth but still overlapping clusters</td>
        </tr>
        <tr>
          <td class="data-table-cell">UMAP</td>
          <td class="data-table-cell">2D</td>
          <td class="data-table-cell">Moderate</td>
          <td class="data-table-cell">Better than PCA</td>
          <td class="data-table-cell">Some visible clusters; improved separation</td>
        </tr>
        <tr>
          <td class="data-table-cell">UMAP</td>
          <td class="data-table-cell">3D</td>
          <td class="data-table-cell">Good</td>
          <td class="data-table-cell">Improved</td>
          <td class="data-table-cell">Neighborhood structure preserved in 3D space</td>
        </tr>
        <tr>
          <td class="data-table-cell">t-SNE</td>
          <td class="data-table-cell">2D</td>
          <td class="data-table-cell">High</td>
          <td class="data-table-cell">Strong</td>
          <td class="data-table-cell">Tight clusters and clear separation</td>
        </tr>
        <tr>
          <td class="data-table-cell">t-SNE</td>
          <td class="data-table-cell">3D</td>
          <td class="data-table-cell">Very High</td>
          <td class="data-table-cell">Very Strong</td>
          <td class="data-table-cell">Excellent local structure; highly interpretable</td>
        </tr>
      </tbody>
      <tfoot>
        <tr>
          <td colspan="5" class="data-table-footer">
            Summary of dimensionality reduction methods applied to audio features in both 2D and 3D projections
          </td>
        </tr>
      </tfoot>
    </table>

    <p class="text">Additional Note: Examine the t-SNE 3D projection. It seems like the labels are in a layer/sheet formation. We can possibly apply kernel tricks to our data and use SVMs to see if there is a hyper place to split on.</p>
  
    <details>
      <summary style="padding-top: 1em; font-family:Arial, Helvetica, sans-serif;">Show/Hide 2D + 3D Projection Code</summary>
      <pre><code class="python">
### Plotting the correlation matrix and 2D projections

# Selecting MFCCs and Pitch variance as features for 2d projection
features = ["PitchVar", "MFCC-1", "MFCC-2", "MFCC-3"]
corr = df[features].corr()  # Correlation matrix
plt.figure(figsize=(16, 16)) 

plt.subplot(221)  # Heat map
sns.heatmap(data=corr, annot=True, cmap="coolwarm", xticklabels=features, yticklabels=features)
plt.title("Correlations between Features")

# Prepare data for PCA, UMAP, and t-SNE
mfccs = ["MFCC-" + str(i) for i in range(1, 4)]
X = df[mfccs].to_numpy()
labels = df["Label"].to_numpy()

# Scaling the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# UMAP
reducer = umap.UMAP(n_components=2, random_state=42)
X_umap = reducer.fit_transform(X_scaled)

# t-SNE
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X_scaled)

# Plot PCA
plt.subplot(222)
unique_labels = np.unique(labels)
for label in unique_labels:
    idx = labels == label
    plt.scatter(X_pca[idx, 0], X_pca[idx, 1], label=label, alpha=0.6)
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.title("2D PCA of Audio Features")
plt.legend(loc="best", fontsize="small")
plt.grid(True)

# Plot UMAP
plt.subplot(223)
for label in unique_labels:
    idx = labels == label
    plt.scatter(X_umap[idx, 0], X_umap[idx, 1], label=label, alpha=0.6)
plt.xlabel("UMAP Component 1")
plt.ylabel("UMAP Component 2")
plt.title("UMAP Projection of Audio Features")
plt.legend(loc="best", fontsize="small")
plt.grid(True)

# Plot t-SNE
plt.subplot(224)
for label in unique_labels:
    idx = labels == label
    plt.scatter(X_tsne[idx, 0], X_tsne[idx, 1], label=label, alpha=0.6)
plt.xlabel("t-SNE Component 1")
plt.ylabel("t-SNE Component 2")
plt.title("t-SNE Projection of Audio Features")
plt.legend(loc="best", fontsize="small")
plt.grid(True)

plt.tight_layout()
plt.show()

### Plotting the 3D projections

# Prepare MFCC data
mfccs = ["MFCC-" + str(i) for i in range(1, 4)]
X = df[mfccs].to_numpy()
labels = df["Label"].to_numpy()

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Helper to make 3D plot and save HTML
def save_3d_plot(X_3d, labels, title, filename):
    fig = px.scatter_3d(
        x=X_3d[:, 0], y=X_3d[:, 1], z=X_3d[:, 2],
        color=labels.astype(str),
        title=title,
        labels={"x": "Component 1", "y": "Component 2", "z": "Component 3"},
        opacity=0.7
    )
    fig.write_html(filename)

# 2. PCA (3D)
pca = PCA(n_components=3)
X_pca = pca.fit_transform(X_scaled)
save_3d_plot(X_pca, labels, "3D PCA of Audio Features", "pca_3d.html")

# 3. UMAP (3D)
reducer = umap.UMAP(n_components=3, random_state=42)
X_umap = reducer.fit_transform(X_scaled)
save_3d_plot(X_umap, labels, "3D UMAP Projection of Audio Features", "umap_3d.html")

# 4. t-SNE (3D)
tsne = TSNE(n_components=3, random_state=42)
X_tsne = tsne.fit_transform(X_scaled)
save_3d_plot(X_tsne, labels, "3D t-SNE Projection of Audio Features", "tsne_3d.html")
      </code></pre>
    </details>
  </div>

  <div class="container blog main gray">
    <h3 style="text-align: center;">Correlation/Tsne/PCA/Umap (Dimensions = 2)</h3>
    <div class="slide-display">
      <div class="slide-menu">
        <ul class="dots" id="slide-menu">
          <li class="dot active"></li>
          <li class="dot"></li>
          <li class="dot"></li>
          <li class="dot"></li>
        </ul>
      </div>

      <div class="slide-content" , style="display: block;">
        <img src="clarity/images/correlation_heatmap.png" alt="Correlation Heatmap">
      </div>

      <div class="slide-content" , style="display: none;">
        <img src="clarity/images/tsne_projection.png" alt="TSNE Projection">
      </div>

      <div class="slide-content" , style="display: none;">
        <img src="clarity/images/pca_projection.png" alt="PCA Projection">
      </div>

      <div class="slide-content" , style="display: none;">
      <img src="clarity/images/umap_projection.png" alt="UMAP Projection">
    </div>


      <p class="caption">
        <span style="font-weight: bold">Figure 1:</span> Correlation heatmap of MFCCs and Pitch Variance. <br>
        <span style="font-weight: bold">Figure 2:</span> t-SNE projection of audio features. <br>
        <span style="font-weight: bold">Figure 3:</span> PCA projection of audio features. <br>
        <span style="font-weight: bold">Figure 4:</span> UMAP projection of audio features.
      </p>
    </div>
  </div>
  
  <div class="container blog main gray">
      <h3 style="text-align: center;">Correlation/Tsne/PCA/Umap (Dimensions = 3)</h3>
      <div class="slide-display">
        <div class="slide-menu">
          <ul class="dots" id="slide-menu">
            <li class="dot active"></li>
            <li class="dot"></li>
            <li class="dot"></li>
          </ul>
        </div>

        <div class="slide-content", style="display: none;">
          <div class="plot-container">
            <h4>3D PCA</h4>
            <iframe src="clarity/images/pca_3d.html"></iframe>
          </div>
        </div>
        
        <div class="slide-content", style="display: none;">
          <div class="plot-container">
            <h4>3D UMAP</h4>
            <iframe src="clarity/images/umap_3d.html"></iframe>
          </div>
        </div>

        <div class="slide-content", style="display: none;">
          <div class="plot-container">
            <h4>3D t-SNE</h4>
            <iframe src="clarity/images/tsne_3d.html"></iframe>
          </div>
        </div>
      </div>
  </div>

  <div class="container blog main">
    <h1>Feature Extraction & Hypothesis Testing</h1>
    <section style="font-family: sans-serif; line-height: 1.6;">
      <p class="text">
        In this section, we extract acoustic features from vocalizations and statistically evaluate whether they differ
        significantly across expression labels. These tests aim to determine if vocal cues such as <span
          style="font-weight: bold">pitch variability</span> and <span style="font-weight: bold">spectral shape</span>
        (MFCCs) carry meaningful information that can distinguish between intents like <code>"yes"</code> and
        <code>"no".</code>
      </p>

      <hr>

      <h3>Tests Conducted:</h3>
      <ol style="padding-left: 20px;">
        <li>
          <p class="text"><span style="font-weight: bold">Pitch Variability</span> – Mann-Whitney U test on pitch
            standard deviation across samples.</p>
        </li>
        <li>
          <p class="text"><span style="font-weight: bold">MFCC Differences</span> – Mann-Whitney U test on mean MFCC
            coefficients (1–3).</p>
        </li>
        <li>
          <p class="text"><span style="font-weight: bold">Entropy Differences</span> – ANOVA + Ad-Hoc Pairwise T tests
            on Spectral Entropy.</p>
        </li>
      </ol>

      <hr>

      <h3>Results:</h3>
      <p class="text">
        Our statistical tests revealed <span style="font-weight: bold">significant acoustic differences</span> between
        <code>"yes"</code> and <code>"no"</code> vocalizations:
      </p>

      <ul style="padding-left: 20px; list-style-type: disc;">
        <li>
          <p class="text"><span style="font-weight: bold">Pitch Variability</span>:</p>
          <ul style="padding-left: 20px; list-style-type: circle;">
            <li>
              <p class="text"><code>"No"</code> vocalizations showed <span style="font-weight: bold">much higher pitch
                  variability</span> (std = 119.13) compared to <code>"Yes"</code> (std = 22.46).</p>
            </li>
            <li>
              <p class="text">Mann-Whitney U test confirmed this with <span style="font-weight: bold">p &lt;
                  0.001</span> and a large effect size (<span style="font-weight: bold">Cohen's d = -2.38</span>).</p>
            </li>
          </ul>
        </li>

        <li>
          <p class="text"><span style="font-weight: bold">MFCCs (Spectral Shape)</span>:</p>
          <ul style="padding-left: 20px; list-style-type: circle;">
            <li>
              <p class="text">Significant differences were found in <span style="font-weight: bold">MFCC-1</span> and
                <span style="font-weight: bold">MFCC-3</span> (both <span style="font-weight: bold">p &lt; 0.001</span>,
                Cohen's d &gt; 1.6), indicating differences in <span style="font-weight: bold">spectral slope</span> and
                <span style="font-weight: bold">fine spectral variation</span>.
              </p>
            </li>
            <li>
              <p class="text"><span style="font-weight: bold">MFCC-2</span> showed <span style="font-weight: bold">no
                  significant difference</span>, suggesting similar mid-frequency emphasis in both groups.</p>
            </li>
          </ul>
        </li>

        <li>
          <p class="text"><span style="font-weight: bold">Spectral Entropy</span>:</p>
          <ul style="padding-left: 20px; list-style-type: circle;">
            <li>
              <p class="text">Significant differences in spectral entropy were found between various vocalization
                labels, indicating that certain emotional states could be distinguished based on their spectral
                characteristics.</p>
            </li>
            <li>
              <p class="text">Strong entropy differences were notably present between <code>"dysregulated"</code> and
                <code>"delighted"</code>, and between <code>"selftalk"</code> and <code>"frustrated"</code>,
                highlighting that spectral entropy can be a useful feature in distinguishing emotional states in
                vocalizations.
              </p>
            </li>
          </ul>
        </li>
      </ul>

      <p class="text">
        These findings suggest that both <span style="font-weight: bold">pitch dynamics</span> and <span
          style="font-weight: bold">spectral shape</span> are promising features for distinguishing vocal intent in
        non-verbal utterances for the model development phase.
      </p>
    </section>
  </div>

  <div class="container blog main">
    <h2>Test 1: Pitch Variability Differences between "Yes" and "No" Vocalizations (Mann-Whitney U Test)</h2>

    <p class="text">
      This test evaluates whether there are statistically significant differences in <span
        style="font-weight: bold">pitch variability</span> between vocalizations labeled as <code>yes</code> and
      <code>no</code>. Pitch variability is measured as the <span style="font-weight: bold">standard deviation of
        estimated pitch (f₀)</span> across time for each audio sample. This metric reflects how much the speaker's pitch
      varies within a vocalization type, often tied to emotional expressiveness or vocal intent.
    </p>

    <h3>What is Pitch Variability?</h3>

    <ul style="padding-left: 20px; list-style-type: disc;">
      <li>
        <p class="text">Calculated using <span style="font-weight: bold"><code>librosa</code>'s PYIN algorithm</span>,
          which
          estimates fundamental frequency (f₀) for voiced segments of an audio signal.</p>
      </li>
      <li>
        <p class="text">We then compute the <span style="font-weight: bold">standard deviation</span> of those f₀ values
          per sample.</p>
      </li>
      <li>
        <p class="text">A <span style="font-weight: bold">higher pitch std</span> generally means more variation in
          tone, while a lower std suggests more monotonic vocalization.</p>
      </li>
    </ul>

    <h3>Test Setup</h3>

    <ul style="padding-left: 20px; list-style-type: disc;">
      <li>
        <p class="text"><span style="font-weight: bold">Statistic</span>: Mann-Whitney U test (non-parametric)</p>
      </li>
      <li>
        <p class="text"><span style="font-weight: bold">Effect Size</span>: Cohen's d</p>
      </li>
      <li>
        <p class="text"><span style="font-weight: bold">Input Feature</span>: Standard deviation of pitch per sample</p>
      </li>
      <li>
        <p class="text"><span style="font-weight: bold">Groups Compared</span>: <code>yes</code> vs <code>no</code>
          vocalizations</p>
      </li>
      <li>
        <p class="text"><span style="font-weight: bold">Sample Size</span>: 100 samples for <code>yes</code>, 12 samples
          for <code>no</code></p>
      </li>
      <li>
        <p class="text"><span style="font-weight: bold">Alpha</span>: Will use a significance level of 0.05 </p>
      </li>
    </ul>

    <hr>

    <h3>Null Hypothesis (H₀):</h>
      <p class="text">
        There is <span style="font-weight: bold">no difference</span> in pitch variability between vocalizations labeled
        as <code>yes</code> and <code>no</code>. The distributions of pitch standard deviation are the same for both
        groups.
      </p>
      <h3>Alternative Hypothesis (H₁):</h2>
        <p class="text">
          There is a <span style="font-weight: bold">difference</span> in pitch variability between vocalizations
          labeled as
          <code>yes</code> and <code>no</code>. The distributions of pitch standard deviation are not the same for both
          groups, indicating that one group may exhibit more pitch variation than the other.
        </p>
  </div>
  <div class="container blog main gray">
    <h3>Group Means & Standard Deviations</h3>
    <div class="data-table-container">
      <table class="data-table">
        <thead>
          <tr>
            <th class="data-table-header">Label</th>
            <th class="data-table-header">Pitch Std (Mean ± Std)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td class="data-table-cell">"Yes"</td>
            <td class="data-table-cell">19.818 ± 20.91</td>
          </tr>
          <tr>
            <td class="data-table-cell">"No"</td>
            <td class="data-table-cell">114.964 ± 110.339</td>
          </tr>
      </table>
    </div>

    <h3>Statistical Results Summary</h3>

    <div class="data-table-container">
      <table class="data-table">
        <thead>
          <tr>
            <th class="data-table-header">Metric</th>
            <th class="data-table-header">Value</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td class="data-table-cell">"U Statistic"</td>
            <td class="data-table-cell">260.0</td>
          </tr>
          <tr>
            <td class="data-table-cell">"p-value"</td>
            <td class="data-table-cell">0.001</td>
          </tr>
          <tr>
            <td class="data-table-cell">"Cohen's d"</td>
            <td class="data-table-cell">-2.370</td>
          </tr>
          <tr>
            <td class="data-table-cell">"Mean Difference"</td>
            <td class="data-table-cell">-95.147</td>
          </tr>
          <tr>
            <td class="data-table-cell">"Significant"</td>
            <td class="data-table-cell">Yes</td>
          </tr>
      </table>
    </div>
  </div>
  <div class="container blog main">
    <h3>Interpretation</h3>

    <p class="text">
      Since our p = 0.01 < alpha, we <span style="font-weight: bold">reject</span> the null hypothesis. We interpret
        that:
    </p>

    <ul style="padding-left: 20px; list-style-type: disc;">
      <li>
        <p class="text">The <span style="font-weight: bold">no</span> vocalizations exhibit <span
            style="font-weight: bold">dramatically higher pitch variability</span> than <code>yes</code> samples —
          almost <span style="font-weight: bold">5x higher on average</span></p>
      </li>
      <li>
        <p class="text">The test yields a <span style="font-weight: bold">low p-value (0.01)</span> and a <span
            style="font-weight: bold">large negative effect size (Cohen’s d = -2.38)</span>, indicating a strong and
          statistically significant difference.</p>
      </li>
      <li>
        <p class="text">This suggests that <span style="font-weight: bold">pitch dynamics</span> could be a powerful
          feature in differentiating certain types of vocal intent, especially when classifying expressive vs. flat
          responses.</p>
      </li>
    </ul>
    <details>
      <summary style="padding-top: 1em; font-family:Arial, Helvetica, sans-serif;">Show/Hide Full Loading code</summary>
      <pre><code class="python">
def batch_pitch_extraction(audio_list: List,
                           max_samples_per_batch: int=50,
                           sr: int=SAMPLE_RATE) -> List[float]:
    # Randomly sample if batch is too large
    if len(audio_list) > max_samples_per_batch:
        sample_indices = np.random.choice(len(audio_list), max_samples_per_batch, replace=False)
        audio_list = [audio_list[i] for i in sample_indices]
    
    pitch_stds = []
    for audio_array in audio_list:
        audio_array = np.asarray(audio_array, dtype=np.float64)
        
        # Extract pitch using PYIN
        f0, voiced_flag, _ = librosa.pyin(
            audio_array, 
            fmin=librosa.note_to_hz('C2'),
            fmax=librosa.note_to_hz('C7'),
            sr=sr
        )
        
        # Filter for voiced segments
        f0_voiced = f0[voiced_flag]
        
        # Calculate pitch std, handle empty case
        pitch_std = float(np.std(f0_voiced)) if len(f0_voiced) > 0 else 0.0
        pitch_stds.append(pitch_std)
    
    return pitch_stds

def pitch_variability_test(df: pl.DataFrame,
                           max_batch_size: int=50,
                           target_labels: List[str]=['frustrated', 'delighted']) -> Dict[str, float]:
    # Group audio by label
    label_audio_groups = {}
    for label in target_labels:
        # Extract audio for each label
        label_audio_groups[label] = df.filter(pl.col("Label") == label)["Audio"].to_list()
    
    # Batch pitch extraction
    label_pitch_stds = {}
    for label, audio_list in label_audio_groups.items():
        label_pitch_stds[label] = batch_pitch_extraction(audio_list=audio_list, max_samples_per_batch=max_batch_size)
        
        # Print basic stats
        pitch_array = np.array(label_pitch_stds[label])
        print(f"{label} samples: {len(pitch_array)}")
        print(f"  Mean pitch std: {np.mean(pitch_array):.4f}")
        print(f"  Std of pitch std: {np.std(pitch_array):.4f}")
    
    # Perform statistical tests
    label1_data = label_pitch_stds[target_labels[0]]
    label2_data = label_pitch_stds[target_labels[1]]
    
    # Mann-Whitney U Test
    u_statistic, p_value = scipy.stats.mannwhitneyu(
        label1_data, 
        label2_data, 
        alternative='two-sided'
    )
    
    # Effect size calculation (Cohen's d)
    mean1, std1 = np.mean(label1_data), np.std(label1_data)
    mean2, std2 = np.mean(label2_data), np.std(label2_data)
    
    # Pooled standard deviation
    pooled_std = np.sqrt(((len(label1_data) - 1) * std1**2 + 
                          (len(label2_data) - 1) * std2**2) / 
                         (len(label1_data) + len(label2_data) - 2))
    
    # Cohen's d
    cohens_d = (mean1 - mean2) / pooled_std
    
    # Prepare results
    results = {
        'Mann-Whitney U Statistic': u_statistic,
        'p-value': p_value,
        'Cohen\'s d': cohens_d,
        'Mean Difference': mean1 - mean2,
        'Significant': p_value < 0.05
    }
    
    # Print results
    print("\n=== Hypothesis Test Results ===")
    for key, value in results.items():
        print(f"{key}: {value}")
    
    return results

# Spectogram plotting functions to compare labels 
def plot_spectrogram_comparison(df, label1="yes", label2="no", sr=SAMPLE_RATE, n_examples=2):
    fig, axes = plt.subplots(n_examples, 2, figsize=(12, 4 * n_examples))
    label_map = {0: label1, 1: label2}

    for i, label in enumerate([label1, label2]):
        examples = df.filter(pl.col("Label") == label).head(n_examples).iter_rows(named=True)
        for j, row in enumerate(examples):
            y = np.array(row["Audio"])
            S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)
            S_db = librosa.power_to_db(S, ref=np.max)
            ax = axes[j, i] if n_examples > 1 else axes[i]
            librosa.display.specshow(S_db, sr=sr, x_axis='time', y_axis='mel', ax=ax)
            ax.set_title(f"{label_map[i].upper()} Sample #{j+1}")
            ax.set_xlabel("")
            ax.set_ylabel("")

    plt.suptitle("Mel Spectrogram Comparison: YES vs NO", fontsize=16)
    plt.tight_layout()
    plt.show()
        </code></pre>
    </details>
    <pre><code class="python">
t0 = time.time()
results = pitch_variability_test(df=df, max_batch_size=100, target_labels=["yes", "no"])
print(f"\n🎶 Pitch Variability Test completed in {time.time() - t0:.2f} seconds")
  </code></pre>

    <p class="text">
      This process results in the following results:
    </p>
    <p style="font-family: 'Courier New', Courier, monospace;">yes samples: 100</p>
    <p style="font-family: 'Courier New', Courier, monospace; margin-left: 20px;">Mean pitch std: 19.8174</p>
    <p style="font-family: 'Courier New', Courier, monospace; margin-left: 20px;">Std of pitch std: 20.9069</p>
    <p style="font-family: 'Courier New', Courier, monospace;">no samples: 12</p>
    <p style="font-family: 'Courier New', Courier, monospace; margin-left: 20px;">Mean pitch std: 114.9644</p>
    <p style="font-family: 'Courier New', Courier, monospace; margin-left: 20px;">Std of pitch std: 110.3390<br><br></p>

    <p style="font-family: 'Courier New', Courier, monospace;">=== Hypothesis Test Results ===</p>
    <p style="font-family: 'Courier New', Courier, monospace;">Mann-Whitney U Statistic: 260.0</p>
    <p style="font-family: 'Courier New', Courier, monospace;">p-value: 0.0014043896888058</p>
    <p style="font-family: 'Courier New', Courier, monospace;">Cohen's d: -2.3706435406222472</p>
    <p style="font-family: 'Courier New', Courier, monospace;">Mean Difference: -95.14698434279867</p>
    <p style="font-family: 'Courier New', Courier, monospace;">Significant: True<br><br></p>
    <p style="font-family: 'Courier New', Courier, monospace;">🎶 Pitch Variability Test completed in 30.40 seconds</p>

    </p>

    <ul style="padding-left: 20px; list-style-type: disc;">
      <li>
        <p class="text">The <span style="font-weight: bold">YES Samples</span>: Show varied frequency patterns with
          bright spots indicating dynamic changes in pitch.</p>
      </li>
      <li>
        <p class="text">The <span style="font-weight: bold">NO Samples</span>: Display consistent and flat energy
          patterns with fewer changes.</p>
      </li>
    </ul>

    <pre><code class="python">
plot_spectrogram_comparison(df, label1="yes", label2="no", sr=SAMPLE_RATE, n_examples=4)
            </code></pre>
  </div>

  <div class="container blog main gray">
    <h3 style="text-align: center;">Yes/No Spectrograms</h3>
    <div class="slide-display">
      <div class="slide-menu">
        <ul class="dots" id="slide-menu">
          <li class="dot active"></li>
          <li class="dot"></li>
          <li class="dot"></li>
          <li class="dot"></li>
        </ul>
      </div>

      <div class="slide-content" , style="display: block;">
        <img src="clarity/images/YesVsNoSample1.png" alt="Yes Vs No Sample 1">
      </div>

      <div class="slide-content" , style="display: none;">
        <img src="clarity/images/YesVsNoSample2.png" alt="Yes Vs No Sample 2">
      </div>

      <div class="slide-content" , style="display: none;">
        <img src="clarity/images/YesVsNoSample3.png" alt="Yes Vs No Sample 3">
      </div>

      <div class="slide-content" , style="display: none;">
        <img src="clarity/images/YesVsNoSample4.png" alt="Yes Vs No Sample 4">
      </div>

      <p class="caption">
        The spectrograms above illustrate the differences in pitch variability between "yes" and "no" vocalizations.
      </p>
    </div>
  </div>

  <div class="container blog main">
    <h2>Test 2: Mel Frequency Cepstral Coefficients (MFCCs) Mean Differences between "Yes" and "No" Vocalizations
      (Pairwise Mann-Whitney U Test)</h2>

    <p class="text">
      This test evaluates whether there are statistically significant differences in <span
        style="font-weight: bold">spectral shape</span> between vocalizations labeled as <code>"yes"</code> and
      <code>"no"</code>, focusing on the <span style="font-weight: bold">mean values of the first three MFCCs</span>.
    </p>

    <h3>What are MFCCs?</h3>

    <ul style="padding-left: 20px; list-style-type: disc;">
      <li>
        <p class="text"><span style="font-weight: bold">MFCC-1</span>: Captures the overall <span
            style="font-weight: bold">spectral slope</span> — indicates the energy balance between low and high
          frequencies.</p>
      </li>
      <li>
        <p class="text"><span style="font-weight: bold">MFCC-2</span>: Captures the <span
            style="font-weight: bold">curvature</span> of the spectral envelope — flat vs. peaked energy in the mid
          frequencies.</p>
      </li>
      <li>
        <p class="text"><span style="font-weight: bold">MFCC-3</span>: Represents <span
            style="font-weight: bold">fine-grained variation</span> — subtle changes or "ripples" in the spectral shape.
        </p>
      </li>
      <li>
        <p class="text">Higher-order MFCCs (4, 5, …) capture increasingly localized detail and high-frequency texture.
        </p>
      </li>
    </ul>

    <hr>

    <h3>Test Setup</h3>

    <ul style="padding-left: 20px; list-style-type: disc;">
      <li>
        <p class="text"><span style="font-weight: bold">Statistic</span>: Pairwise Mann-Whitney U test (non-parametric)
        </p>
      </li>
      <li>
        <p class="text"><span style="font-weight: bold">Effect Size</span>: Cohen's d</p>
      </li>
      <li>
        <p class="text"><span style="font-weight: bold">Input Features</span>: Mean of MFCC-1, MFCC-2, and MFCC-3 per
          sample</p>
      </li>
      <li>
        <p class="text"><span style="font-weight: bold">Groups Compared</span>: <code>"yes"</code> vs <code>"no"</code>
          vocalizations</p>
      </li>
      <li>
        <p class="text"><span style="font-weight: bold">Sample Size</span>: 100 samples for <code>"yes"</code>, 12
          samples for <code>"no"</code></p>
      </li>
      <li>
        <p class="text"><span style="font-weight: bold">Alpha</span>: Will use a significance level of 0.05 </p>
      </li>
    </ul>

    <hr>

    <h3>Null Hypothesis (H₀):</h3>
    <p class="text">
      There are <span style="font-weight: bold">no significant differences</span> in the mean values of the first three
      MFCCs (MFCC-1, MFCC-2, and MFCC-3) between vocalizations labeled as <code>"yes"</code> and <code>"no"</code>. The
      distributions of these spectral shape measures are the same across both groups.
    </p>
    <h3>Alternative Hypothesis (H₁):</h3>
    <p class="text">
      There are <span style="font-weight: bold">significant differences</span> in the mean values of the first three
      MFCCs (MFCC-1, MFCC-2, and MFCC-3) between vocalizations labeled as <code>"yes"</code> and <code>"no"</code>. The
      distributions of these spectral shape measures vary between the two groups, indicating discriminative spectral
      characteristics.
    </p>
  </div>
  <div class="container blog main gray">
    <h3>Group Means & Standard Deviations</h3>
    <div class="data-table-container">
      <table class="data-table">
        <thead>
          <tr>
            <th class="data-table-header">Label</th>
            <th class="data-table-header">MFCC-1 Mean ± Std</th>
            <th class="data-table-header">MFCC-2 Mean ± Std</th>
            <th class="data-table-header">MFCC-3 Mean ± Std</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td class="data-table-cell">"Yes"</td>
            <td class="data-table-cell">-323.365 ± 32.903</td>
            <td class="data-table-cell">122.607 ± 18.428</td>
            <td class="data-table-cell">-16.314 ± 15.998</td>
          </tr>
          <tr>
            <td class="data-table-cell">"No"</td>
            <td class="data-table-cell">-262.130 ± 42.377</td>
            <td class="data-table-cell">114.124 ± 25.801</td>
            <td class="data-table-cell">-52.627 ± 27.668</td>
          </tr>
      </table>
    </div>

    <h3>Statistical Results Summary</h3>

    <div class="data-table-container">
      <table class="data-table">
        <thead>
          <tr>
            <th class="data-table-header">MFCC</th>
            <th class="data-table-header">U Statistic</th>
            <th class="data-table-header">p-value</th>
            <th class="data-table-header">Cohen's d</th>
            <th class="data-table-header">Mean Diff</th>
            <th class="data-table-header">Significant</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td class="data-table-cell">"MFCC-1"</td>
            <td class="data-table-cell">158.0</td>
            <td class="data-table-cell">3.277e-05</td>
            <td class="data-table-cell">-1.803</td>
            <td class="data-table-cell">-61.235</td>
            <td class="data-table-cell">Yes</td>
          </tr>
          <tr>
            <td class="data-table-cell">"MFCC-2"</td>
            <td class="data-table-cell">708.0</td>
            <td class="data-table-cell">0.312</td>
            <td class="data-table-cell">0.440</td>
            <td class="data-table-cell">8.483</td>
            <td class="data-table-cell">No</td>
          </tr>
          <tr>
            <td class="data-table-cell">"MFCC-3"</td>
            <td class="data-table-cell">1048.0</td>
            <td class="data-table-cell">2.557e-05</td>
            <td class="data-table-cell">2.073</td>
            <td class="data-table-cell">36.312</td>
            <td class="data-table-cell">Yes</td>
          </tr>
      </table>
    </div>
  </div>
  <div class="container blog main">
    <h3>Interpretation</h3>

    <p class="text">
      Since our p-values for MFCC-1 (p = 3.277e-05) and MFCC-3 (p = 2.557e-05) are both below alpha = 0.05, we <span
        style="font-weight: bold">reject</span> the null hypothesis for these coefficients. We interpret that:
    </p>

    <ul style="padding-left: 20px; list-style-type: disc;">
      <li>
        <p class="text"><span style="font-weight: bold">MFCC-1</span> shows significant differences between
          <code>"yes"</code> and <code>"no"</code> vocalizations, with a <span style="font-weight: bold">large negative
            effect size</span> (Cohen's d = -1.803), indicating <code>"yes"</code> vocalizations have a steeper spectral
          slope.
        </p>
      </li>
      <li>
        <p class="text"><span style="font-weight: bold">MFCC-3</span> exhibits significant differences with an even
          <span style="font-weight: bold">larger positive effect size</span> (Cohen's d = 2.073), showing that
          <code>"yes"</code> vocalizations have more subtle spectral variations.
        </p>
      </li>
      <li>
        <p class="text"><span style="font-weight: bold">MFCC-2</span> does not show a statistically significant
          difference (p = 0.312), suggesting that the mid-frequency curvature is similar between the two vocalization
          types.</p>
      </li>
    </ul>

    <p class="text">
      These results suggest that <span style="font-weight: bold">spectral slope</span> (MFCC-1) and <span
        style="font-weight: bold">fine-grained spectral variation</span> (MFCC-3) are powerful discriminators between
      <code>"yes"</code> and <code>"no"</code> vocalizations, while the <span style="font-weight: bold">mid-frequency
        curvature</span> (MFCC-2) carries less discriminative information.
    </p>

    <details>
      <summary style="padding-top: 1em; font-family:Arial, Helvetica, sans-serif;">Show/Hide Full Loading code</summary>
      <pre><code class="python">
    def batch_mfcc_extraction(audio_list: List,
                               max_samples_per_batch: int=50,
                               sr: int=SAMPLE_RATE,
                               n_coeffs: int=3) -> List[float]:
        if len(audio_list) > max_samples_per_batch:
            sample_indices = np.random.choice(len(audio_list), max_samples_per_batch, replace=False)
            audio_list = [audio_list[i] for i in sample_indices]
            
        mfcc_means = []
        for audio_array in audio_list:
            audio_array = np.asarray(audio_array, dtype=np.float32)
            mfccs = librosa.feature.mfcc(y=audio_array, sr=sr, n_mfcc=n_coeffs)
            mfcc_mean = np.mean(mfccs, axis=1)
            mfcc_means.append(mfcc_mean)
            
        return mfcc_means
    
    def mfcc_significance_test(df, max_batch_size=50, target_labels=["frustrated", "delighted"], n_coeffs=3):
        label_audio_groups = {}
        for label in target_labels:
            label_audio_groups[label] = df.filter(pl.col("Label") == label)["Audio"].to_list()
            
        label_mfcc_means = {}
        for label, audio_list in label_audio_groups.items():
            label_mfcc_means[label] = batch_mfcc_extraction(
                audio_list,
                max_samples_per_batch=max_batch_size,
                n_coeffs=n_coeffs,
                sr=SAMPLE_RATE
            )
            mfcc_array = np.array(label_mfcc_means[label])
            print(f"{label} samples: {len(mfcc_array)}")
            
            for i in range(n_coeffs):
                print(f"  MFCC-{i+1} Mean: {np.mean(mfcc_array[:, i]):.4f}, Std: {np.std(mfcc_array[:, i]):.4f}")
        
        results = {}
        for i in range(n_coeffs):
            data1 = [x[i] for x in label_mfcc_means[target_labels[0]]]
            data2 = [x[i] for x in label_mfcc_means[target_labels[1]]]
            
            u_statistic, p_value = scipy.stats.mannwhitneyu(data1, data2, alternative='two-sided')
            mean1, std1 = np.mean(data1), np.std(data1)
            mean2, std2 = np.mean(data2), np.std(data2)
            
            pooled_std = np.sqrt(((len(data1) - 1) * std1**2 + (len(data2) - 1) * std2**2) / 
                                  (len(data1) + len(data2) - 2))
            cohens_d = (mean1 - mean2) / pooled_std
            
            results[f"MFCC-{i+1}"] = {
                'U Statistic': u_statistic,
                'p-value': p_value,
                'Cohen\'s d': cohens_d,
                'Mean Difference': mean1 - mean2,
                'Significant': p_value < 0.05
            }
        
        print("\n=== MFCC Significance Test Results ===")
        for k, v in results.items():
            print(f"\n{k}")
            for stat, val in v.items():
                print(f"  {stat}: {val}")
            
        return results
          </code></pre>
    </details>
    <details>
      <summary style="padding-top: 1em; font-family:Arial, Helvetica, sans-serif;">Show/Hide Full Loading code</summary>
      <pre><code class="python">
    # Box whisker and heatmap to visualize distribution of MFCC (compare means)
    def box_and_heat_mfcc_comparison(df, labels=["yes", "no"], sr=SAMPLE_RATE, n_mfcc=3):
        # Step 1: Prepare data
        data = []
        mfcc_data = {label: [] for label in labels}
        
        for label in labels:
            for row in df.filter(pl.col("Label") == label).iter_rows(named=True):
                y = np.array(row["Audio"])
                mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)
                mfcc_mean = np.mean(mfccs, axis=1)
                
                # For boxplot
                for i in range(n_mfcc):
                    data.append({
                        "MFCC": f"MFCC-{i+1}",
                        "Value": mfcc_mean[i],
                        "Label": label
                    })
                
                # For heatmap
                mfcc_data[label].append(mfcc_mean)
        
        # DataFrames
        df_plot = pd.DataFrame(data)
        heat_data = []
        for label in labels:
            means = np.mean(np.stack(mfcc_data[label]), axis=0)
            row = [means[i] for i in range(n_mfcc)]
            heat_data.append(row)
        df_heat = pd.DataFrame(heat_data, columns=[f"MFCC-{i+1}" for i in range(n_mfcc)], index=[label.upper() for label in labels])
        
        # Step 2: Plot side-by-side
        fig, axes = plt.subplots(1, 2, figsize=(16, 6))
        
        # Boxplot
        sns.boxplot(data=df_plot, x="MFCC", y="Value", hue="Label", ax=axes[0])
        axes[0].set_title("MFCC Distribution (Boxplot)")
        axes[0].grid(True)
        
        # Heatmap
        sns.heatmap(df_heat, annot=True, fmt=".1f", cmap="viridis", ax=axes[1])
        axes[1].set_title("MFCC Mean Comparison (Heatmap)")
        axes[1].set_ylabel("Label")
        axes[1].set_xlabel("MFCC Coefficient")
        
        plt.tight_layout()
        plt.show()
      </code></pre>
    </details>
    <pre><code class="python">
    t0 = time.time()
    results = mfcc_significance_test(df, max_batch_size=100, target_labels=["yes", "no"], n_coeffs=3)
    print(f"\n🎛️ MFCC Significance Test completed in {time.time() - t0:.2f} seconds")
    
    # Visualize results
    box_and_heat_mfcc_comparison(df, labels=["yes", "no"], n_mfcc=3)
      </code></pre>

    <p class="text">
      This process results in the following results:
    </p>
    <p style="font-family: 'Courier New', Courier, monospace;">yes samples: 100</p>
    <p style="font-family: 'Courier New', Courier, monospace; margin-left: 20px;">MFCC-1 Mean: -323.3647, Std: 32.9024
    </p>
    <p style="font-family: 'Courier New', Courier, monospace; margin-left: 20px;">MFCC-2 Mean: 122.6066, Std: 18.4277
    </p>
    <p style="font-family: 'Courier New', Courier, monospace; margin-left: 20px;">MFCC-3 Mean: -16.3141, Std: 15.9972
    </p>
    <p style="font-family: 'Courier New', Courier, monospace;">no samples: 12</p>
    <p style="font-family: 'Courier New', Courier, monospace; margin-left: 20px;">Mean pitch std: 114.9644</p>
    <p style="font-family: 'Courier New', Courier, monospace; margin-left: 20px;">Std of pitch std: 110.3390<br><br></p>

    <p style="font-family: 'Courier New', Courier, monospace;">=== Hypothesis Test Results ===<br><br></p>

    <p style="font-family: 'Courier New', Courier, monospace;">MFCC-1</p>
    <p style="font-family: 'Courier New', Courier, monospace; margin-left: 20px;">U Statistic: 158.0</p>
    <p style="font-family: 'Courier New', Courier, monospace; margin-left: 20px;">p-value: 3.277395269119212e-05</p>
    <p style="font-family: 'Courier New', Courier, monospace; margin-left: 20px;">Cohen's d: -1.8026657104492188</p>
    <p style="font-family: 'Courier New', Courier, monospace; margin-left: 20px;">Mean Difference: -61.2347412109375</p>
    <p style="font-family: 'Courier New', Courier, monospace; margin-left: 20px;">Significant: True<br><br></p>
    <p style="font-family: 'Courier New', Courier, monospace;">MFCC-2</p>
    <p style="font-family: 'Courier New', Courier, monospace; margin-left: 20px;">U Statistic: 708.0</p>
    <p style="font-family: 'Courier New', Courier, monospace; margin-left: 20px;">p-value: 0.31188485986767844</p>
    <p style="font-family: 'Courier New', Courier, monospace; margin-left: 20px;">Cohen's d: 0.43969401717185974</p>
    <p style="font-family: 'Courier New', Courier, monospace; margin-left: 20px;">Mean Difference: 8.482711791992188</p>
    <p style="font-family: 'Courier New', Courier, monospace; margin-left: 20px;">Significant: False<br><br></p>
    <p style="font-family: 'Courier New', Courier, monospace;">MFCC-3</p>
    <p style="font-family: 'Courier New', Courier, monospace; margin-left: 20px;">U Statistic: 1048.0</p>
    <p style="font-family: 'Courier New', Courier, monospace; margin-left: 20px;">p-value: 2.557395269119212e-05</p>
    <p style="font-family: 'Courier New', Courier, monospace; margin-left: 20px;">Cohen's d: 2.0729801654815674</p>
    <p style="font-family: 'Courier New', Courier, monospace; margin-left: 20px;">Mean Difference: 36.31230163574219</p>
    <p style="font-family: 'Courier New', Courier, monospace; margin-left: 20px;">Significant: True<br><br></p>
    <p style="font-family: 'Courier New', Courier, monospace;">🎛️ MFCC Significance Test completed in 0.44
      seconds<br><br></p>
    <pre><code class="python">
box_and_heat_mfcc_comparison(df, labels=["yes", "no"], n_mfcc=3)
  </code></pre>
  </div>

  <div class="container blog main gray">
    <h3 style="text-align: center;">Boxplot/Heatmap</h3>
    <div class="slide-display">
      <div class="slide-menu">
        <ul class="dots" id="slide-menu">
          <li class="dot active"></li>
          <li class="dot"></li>
        </ul>
      </div>

      <div class="slide-content" , style="display: block;">
        <img src="clarity/images/MFCCDistributionBoxPlot.png" alt="MFCC Distribution Box Plot">
      </div>

      <div class="slide-content" , style="display: none;">
        <img src="clarity/images/MFCCMeanComparisonHeatMap.png" alt="MFCC Mean Comparison Heat Map">
      </div>

      <p class="caption">
        The boxplot shows the distribution of MFCC values for "yes" and "no" vocalizations, while the heatmap shows the
        similartity between the means
      </p>
    </div>
  </div>

  <div class="container blog main">
    <h2>Test 3: Spectral Entropy Differences Across Vocalization Labels (ANOVA & T-Tests)</h2>

    <p class="text">
      This analysis investigates whether there are statistically significant differences in <span
        style="font-weight: bold">spectral entropy</span> between different vocalization labels
      (<code>"dysregulated"</code>, <code>"hunger"</code>, <code>"delighted"</code>).
    </p>

    <h3>What is Spectral Entropy?</h3>

    <p class="text">
      Spectral entropy measures the <span style="font-weight: bold">disorder</span> or <span
        style="font-weight: bold">randomness</span> in an audio signal's frequency distribution. A higher entropy
      indicates a more uniform spectral distribution, while lower entropy suggests a more structured or tonal signal.
    </p>

    <hr>

    <h3>Test Setup</h3>

    <ul style="padding-left: 20px; list-style-type: disc;">
      <li>
        <p class="text"><span style="font-weight: bold">Statistic</span>: One-way ANOVA & Pairwise T-tests</p>
      </li>
      <li>
        <p class="text"><span style="font-weight: bold">Effect Size</span>: Cohen's <i>d</i> (for pairwise comparisons)
        </p>
      </li>
      <li>
        <p class="text"><span style="font-weight: bold">Input Feature</span>: Spectral entropy computed from short-time
          Fourier transform (STFT)</p>
      </li>
      <li>
        <p class="text"><span style="font-weight: bold">Groups Compared</span>: <code>"dysregulated"</code>,
          <code>"hunger"</code>, <code>"delighted"</code>
        </p>
      </li>
      <li>
        <p class="text"><span style="font-weight: bold">Sample Size</span>: Maximum of 100 samples per label</p>
      </li>
    </ul>

    <hr>

    <h3>Null Hypothesis (H₀):</h3>
    <p class="text">
      There are <span style="font-weight: bold">no significant differences</span> in spectral entropy among the
      vocalization labels <code>"dysregulated"</code>, <code>"hunger"</code>, and <code>"delighted"</code>. All groups
      exhibit similar entropy distributions.
    </p>
    <h3>Alternative Hypothesis (H₁):</h3>
    <p class="text">
      There are <span style="font-weight: bold">significant differences</span> in spectral entropy among the
      vocalization
      labels <code>"dysregulated"</code>, <code>"hunger"</code>, and <code>"delighted"</code>. At least one of these
      groups exhibits a different entropy distribution compared to the others.
    </p>
  </div>

  <div class="container blog main gray">
    <h3>ANOVA Results</h3>
    <div class="data-table-container">
      <table class="data-table">
        <thead>
          <tr>
            <th class="data-table-header">Test</th>
            <th class="data-table-header">F-Statistic</th>
            <th class="data-table-header">p-value</th>
            <th class="data-table-header">Significant</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td class="data-table-cell"><strong>Spectral Entropy</strong></td>
            <td class="data-table-cell">22.914</td>
            <td class="data-table-cell">1.067e-13</td>
            <td class="data-table-cell">Yes</td>
          </tr>
        </tbody>
      </table>
    </div>

    <p class="text">
      A significant ANOVA result suggests that at least one group has a different spectral entropy distribution.
    </p>

    <h3>Pairwise T-Test Summary</h3>

    <div class="data-table-container">
      <table class="data-table">
        <thead>
          <tr>
            <th class="data-table-header">Comparison</th>
            <th class="data-table-header">T-Statistic</th>
            <th class="data-table-header">p-value</th>
            <th class="data-table-header">Significant</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td class="data-table-cell"><strong>Dysregulated vs Selftalk</strong></td>
            <td class="data-table-cell">-15.48</td>
            <td class="data-table-cell">5.76e-50</td>
            <td class="data-table-cell">Yes</td>
          </tr>
          <tr>
            <td class="data-table-cell"><strong>Dysregulated vs Delighted</strong></td>
            <td class="data-table-cell">-11.94</td>
            <td class="data-table-cell">1.19e-31</td>
            <td class="data-table-cell">Yes</td>
          </tr>
          <tr>
            <td class="data-table-cell"><strong>Dysregulated vs Frustrated</strong></td>
            <td class="data-table-cell">-2.10</td>
            <td class="data-table-cell">0.036</td>
            <td class="data-table-cell">No</td>
          </tr>
          <tr>
            <td class="data-table-cell"><strong>Selftalk vs Delighted</strong></td>
            <td class="data-table-cell">+1.65</td>
            <td class="data-table-cell">0.100</td>
            <td class="data-table-cell">No</td>
          </tr>
          <tr>
            <td class="data-table-cell"><strong>Selftalk vs Frustrated</strong></td>
            <td class="data-table-cell">+14.18</td>
            <td class="data-table-cell">3.37e-44</td>
            <td class="data-table-cell">Yes</td>
          </tr>
          <tr>
            <td class="data-table-cell"><strong>Delighted vs Frustrated</strong></td>
            <td class="data-table-cell">+10.49</td>
            <td class="data-table-cell">3.05e-25</td>
            <td class="data-table-cell">Yes</td>
          </tr>
        </tbody>
      </table>
    </div>
  </div>

  <div class="container blog main">
    <h3>Interpretation</h3>

    <p class="text">
      Since our ANOVA p-value (p = 1.067e-13) is well below alpha = 0.05, we <span
        style="font-weight: bold">reject</span>
      the null hypothesis. This indicates there are significant differences in spectral entropy among the vocalization
      labels tested.
    </p>

    <p class="text">
      From the pairwise T-tests, we observe:
    </p>

    <ul style="padding-left: 20px; list-style-type: disc;">
      <li>
        <p class="text"><span style="font-weight: bold">Strong differences</span> in spectral entropy are observed
          between:
        <ul style="padding-left: 20px; list-style-type: circle;">
          <li>
            <p class="text"><code>"dysregulated"</code> and both <code>"selftalk"</code> & <code>"delighted"</code></p>
          </li>
          <li>
            <p class="text"><code>"selftalk"</code> and <code>"frustrated"</code></p>
          </li>
          <li>
            <p class="text"><code>"delighted"</code> and <code>"frustrated"</code></p>
          </li>
        </ul>
        </p>
      </li>
      <li>
        <p class="text"><span style="font-weight: bold">No significant difference</span> between:
        <ul style="padding-left: 20px; list-style-type: circle;">
          <li>
            <p class="text"><code>"dysregulated"</code> and <code>"frustrated"</code></p>
          </li>
          <li>
            <p class="text"><code>"selftalk"</code> and <code>"delighted"</code></p>
          </li>
        </ul>
        </p>
      </li>
    </ul>

    <p class="text">
      These results suggest that <span style="font-weight: bold">spectral entropy can effectively differentiate between
        some vocal states</span>, particularly those on emotional extremes. However, overlaps exist, indicating entropy
      may
      not capture all acoustic nuance across labels.
    </p>

    <details>
      <summary style="padding-top: 1em; font-family:Arial, Helvetica, sans-serif;">Show/Hide Spectral Entropy
        Computation
        Code</summary>
      <pre><code class="python">
  def compute_spectral_entropy(y):
      # Compute power spectral density
      S = np.abs(librosa.stft(y))**2
      
      # Normalize each frame to create a probability distribution
      S_norm = S / (np.sum(S, axis=0) + 1e-10)
      
      # Compute Shannon entropy using log base 2 (information-theoretic interpretation)
      spectral_entropy = -np.sum(S_norm * np.log2(S_norm + 1e-10), axis=0)
      
      # Normalize by maximum possible entropy for the given frequency bins
      max_entropy = np.log2(S.shape[0])  # log2(n_bins)
      normalized_entropy = spectral_entropy / max_entropy
      
      return float(np.mean(normalized_entropy))
      </code></pre>
    </details>
    <details>
      <summary style="padding-top: 1em; font-family:Arial, Helvetica, sans-serif;">Show/Hide ANOVA Test Code</summary>
      <pre><code class="python">
  def spectral_entropy_anova_test(df, target_labels=["dysregulated", "hunger", "delighted"], max_samples_per_label=50):
      """Perform ANOVA on spectral entropy differences."""
      label_audio_groups = {label: df.filter(pl.col("Label") == label)["Audio"].to_list() for label in target_labels}
      label_entropy_means = {label: [compute_spectral_entropy(np.array(y)) for y in audio_list[:max_samples_per_label]]
                              for label, audio_list in label_audio_groups.items()}
      data = [label_entropy_means[label] for label in target_labels]
      
      # One-way ANOVA test
      f_statistic, p_value = scipy.stats.f_oneway(*data)
      
      results = {
          'ANOVA F-Statistic': f_statistic,
          'p-value': p_value,
          'Significant': p_value < 0.05
      }
      
      print("\n=== Spectral Entropy ANOVA Test Results ===")
      for key, value in results.items():
          print(f"{key}: {value}")
      
      return results
      </code></pre>
    </details>
    <pre><code class="python">
  target_labels = ["dysregulated", "selftalk", "delighted", "frustrated"]
  t0 = time.time()
  results = spectral_entropy_anova_test(df, target_labels=target_labels, max_samples_per_label=100)
  print(f"\nSpectral Entropy ANOVA Test completed in {time.time() - t0:.2f} seconds")
    </code></pre>

    <p class="text">
      This process results in the following ANOVA results:
    </p>
    <p style="font-family: 'Courier New', Courier, monospace;">=== Spectral Entropy ANOVA Test Results ===</p><br>
    <p style="font-family: 'Courier New', Courier, monospace;">ANOVA F-Statistic: 22.914553798899618
    </p>
    <p style="font-family: 'Courier New', Courier, monospace;">p-value: 1.0669701185254439e-13</p>
    <p style="font-family: 'Courier New', Courier, monospace;">Significant: True</p><br>
    <p style="font-family: 'Courier New', Courier, monospace;">Spectral Entropy ANOVA Test completed in 54.24 seconds
    </p>

    <details>
      <summary style="padding-top: 1em; font-family:Arial, Helvetica, sans-serif;">Show/Hide Pairwise T-Test Code
      </summary>
      <pre><code class="python">
  def pairwise_t_test(df, target_labels, feature="Spectral Entropy"):
      label_data = {label: df.filter(pl.col("Label") == label)[feature].to_list() for label in target_labels}
      
      results = {}
      alpha = 0.05 / (len(target_labels) * (len(target_labels) - 1) / 2)
      
      for l1, l2 in list(combinations(target_labels, 2)):
          
          data1, data2 = label_data[l1], label_data[l2]
          
          t_statistic, p_value = scipy.stats.ttest_ind(data1, data2, equal_var=False)
          
          results[f"{l1} vs {l2}"] = {
              "T-Statistic": t_statistic,
              "P-Value": p_value,
              "Significant": p_value < alpha
          }
      
      return results
  
  def format_t_test_results(results_dict):
      df_results = pd.DataFrame.from_dict(results_dict, orient="index")
      df_results.rename(columns={"T-Statistic": "T-Statistic", "P-Value": "P-Value", "Significant": "Significant"}, inplace=True)
      
      # Apply scientific notation for small p-values
      df_results["P-Value"] = df_results["P-Value"].apply(lambda x: f"{x:.15e}" if x < 1e-5 else f"{x:.15f}")
      
      print("\n=== Pairwise T-Test Results ===\n")
      print(df_results.to_string(index=True))
      </code></pre>
    </details>
    <pre><code class="python">
  # Calculate spectral entropy for t test
  df = df.with_columns([
      pl.col("Audio").map_elements(
        lambda y: compute_spectral_entropy(np.array(y)
      ), return_dtype=pl.Float64).alias("Spectral Entropy")
  ])
  
  labels = df["Label"].unique()
  t_test_results = pairwise_t_test(
    df,
    target_labels=target_labels,
    feature="Spectral Entropy"
  )
  format_t_test_results(t_test_results)
    </code></pre>

    <p class="text">
      The pairwise T-test results:
    </p>
    <p style="font-family: 'Courier New', Courier, monospace;">=== Pairwise T-Test Results ===</p><br>
    <p style="font-family: 'Courier New', Courier, monospace;">T-Statistic P-Value Significant</p>
    <p style="font-family: 'Courier New', Courier, monospace;">dysregulated vs selftalk -15.478506
      5.762621024926745e-50 True</p>
    <p style="font-family: 'Courier New', Courier, monospace;">dysregulated vs delighted -11.935268
      1.187641968091671e-31 True</p>
    <p style="font-family: 'Courier New', Courier, monospace;">dysregulated vs frustrated -2.101557
      0.035734114041646 False</p>
    <p style="font-family: 'Courier New', Courier, monospace;">selftalk vs delighted 1.645512
      0.099995279192350 False</p>
    <p style="font-family: 'Courier New', Courier, monospace;">selftalk vs frustrated 14.175571
      3.365473807647995e-44 True</p>
    <p style="font-family: 'Courier New', Courier, monospace;">delighted vs frustrated 10.485234
      3.053207937235983e-25 True</p><br>

    <img src="clarity/images/SpectralEntropyAcrossLabels.png" alt="Spectral Entropy Across Labels">
  </div>


  </div>


  <!-- Footer Page -->
  <footer>
    <div class="container">
      <p>
        This website is built on the <a href="https://shikun.io/projects/clarity">Clarity Template</a>, designed by <a
          href="https://shikun.io/">Shikun Liu</a>.
      </p>
    </div>
  </footer>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <script src="clarity/clarity.js"></script>
  <script src="assets/scripts/main.js"></script>

</html>
</body>